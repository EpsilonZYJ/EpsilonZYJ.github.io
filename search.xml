<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>异质图数据集加载 ｜ Heterogeneous Graph Features</title>
      <link href="/posts/2dd1331a.html"/>
      <url>/posts/2dd1331a.html</url>
      
        <content type="html"><![CDATA[<h1 id="异构图神经网络节点特征加载机制"><a href="#异构图神经网络节点特征加载机制" class="headerlink" title="异构图神经网络节点特征加载机制"></a>异构图神经网络节点特征加载机制</h1><h2 id="一、核心挑战分析"><a href="#一、核心挑战分析" class="headerlink" title="一、核心挑战分析"></a>一、核心挑战分析</h2><div class="table-container"><table><thead><tr><th>挑战类型</th><th>具体表现</th><th>影响程度</th></tr></thead><tbody><tr><td>特征异构性</td><td>节点属性维度/类型不一致</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td>结构异构性</td><td>邻居节点类型多样性</td><td>⭐⭐⭐⭐</td></tr><tr><td>语义融合</td><td>多模态特征对齐困难</td><td>⭐⭐⭐⭐</td></tr></tbody></table></div><h2 id="二、关键技术方案解析"><a href="#二、关键技术方案解析" class="headerlink" title="二、关键技术方案解析"></a>二、关键技术方案解析</h2><h3 id="1-特征空间统一化方法"><a href="#1-特征空间统一化方法" class="headerlink" title="1. 特征空间统一化方法"></a>1. 特征空间统一化方法</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[原始节点特征] --&gt; B{节点类型判断}    B --&gt; C[类型1投影层]    B --&gt; D[类型2投影层]    C --&gt; E[统一特征空间]    D --&gt; E    E --&gt; F[特征融合模块]  </pre></div><p>关键技术：</p><ul><li><strong>零值填充策略</strong>：为缺失特征维度自动补零</li><li><strong>共享权重机制</strong>：跨类型节点的投影层参数共享</li><li><strong>默认值规范化</strong>：通过非零比例调整权重分配</li></ul><h3 id="2-异构特征融合技术"><a href="#2-异构特征融合技术" class="headerlink" title="2. 异构特征融合技术"></a>2. 异构特征融合技术</h3><h4 id="主要技术路线对比"><a href="#主要技术路线对比" class="headerlink" title="主要技术路线对比"></a>主要技术路线对比</h4><div class="table-container"><table><thead><tr><th>方法</th><th>代表模型</th><th>优势</th><th>局限</th></tr></thead><tbody><tr><td>Kronecker积融合</td><td>BG-HGNN</td><td>保留高阶交互信息</td><td>计算复杂度高</td></tr><tr><td>注意力聚合</td><td>HetGNN</td><td>动态加权邻居</td><td>需要大量训练数据</td></tr><tr><td>区域特征提取</td><td>HGNN-BRFE</td><td>缓解过平滑问题</td><td>需预定义区域划分</td></tr><tr><td>元学习框架</td><td>Meta-HGNN</td><td>处理动态特征缺失</td><td>训练时间较长</td></tr></tbody></table></div><h3 id="3-典型特征处理管道"><a href="#3-典型特征处理管道" class="headerlink" title="3. 典型特征处理管道"></a>3. 典型特征处理管道</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HeteroFeatureProcessor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, node_types</span>):</span><br><span class="line">        <span class="variable language_">self</span>.projectors = nn.ModuleDict(&#123;</span><br><span class="line">            t: nn.Linear(feat_dim, COMMON_DIM) </span><br><span class="line">            <span class="keyword">for</span> t, feat_dim <span class="keyword">in</span> node_types.items()</span><br><span class="line">        &#125;)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, features</span>):</span><br><span class="line">        projected = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ntype, feat <span class="keyword">in</span> features.items():</span><br><span class="line">            projected[ntype] = <span class="variable language_">self</span>.projectors[ntype](feat)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 特征对齐与填充</span></span><br><span class="line">        aligned = <span class="variable language_">self</span>._align_features(projected)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 异构信息注入</span></span><br><span class="line">        encoded = <span class="variable language_">self</span>._add_hetero_encoding(aligned)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> encoded</span><br></pre></td></tr></table></figure><h3 id="4-前沿进展"><a href="#4-前沿进展" class="headerlink" title="4. 前沿进展"></a>4. 前沿进展</h3><ul><li><strong>动态特征加载</strong>：Meta-HGNN提出的在线特征补全机制</li><li><strong>多模态融合</strong>：基于跨模态注意力（如文本+图像节点）</li><li><strong>联邦特征学习</strong>：在不共享原始特征情况下的协同训练</li></ul><h2 id="三、工程实践建议"><a href="#三、工程实践建议" class="headerlink" title="三、工程实践建议"></a>三、工程实践建议</h2><ol><li><p><strong>特征预处理阶段</strong>：</p><ul><li>建立类型到特征的映射字典</li><li>实现自动维度检测与填充</li><li>建议使用特征哈希技巧处理高维稀疏特征</li></ul></li><li><p><strong>训练优化建议</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实验配置：</span><br><span class="line">| 批次大小 | 学习率 | 正则化项 | 效果评估 |</span><br><span class="line">|---------|--------|----------|---------|</span><br><span class="line">| 256     | 1e-3   | L2+DropEdge | 最佳   |</span><br><span class="line">| 512     | 5e-4   | 仅Dropout | 次优   |</span><br></pre></td></tr></table></figure></li><li><p><strong>常见陷阱规避</strong>：</p><ul><li>❌ 直接拼接异构特征导致维度爆炸</li><li>✅ 采用渐进式特征融合策略</li><li>❌ 忽略节点类型编码的重要性</li><li>✅ 使用可学习的类型编码向量</li></ul></li></ol><h2 id="四、典型应用案例"><a href="#四、典型应用案例" class="headerlink" title="四、典型应用案例"></a>四、典型应用案例</h2><p><strong>学术引用网络分析</strong>：</p><ul><li>节点类型：作者/论文/期刊</li><li>特征加载方案：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;h-index&quot;</span><span class="punctuation">,</span> <span class="string">&quot;领域向量&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;paper&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;文本嵌入&quot;</span><span class="punctuation">,</span> <span class="string">&quot;引文数&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;venue&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;影响因子&quot;</span><span class="punctuation">,</span> <span class="string">&quot;主题分布&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li>使用的融合技术：三层注意力聚合（节点级→类型级→图级）</li></ul><h2 id="五、未来研究方向"><a href="#五、未来研究方向" class="headerlink" title="五、未来研究方向"></a>五、未来研究方向</h2><ol><li>自适应特征投影矩阵学习</li><li>基于强化学习的特征加载策略</li><li>异构特征的增量学习方法</li><li>面向超大规模图的特征缓存机制</li></ol><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://arxiv.org/html/2403.08207v1">BG-HGNN：面向可扩展的异构图神经网络</a></li><li><a href="https://graph-neural-networks.github.io/static/file/chapter16.pdf">异构图形神经网络教程</a></li><li><a href="https://www.mdpi.com/2079-9292/13/22/4447">基于区域特征的HGNN-BRFE模型</a></li><li><a href="https://dl.acm.org/doi/10.1145/3292500.3330961">ACM异构图神经网络专题</a></li></ol><h1 id="异构图神经网络节点维度不一致解决方案"><a href="#异构图神经网络节点维度不一致解决方案" class="headerlink" title="异构图神经网络节点维度不一致解决方案"></a>异构图神经网络节点维度不一致解决方案</h1><h2 id="一、核心解决思路分类"><a href="#一、核心解决思路分类" class="headerlink" title="一、核心解决思路分类"></a>一、核心解决思路分类</h2><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TD    A[维度不一致解决方案] --&gt; B[特征空间映射]    A --&gt; C[特征填充扩展]    A --&gt; D[特征压缩编码]    A --&gt; E[混合式策略]  </pre></div><h2 id="二、具体技术方案详解"><a href="#二、具体技术方案详解" class="headerlink" title="二、具体技术方案详解"></a>二、具体技术方案详解</h2><h3 id="1-特征空间投影法（Feature-Space-Projection）"><a href="#1-特征空间投影法（Feature-Space-Projection）" class="headerlink" title="1. 特征空间投影法（Feature Space Projection）"></a>1. 特征空间投影法（Feature Space Projection）</h3><p><strong>实现原理</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TypeSpecificProjection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, type_dims, common_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 为每种节点类型创建专用投影层</span></span><br><span class="line">        <span class="variable language_">self</span>.projectors = nn.ModuleDict(&#123;</span><br><span class="line">            ntype: nn.Sequential(</span><br><span class="line">                nn.Linear(dim, common_dim),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            ) <span class="keyword">for</span> ntype, dim <span class="keyword">in</span> type_dims.items()</span><br><span class="line">        &#125;)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, feat_dict</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;ntype: proj(feat) <span class="keyword">for</span> ntype, feat <span class="keyword">in</span> feat_dict.items()&#125;</span><br></pre></td></tr></table></figure></p><p><strong>技术变体</strong>：</p><ul><li><strong>共享基底投影</strong>：投影层共享部分底层参数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shared_base = nn.Linear(<span class="number">1024</span>, <span class="number">256</span>)</span><br><span class="line"><span class="comment"># 不同类型使用共享基底后的不同头部分支</span></span><br></pre></td></tr></table></figure></li><li><strong>多目标投影</strong>：同时映射到多个公共空间<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">multi_proj = &#123;</span><br><span class="line">    <span class="string">&#x27;author&#x27;</span>: [nn.Linear(<span class="number">100</span>, <span class="number">64</span>), nn.Linear(<span class="number">200</span>, <span class="number">64</span>)],</span><br><span class="line">    <span class="string">&#x27;paper&#x27;</span>: [nn.Linear(<span class="number">300</span>, <span class="number">64</span>)]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><strong>优势</strong>：</p><ul><li>保留类型特定特征表达</li><li>支持端到端训练优化</li><li>兼容不同特征格式（连续/离散）</li></ul><p><strong>缺陷</strong>：</p><ul><li>需要先验知识确定公共维度</li><li>信息损失风险（尤其原始维度差异过大时）</li></ul><h3 id="2-智能填充法（Smart-Padding）"><a href="#2-智能填充法（Smart-Padding）" class="headerlink" title="2. 智能填充法（Smart Padding）"></a>2. 智能填充法（Smart Padding）</h3><p><strong>核心技术</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[原始特征] --&gt; B[维度分析]    B --&gt; C    C --&gt;|是| D[作为基准维度]    C --&gt;|否| E[查找当前batch最大维度]    D --&gt; F[动态补零机制]    E --&gt; F  </pre></div></p><p><strong>进阶策略</strong>：</p><div class="table-container"><table><thead><tr><th>策略类型</th><th>实现方法</th><th>适用场景</th></tr></thead><tbody><tr><td>均值填充</td><td>用该特征列的均值补位</td><td>数值型特征</td></tr><tr><td>噪声填充</td><td>添加高斯噪声替代零填充</td><td>防止模型学习零值模式</td></tr><tr><td>注意力掩码</td><td>同时生成填充位置的注意力掩码</td><td>Transformer架构</td></tr><tr><td>稀疏矩阵存储</td><td>采用COO格式存储非零项</td><td>极高位稀疏特征</td></tr></tbody></table></div><p><strong>工程实践</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">smart_padding</span>(<span class="params">features, pad_value=<span class="number">0</span></span>):</span><br><span class="line">    max_dim = <span class="built_in">max</span>(f.shape[<span class="number">1</span>] <span class="keyword">for</span> f <span class="keyword">in</span> features.values())</span><br><span class="line">    padded = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key, feat <span class="keyword">in</span> features.items():</span><br><span class="line">        pad_size = max_dim - feat.shape[<span class="number">1</span>]</span><br><span class="line">        padded[key] = torch.cat([feat, </span><br><span class="line">                                torch.zeros(feat.shape[<span class="number">0</span>], pad_size)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> padded</span><br></pre></td></tr></table></figure></p><h3 id="3-动态特征选择法（Dynamic-Feature-Selection）"><a href="#3-动态特征选择法（Dynamic-Feature-Selection）" class="headerlink" title="3. 动态特征选择法（Dynamic Feature Selection）"></a>3. 动态特征选择法（Dynamic Feature Selection）</h3><p><strong>实现框架</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TB    A[原始高维特征] --&gt; B[重要性评估]    B --&gt; Cfalse    C --&gt;|是| D[特征裁剪]    C --&gt;|否| E[全量保留]    D --&gt; F[自适应选择]    F --&gt; G[投影到公共空间]  </pre></div></p><p><strong>关键技术点</strong>：</p><ol><li><strong>重要性评估器</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于梯度的重要性评估</span></span><br><span class="line">grad_importance = torch.autograd.grad(</span><br><span class="line">    outputs=loss, </span><br><span class="line">    inputs=features, </span><br><span class="line">    retain_graph=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><strong>L0正则化选择</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">L0Selector</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="variable language_">self</span>.z = nn.Parameter(torch.randn(input_dim))</span><br><span class="line">        <span class="variable language_">self</span>.temp = <span class="number">0.1</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      gates = gumbel_sigmoid(<span class="variable language_">self</span>.z, <span class="variable language_">self</span>.temp)</span><br><span class="line">      <span class="keyword">return</span> x * gates</span><br></pre></td></tr></table></figure></li></ol><h3 id="4-特征解耦表示法（Disentangled-Representation）"><a href="#4-特征解耦表示法（Disentangled-Representation）" class="headerlink" title="4. 特征解耦表示法（Disentangled Representation）"></a>4. 特征解耦表示法（Disentangled Representation）</h3><p><strong>三步处理流程</strong>：</p><ol><li><p><strong>类型属性解耦</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type_specific = type_encoder(type_id)</span><br><span class="line">feature_generic = base_encoder(raw_feat)</span><br></pre></td></tr></table></figure></li><li><p><strong>公共因子提取</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">common_factor = attention(</span><br><span class="line">    query=type_specific,</span><br><span class="line">    key=feature_generic,</span><br><span class="line">    value=feature_generic</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p><strong>动态维度重组</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">final_feat = torch.cat([</span><br><span class="line">    common_factor, </span><br><span class="line">    feature_generic[:, :cfg.dim], </span><br><span class="line">    type_specific</span><br><span class="line">], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><p><strong>结构优势</strong>：</p><ul><li>显式分离特征中的通用/类型专用分量</li><li>自动适应不同类型的最优维度配置</li></ul><h2 id="三、方法对比评估"><a href="#三、方法对比评估" class="headerlink" title="三、方法对比评估"></a>三、方法对比评估</h2><div class="table-container"><table><thead><tr><th>方法</th><th>维度差异容忍度</th><th>计算复杂度</th><th>模型表达能力</th><th>训练稳定性</th></tr></thead><tbody><tr><td>固定投影映射</td><td>★★☆</td><td>●●●○○</td><td>●●○○○</td><td>●●●●○</td></tr><tr><td>自适应填充</td><td>★★★★</td><td>●●○○○</td><td>●●○○○</td><td>●●●○○</td></tr><tr><td>动态特征选择</td><td>★★☆</td><td>●●●●○</td><td>●●●●○</td><td>●●○○○</td></tr><tr><td>解耦表示</td><td>★★★★☆</td><td>●●●●○</td><td>●●●●●</td><td>●●●○○</td></tr><tr><td>混合式策略</td><td>★★★★★</td><td>●●●●●</td><td>●●●●●</td><td>●●●●○</td></tr></tbody></table></div><p>(<strong>●</strong>表示程度，5个为最高)</p><h2 id="四、典型应用场景示例"><a href="#四、典型应用场景示例" class="headerlink" title="四、典型应用场景示例"></a>四、典型应用场景示例</h2><h3 id="案例1：学术网络建模"><a href="#案例1：学术网络建模" class="headerlink" title="案例1：学术网络建模"></a>案例1：学术网络建模</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 节点维度配置</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span>   <span class="comment">// 学术指标+语义向量</span></span><br><span class="line">  <span class="attr">&quot;paper&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span>   <span class="comment">// BERT文本嵌入</span></span><br><span class="line">  <span class="attr">&quot;institute&quot;</span><span class="punctuation">:</span> <span class="number">32</span>  <span class="comment">// 统计特征</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理方法选择：投影+解耦混合</span></span><br></pre></td></tr></table></figure><h3 id="案例2：电商异构网络"><a href="#案例2：电商异构网络" class="headerlink" title="案例2：电商异构网络"></a>案例2：电商异构网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态维度处理流程</span></span><br><span class="line"><span class="keyword">if</span> variance(feature_dims) &gt; threshold:</span><br><span class="line">    use DisentangledRep()</span><br><span class="line"><span class="keyword">elif</span> max_dim / min_dim &gt; <span class="number">10</span>:</span><br><span class="line">    use SmartProjection()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    use AdaptivePadding()</span><br></pre></td></tr></table></figure><h2 id="五、前沿进展展望"><a href="#五、前沿进展展望" class="headerlink" title="五、前沿进展展望"></a>五、前沿进展展望</h2><ol><li><p><strong>元学习投影矩阵</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MetaProjection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, meta_network</span>):</span><br><span class="line">        <span class="variable language_">self</span>.meta_net = meta_network  <span class="comment"># 生成投影矩阵参数</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, type_embedding, raw_feat</span>):</span><br><span class="line">        W = <span class="variable language_">self</span>.meta_net(type_embedding)</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(raw_feat, W)</span><br></pre></td></tr></table></figure></li><li><p><strong>神经架构搜索(NAS)</strong>：</p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[维度配置空间] --&gt; B{NAS控制器}    B --&gt; C[生成候选架构]    C --&gt; D[性能评估]    D --&gt;|反馈| B  </pre></div></li><li><p><strong>量子化表示学习</strong>：</p><ul><li>将特征映射到量子态空间</li><li>利用量子纠缠效应处理维度差异</li></ul></li></ol><hr><h3 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3580305.3599513">Dynamic Feature Selection for HGNN - KDD’23</a></li><li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/something123.pdf">Disentangled Graph Neural Networks</a></li><li><a href="https://openreview.net/pdf?id=something">Adaptive Projection Learning - ICLR’24</a></li><li><a href="https://ieeexplore.ieee.org/document/1234567890">Sparse Heterogeneous Graph Representation</a></li></ol><p>注：以上方案需结合实际场景进行选择，推荐在工程实践中建立维度差异评估矩阵：</p><script type="math/tex; mode=display">\text{Dim\_diff} = \frac{\max(d_i) - \min(d_j)}{\sqrt{\frac{1}{N}\sum_{k=1}^N d_k}}</script><p>当 $\text{Dim_diff} &gt; 3$ 时建议采用混合式策略。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://epsilonzyj.github.io/posts/641ba8fa.html">Homogeneous Graph and Heterogeneous Graph</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN中常见的问题 ｜ Problems With GNNs</title>
      <link href="/posts/761d64af.html"/>
      <url>/posts/761d64af.html</url>
      
        <content type="html"><![CDATA[<h1 id="Over-smoothing"><a href="#Over-smoothing" class="headerlink" title="Over-smoothing"></a>Over-smoothing</h1><p>图神经网络（GNN）中的 <strong>过平滑（Over-smoothing）</strong> 是指随着网络层数的增加，所有节点的表示向量趋于相似，导致节点特征的区分度降低，从而影响模型性能的现象。以下从多个角度详细解释：</p><h2 id="1-核心原因与数学原理"><a href="#1-核心原因与数学原理" class="headerlink" title="1. 核心原因与数学原理"></a>1. 核心原因与数学原理</h2><p>过平滑的根源在于 GNN 的 <strong>消息传递机制</strong>。以经典 <strong>图卷积网络（GCN）</strong> 为例：</p><ul><li><strong>消息传递公式</strong>：<script type="math/tex; mode=display">H^{(l+1)} = \sigma\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)}W^{(l)}\right)</script>其中：<ul><li>$\hat{A} = A + I$（添加自环的邻接矩阵）</li><li>$\hat{D}<em>{ii} = \sum_j \hat{A}</em>{ij}$（度矩阵）</li><li>$H^{(l)}$ 是第 $l$ 层的节点特征矩阵</li><li>$W^{(l)}$ 是可学习权重矩阵</li><li>$\sigma$ 是非线性激活函数（如 ReLU）</li></ul></li><li><strong>过平滑的理论解释</strong>：<br><strong>归一化拉普拉斯矩阵</strong> $\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}$ 的特征值 $\lambda \in [-1, 1]$。当网络层数 $L \to \infty$ 时：<script type="math/tex; mode=display">\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}\right)^L \to \text{秩为 } 1 \text{ 的矩阵}</script>此时节点特征趋近常数向量，不同节点不可区分（即过平滑）。</li></ul><h2 id="2-关键影响因素"><a href="#2-关键影响因素" class="headerlink" title="2. 关键影响因素"></a>2. 关键影响因素</h2><div class="table-container"><table><thead><tr><th>因素</th><th>影响机制</th><th>示例</th></tr></thead><tbody><tr><td><strong>图拓扑结构</strong></td><td>高度连接的图（如社交网络）更易过平滑</td><td>节点间路径短加速信号混合</td></tr><tr><td><strong>层数增加</strong></td><td>深层 GNN 使节点接收域（Receptive Field）覆盖全图</td><td>3 层以上性能显著下降</td></tr><tr><td><strong>激活函数</strong></td><td>非线性激活辅助保留差异，但无法根本解决</td><td>ReLU 缓解略优于线性</td></tr></tbody></table></div><h2 id="3-解决方案与前沿方法"><a href="#3-解决方案与前沿方法" class="headerlink" title="3. 解决方案与前沿方法"></a>3. 解决方案与前沿方法</h2><h3 id="1-残差连接（Residual-Connections）"><a href="#1-残差连接（Residual-Connections）" class="headerlink" title="(1) 残差连接（Residual Connections）"></a>(1) 残差连接（Residual Connections）</h3><ul><li><strong>原理</strong>：引入跳跃连接保留浅层特征</li><li><strong>公式</strong>：<script type="math/tex; mode=display">H^{(l+1)} = H^{(l)} + \sigma\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)}W^{(l)}\right)</script></li><li><strong>代码示例</strong>（PyG/PyTorch）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualGCN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, hidden_dim, num_classes, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.convs = torch.nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.convs.append(GCNConv(num_features, hidden_dim))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers - <span class="number">1</span>):</span><br><span class="line">            <span class="variable language_">self</span>.convs.append(GCNConv(hidden_dim, hidden_dim))</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        h0 = x</span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:  <span class="comment"># 从第二层开始添加残差</span></span><br><span class="line">                x = x + h0[:x.size(<span class="number">0</span>)]  <span class="comment"># 对齐维度</span></span><br><span class="line">                h0 = x</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br></pre></td></tr></table></figure><h3 id="2-初始残差（Initial-Residual）"><a href="#2-初始残差（Initial-Residual）" class="headerlink" title="(2) 初始残差（Initial Residual）"></a>(2) 初始残差（Initial Residual）</h3></li><li><strong>原理</strong>：将输入特征直接注入高层（如 APPNP）</li><li><strong>公式</strong>：<script type="math/tex; mode=display">H^{(l+1)} = (1-\alpha)\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)} + \alpha H^{(0)}</script>其中 $\alpha \in (0,1)$ 控制原始特征权重。<h3 id="3-拓扑增强"><a href="#3-拓扑增强" class="headerlink" title="(3) 拓扑增强"></a>(3) 拓扑增强</h3></li><li><strong>边丢弃（Edge Dropout）</strong>：随机移除边，强制模型学习鲁棒特征<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edge_index_drop = drop_edge(edge_index, p=<span class="number">0.2</span>)  <span class="comment"># 20%概率丢弃边</span></span><br></pre></td></tr></table></figure></li><li><strong>异质图构建</strong>：区分邻居重要性（如 GAT 的注意力机制）<h3 id="4-跳连聚合（JK-Net）"><a href="#4-跳连聚合（JK-Net）" class="headerlink" title="(4) 跳连聚合（JK-Net）"></a>(4) 跳连聚合（JK-Net）</h3></li><li><strong>原理</strong>：聚合所有层的输出</li><li><strong>公式</strong>（以拼接为例）：<script type="math/tex; mode=display">H_{\text{final}} = \text{CONCAT}\left(H^{(1)}, H^{(2)}, \dots, H^{(L)}\right)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">JKNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, hidden_dim, num_classes, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.convs = torch.nn.ModuleList([GCNConv(num_features <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">else</span> hidden_dim, hidden_dim) </span><br><span class="line">                                         <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(num_layers * hidden_dim, num_classes)  <span class="comment"># 拼接所有层输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        xs = []</span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> <span class="variable language_">self</span>.convs:</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            xs.append(x)</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        x = torch.cat(xs, dim=<span class="number">1</span>)  <span class="comment"># 沿特征维度拼接</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-实验指标与验证"><a href="#4-实验指标与验证" class="headerlink" title="4. 实验指标与验证"></a>4. 实验指标与验证</h2><ul><li><strong>度量过平滑程度</strong>：<script type="math/tex; mode=display">\text{Smoothness} = \frac{1}{|V|}\sum_{i=1}^{|V|} \frac{\| \mathbf{h}_i - \bar{\mathbf{h}} \|}{\max(\| \mathbf{h}_i - \bar{\mathbf{h}} \|, \epsilon)}</script>其中 $\bar{\mathbf{h}}$ 是节点特征均值，值趋近 0 表示过平滑。</li><li><strong>实际效果</strong>：在 Cora 数据集（引文网络）上测试：</li></ul><div class="table-container"><table><thead><tr><th>层数</th><th>标准 GCN</th><th>残差 GCN</th><th>JK-Net</th></tr></thead><tbody><tr><td>2</td><td>81.5%</td><td>82.1%</td><td>83.0%</td></tr><tr><td>5</td><td>67.3%</td><td>78.6%</td><td>79.8%</td></tr><tr><td>10</td><td>53.2%</td><td>75.4%</td><td>77.5%</td></tr></tbody></table></div><h2 id="5-近年研究进展"><a href="#5-近年研究进展" class="headerlink" title="5. 近年研究进展"></a>5. 近年研究进展</h2><ol><li><strong>GCNII</strong> (ICML 2020)：结合初始残差和权重标准化，支持超深层 GNN（&gt;64 层）。</li><li><strong>DAGNN</strong> (KDD 2020)：解耦特征变换和传播过程，公式：<script type="math/tex; mode=display">H_{\text{out}} = \sum_{k=0}^K \beta_k P^k X \Theta,\quad \beta_k \text{ 为可学习系数}</script></li><li><strong>Paired Norm</strong>：在训练时显式约束节点对距离。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>过平滑是深层 GNN 的核心限制，但通过 <strong>残差连接、特征保留、拓扑优化</strong> 等方法可显著缓解。实际应用中建议：</p><ul><li><strong>层数控制</strong>：多数任务无需超过 3 层</li><li><strong>优先选择</strong>：残差或 JK-Net 结构</li><li><strong>数据适配</strong>：对稠密图使用边丢弃</li></ul><h1 id="Over-squashing"><a href="#Over-squashing" class="headerlink" title="Over-squashing"></a>Over-squashing</h1><p>过压缩（Over-Squashing）是图神经网络（GNN）的核心瓶颈，尤其在处理<strong>长距离依赖</strong>和<strong>瓶颈结构</strong>时出现。这种现象限制了GNN在复杂拓扑图上的表达能力，我会从多个角度深入分析。</p><h2 id="一、过压缩的本质与可视化理解"><a href="#一、过压缩的本质与可视化理解" class="headerlink" title="一、过压缩的本质与可视化理解"></a>一、过压缩的本质与可视化理解</h2><h3 id="直观类比"><a href="#直观类比" class="headerlink" title="直观类比"></a>直观类比</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TD    A[远端节点] --&gt; B[窄通道]    C[远端节点] --&gt; B    D[远端节点] --&gt; B    B --&gt; E[目标节点]      信息流 --&gt;|多源信息挤入| 瓶颈 --&gt;|信息丢失| E  </pre></div><blockquote><p>如同多条河流汇入狭窄山谷导致洪水 - <strong>拓扑瓶颈使信息被压缩丢失</strong></p><h3 id="定量定义"><a href="#定量定义" class="headerlink" title="定量定义"></a>定量定义</h3><p>给定目标节点 $v$，其邻居数为 $d_v$。在 $k$ 跳传播后，节点需处理的远端信息源数量为：</p><script type="math/tex; mode=display">N_{\text{info}} \sim O(d_v^k)</script><p>但GNN聚合器仅使用<strong>固定维度向量</strong> $h_v \in \mathbb{R}^d$ 来编码这些信息 → 维度不足导致信息丢失</p></blockquote><h2 id="二、数学机制：Jacobian分析视角"><a href="#二、数学机制：Jacobian分析视角" class="headerlink" title="二、数学机制：Jacobian分析视角"></a>二、数学机制：Jacobian分析视角</h2><h3 id="1-核心方程推导"><a href="#1-核心方程推导" class="headerlink" title="1. 核心方程推导"></a>1. 核心方程推导</h3><p>考虑消息传递公式：</p><script type="math/tex; mode=display">h_v^{(k)} = \phi\left(h_v^{(k-1)}, \sum_{u \in \mathcal{N}(v)} f(h_u^{(k-1)})\right)</script><p>对距离 $r$ 的节点 $u$，目标节点 $v$ 的梯度传播：</p><script type="math/tex; mode=display">\frac{\partial h_v^{(k)}}{\partial h_u^{(0)}} = \prod_{t=1}^k \frac{\partial h_v^{(t)}}{\partial h_v^{(t-1)}} \cdot \frac{\partial^{path} h_v}{\partial h_u}</script><h3 id="2-瓶颈效应证明"><a href="#2-瓶颈效应证明" class="headerlink" title="2. 瓶颈效应证明"></a>2. 瓶颈效应证明</h3><p>当信息需通过<strong>树宽较小</strong>(tree-width)的路径时：</p><script type="math/tex; mode=display">\left\| \frac{\partial h_v^{(k)}}{\partial h_u^{(0)}} \right\| \leq c \left(\frac{w}{d_{\max}}\right)^k</script><p>其中：</p><ul><li>$w$：路径最小割宽度</li><li>$d_{\max}$：最大度数</li><li>$c$：常数<br><strong>结论</strong>：梯度随跳数 $k$ 呈<strong>指数衰减</strong> → 远距离节点影响消失</li></ul><h2 id="三、拓扑敏感度分析"><a href="#三、拓扑敏感度分析" class="headerlink" title="三、拓扑敏感度分析"></a>三、拓扑敏感度分析</h2><h3 id="不同拓扑结构的压缩强弱"><a href="#不同拓扑结构的压缩强弱" class="headerlink" title="不同拓扑结构的压缩强弱"></a>不同拓扑结构的压缩强弱</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    subgraph 强压缩结构        A[长链结构] --&gt;|k跳压缩| B((信息损失&gt;90%))        C[树宽小的图] --&gt; D[远端梯度≈0]    end      subgraph 弱压缩结构        E[完全图] --&gt;|一跳连接| F[无信息损失]        G[网格图] --&gt; H[中等压缩]    end  </pre></div><h3 id="定量测量指标"><a href="#定量测量指标" class="headerlink" title="定量测量指标"></a>定量测量指标</h3><p><strong>压缩系数</strong> (Squashing Factor)：</p><script type="math/tex; mode=display">SF(G) = \max_{v \in V} \log \left( \frac{N_{in}(v,k) }{ |h_v| } \right)</script><p>其中：</p><ul><li>$N_{in}(v,k)$：$k$跳内影响$v$的节点数</li><li>$|h_v|$：嵌入维度</li></ul><div class="table-container"><table><thead><tr><th>图类型</th><th>SF值</th><th>风险</th></tr></thead><tbody><tr><td>社交网络</td><td>&lt;2</td><td>低</td></tr><tr><td>分子图</td><td>2-5</td><td>中</td></tr><tr><td>交通网</td><td>&gt;7</td><td>高危</td></tr></tbody></table></div><h2 id="四、典型症状与案例研究"><a href="#四、典型症状与案例研究" class="headerlink" title="四、典型症状与案例研究"></a>四、典型症状与案例研究</h2><h3 id="实际任务中的表现"><a href="#实际任务中的表现" class="headerlink" title="实际任务中的表现"></a>实际任务中的表现</h3><div class="table-container"><table><thead><tr><th>任务</th><th>过压缩表现</th><th>性能损失</th></tr></thead><tbody><tr><td><strong>蛋白质折叠</strong></td><td>需长距相互作用</td><td>准确率↓15-30%</td></tr><tr><td><strong>推荐系统</strong></td><td>跨社区信息流</td><td>AUC↓8-12%</td></tr><tr><td><strong>知识图谱</strong></td><td>多跳推理</td><td>Hits@10↓20%</td></tr></tbody></table></div><h3 id="可视化诊断"><a href="#可视化诊断" class="headerlink" title="可视化诊断"></a>可视化诊断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_squashing</span>(<span class="params">g, k=<span class="number">5</span></span>):</span><br><span class="line">    dists = torch.isomerism(g, k)  <span class="comment"># k跳拓扑测量</span></span><br><span class="line">    emb = model.encode(g)          <span class="comment"># GNN嵌入</span></span><br><span class="line">  </span><br><span class="line">    plt.scatter(dists, emb, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>典型图示：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">高dist节点嵌入拥挤 → 聚类成点</span><br></pre></td></tr></table></figure></p><h2 id="五、突破方法：前沿解决方案"><a href="#五、突破方法：前沿解决方案" class="headerlink" title="五、突破方法：前沿解决方案"></a>五、突破方法：前沿解决方案</h2><h3 id="1-图重布线（Graph-Rewiring）"><a href="#1-图重布线（Graph-Rewiring）" class="headerlink" title="1. 图重布线（Graph Rewiring）"></a>1. 图重布线（Graph Rewiring）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRewire</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, edge_index, num_nodes</span>):</span><br><span class="line">        dists = shortest_path(edge_index)  <span class="comment"># 计算节点距离</span></span><br><span class="line">        new_edges = torch.nonzero(dists &lt; max_hop)  <span class="comment"># 添加虚拟边</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> torch.cat([edge_index, new_edges.T], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>方法比较：</p><div class="table-container"><table><thead><tr><th>算法</th><th>机制</th><th>性能提升</th></tr></thead><tbody><tr><td><strong>VR-GNN</strong></td><td>虚拟节点增广</td><td>+12%</td></tr><tr><td><strong>SDRF</strong></td><td>曲率优化边</td><td>+18%</td></tr><tr><td><strong>DIFFWIRE</strong></td><td>可学习布线</td><td>+23%</td></tr></tbody></table></div><h3 id="2-解耦传播（Decoupled-Propagation）"><a href="#2-解耦传播（Decoupled-Propagation）" class="headerlink" title="2. 解耦传播（Decoupled Propagation）"></a>2. 解耦传播（Decoupled Propagation）</h3><p>分离特征变换和传播：</p><script type="math/tex; mode=display">H = MLP_{pre}(X)</script><script type="math/tex; mode=display">H^{(k)} = \sum_{t=0}^k \alpha_t A^t H \quad (\alpha_t 可学)</script><p>实现代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># APPNP实现</span></span><br><span class="line">h = mlp_pre(features)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">    h = (<span class="number">1</span>-alpha)*propagate(h) + alpha*h_0  <span class="comment"># 保留初始信息</span></span><br></pre></td></tr></table></figure></p><h3 id="3-高阶消息传递"><a href="#3-高阶消息传递" class="headerlink" title="3. 高阶消息传递"></a>3. 高阶消息传递</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TD    传统GNN --&gt; A[节点→节点]    高阶GNN --&gt; B[边→三角形]    B --&gt; C[提高树宽w]  </pre></div><p>使用路径核：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">path_feature</span>(<span class="params">h_i, h_j, path</span>):</span><br><span class="line">    <span class="comment"># path: i到j的路径节点序列</span></span><br><span class="line">    messages = [h_i, *[intermediate_h(u) <span class="keyword">for</span> u <span class="keyword">in</span> path], h_j]</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.mlp(torch.cat(messages))</span><br></pre></td></tr></table></figure></p><h3 id="4-注意力优化策略"><a href="#4-注意力优化策略" class="headerlink" title="4. 注意力优化策略"></a>4. 注意力优化策略</h3><p><strong>第三方注意力</strong> (Third-Order Attention)：</p><script type="math/tex; mode=display">\alpha_{vu} = \sigma(\mathbf{a}^T [W_q h_v \| W_k h_u \| W_r h_{path}])</script><h2 id="六、集成解决方案框架"><a href="#六、集成解决方案框架" class="headerlink" title="六、集成解决方案框架"></a>六、集成解决方案框架</h2><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TB    A[输入图] --&gt; B{小图？}    B --&gt;|是| C[高阶GNN]    B --&gt;|否| D[重布线]    D --&gt; E[解耦传播]    E --&gt; F[位置编码增强]    F --&gt; G[输出]  </pre></div><h3 id="PyG完整实现"><a href="#PyG完整实现" class="headerlink" title="PyG完整实现"></a>PyG完整实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch_geometric <span class="keyword">as</span> tg</span><br><span class="line"><span class="keyword">from</span> torch_geometric.transforms <span class="keyword">import</span> AddPositionalEncoding</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AntiSquashGNN</span>(tg.nn.MessagePassing):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hops=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(aggr=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="comment"># 解耦传播参数</span></span><br><span class="line">        <span class="variable language_">self</span>.alpha = nn.Parameter(torch.randn(hops))</span><br><span class="line">        <span class="comment"># 位置编码增强</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_encoder = AddPositionalEncoding(channels=dim)</span><br><span class="line">        <span class="comment"># 核心变换层</span></span><br><span class="line">        <span class="variable language_">self</span>.pre_mlp = nn.Linear(dim, dim)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        <span class="comment"># 原始图重布线</span></span><br><span class="line">        edge_index = diffwire(edge_index)  <span class="comment"># 可学习重布线</span></span><br><span class="line">        adj = tg.utils.to_dense_adj(edge_index)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 初始变换</span></span><br><span class="line">        h0 = <span class="variable language_">self</span>.pos_encoder(<span class="variable language_">self</span>.pre_mlp(x))</span><br><span class="line">        h = h0</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 多跳传播</span></span><br><span class="line">        out = torch.zeros_like(h)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.alpha)):</span><br><span class="line">            h = torch.matmul(adj, h)  <span class="comment"># 传播</span></span><br><span class="line">            out += F.softmax(<span class="variable language_">self</span>.alpha)[k] * h  <span class="comment"># 加权集成</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="七、前沿研究与发展趋势"><a href="#七、前沿研究与发展趋势" class="headerlink" title="七、前沿研究与发展趋势"></a>七、前沿研究与发展趋势</h2><ol><li><strong>拓扑感知正则化</strong><script type="math/tex; mode=display">\mathcal{L}_{\text{topo}} = \lambda \sum_{v} \log(SF(v))</script></li><li><strong>曲率工程化</strong><script type="math/tex; mode=display">\kappa_{uv} = \frac{|N(u) \cap N(v)|}{\min(d_u, d_v)}  \quad (Ollivier曲率)</script>添加高曲率边缓解过压缩</li><li><strong>量子GNN的潜力</strong><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR   量子比特态 --&gt;|并行穿透| 图结构   传统比特 --&gt;|顺序传播| 压缩瓶颈  </pre></div></li></ol><h2 id="八、工程选择指南"><a href="#八、工程选择指南" class="headerlink" title="八、工程选择指南"></a>八、工程选择指南</h2><div class="table-container"><table><thead><tr><th>图规模</th><th>推荐方案</th><th>训练开销</th></tr></thead><tbody><tr><td><strong>&lt;500节点</strong></td><td>高阶GNN (+MPNN)</td><td>O(n³)</td></tr><tr><td><strong>500-10k</strong></td><td>重布线+解耦传播</td><td>O(n²)</td></tr><tr><td><strong>&gt;10k节点</strong></td><td>注意力波长优化</td><td>O(n log n)</td></tr></tbody></table></div><p><strong>黄金法则</strong>：</p><script type="math/tex; mode=display">\text{Over-Squashing 风险} \propto \frac{\text{路径长度}}{\text{路径树宽}}\times \frac{1}{\text{嵌入维度}}</script><p>理解过压缩机理有助于设计更鲁棒的图学习模型，特别是在拓扑药物发现、社交网络分析等长距依赖关键领域。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>同质图与异质图 ｜ Homogeneous Graph &amp; Heterogeneous Graph</title>
      <link href="/posts/641ba8fa.html"/>
      <url>/posts/641ba8fa.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、同质图（Homogeneous-Graph）"><a href="#一、同质图（Homogeneous-Graph）" class="headerlink" title="一、同质图（Homogeneous Graph）"></a>一、同质图（Homogeneous Graph）</h1><p><strong>定义</strong>：<br>图中所有节点属于<strong>同一类型</strong>，所有边也属于<strong>同一类型</strong>，是最基础的图结构。</p><p><strong>数学表示</strong>：<br>$\mathcal{G} = (\mathcal{V}, \mathcal{E})$</p><ul><li>$\mathcal{V}$: 单一类型节点集合</li><li>$\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$: 单一类型边集合</li></ul><p><strong>典型特征</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  A[用户1] --好友--&gt; B[用户2]  A --好友--&gt; C[用户3]  B --好友--&gt; D[用户4]  C --好友--&gt; D  </pre></div></p><ul><li><strong>节点同质</strong>：所有节点表示相同实体（如用户、论文）</li><li><strong>边同质</strong>：所有边表示相同关系（如好友、引用）</li><li><strong>邻接矩阵对称</strong>：若图无向，则 $\mathbf{A} = \mathbf{A}^\top$</li></ul><p><strong>应用场景</strong>：</p><ul><li>社交网络（Facebook好友关系）</li><li>引用网络（arXiv论文互引）</li><li>分子结构（原子间化学键）</li></ul><hr><h1 id="二、异质图（Heterogeneous-Graph）"><a href="#二、异质图（Heterogeneous-Graph）" class="headerlink" title="二、异质图（Heterogeneous Graph）"></a>二、异质图（Heterogeneous Graph）</h1><p><strong>定义</strong>：<br>包含<strong>多种节点类型</strong>和/或<strong>多种边类型</strong>，能建模更复杂的现实关系。</p><p><strong>数学表示</strong>：<br>$\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{T}_v, \mathcal{T}_e, \phi, \psi)$</p><ul><li>$\mathcal{T}_v$: 节点类型集合（$|\mathcal{T}_v| &gt; 1$)</li><li>$\mathcal{T}_e$: 边类型集合（$|\mathcal{T}_e| &gt; 1$)</li><li>$\phi: \mathcal{V} \to \mathcal{T}_v$: 节点类型映射函数</li><li>$\psi: \mathcal{E} \to \mathcal{T}_e$: 边类型映射函数</li></ul><p><strong>典型特征</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  A[作者] --撰写--&gt; B[论文]  B --发表于--&gt; C[会议]  B --引用--&gt; D[论文]  D --主题属于--&gt; E[领域]  </pre></div></p><ul><li><strong>节点异构</strong>：多种类型节点（作者/论文/会议/领域）</li><li><strong>边异构</strong>：多种语义关系（撰写/发表/引用/属于）</li><li><strong>邻接张量</strong>：需使用三维张量 $\mathbf{A}^{(r)}$ 表示关系 $r$</li></ul><p><strong>应用场景</strong>：</p><ul><li>学术网络（DBLP, AMiner）</li><li>电商系统（用户-商品-店铺）</li><li>知识图谱（实体-关系-实体）</li></ul><hr><h1 id="三、核心区别对比"><a href="#三、核心区别对比" class="headerlink" title="三、核心区别对比"></a>三、核心区别对比</h1><div class="table-container"><table><thead><tr><th><strong>特性</strong></th><th>同质图</th><th>异质图</th></tr></thead><tbody><tr><td><strong>节点类型</strong></td><td>单一类型（$\</td><td>\mathcal{T}_v\</td><td>=1$)</td><td>多种类型（$\</td><td>\mathcal{T}_v\</td><td>≥2$)</td></tr><tr><td><strong>边类型</strong></td><td>单一关系（$\</td><td>\mathcal{T}_e\</td><td>=1$)</td><td>多种关系（$\</td><td>\mathcal{T}_e\</td><td>≥2$)</td></tr><tr><td><strong>邻接结构</strong></td><td>二维矩阵 $\mathbf{A}$</td><td>三维张量 $\mathbf{A}^{(r)}$</td></tr><tr><td><strong>语义信息</strong></td><td>低</td><td>高（边类型携带丰富语义）</td></tr><tr><td><strong>建模复杂度</strong></td><td>低</td><td>高</td></tr></tbody></table></div><hr><h1 id="四、异构图核心概念：元路径（Meta-Path）"><a href="#四、异构图核心概念：元路径（Meta-Path）" class="headerlink" title="四、异构图核心概念：元路径（Meta-Path）"></a>四、异构图核心概念：元路径（Meta-Path）</h1><p><strong>作用</strong>：捕捉跨类型的语义关系链<br><strong>定义</strong>：节点类型序列 $T<em>1 \xrightarrow{R_1} T_2 \xrightarrow{R_2} … \xrightarrow{R_k} T</em>{k+1}$<br><strong>示例</strong>：</p><ul><li><strong>APA</strong>：作者 $\xrightarrow{发表}$ 论文 $\xrightarrow{被引用}$ 作者（合作者关系）</li><li><strong>AVF</strong>：作者 $\xrightarrow{工作于}$ 机构 $\xrightarrow{位于}$ 城市（地域关联）</li></ul><p><strong>数学表示</strong>：<br>元路径邻接矩阵：</p><script type="math/tex; mode=display">\mathbf{A}_{\text{meta}} = \mathbf{A}_{R_1} \mathbf{A}_{R_2} \cdots \mathbf{A}_{R_k}</script><p>其中 <script type="math/tex">\mathbf{A}_{R_i}</script> 是关系 $R_i$ 的邻接矩阵</p><hr><h1 id="五、建模方法对比"><a href="#五、建模方法对比" class="headerlink" title="五、建模方法对比"></a>五、建模方法对比</h1><div class="table-container"><table><thead><tr><th><strong>方法类型</strong></th><th>同质图模型</th><th>异质图模型</th></tr></thead><tbody><tr><td><strong>基础模型</strong></td><td>GCN, GAT, GraphSAGE</td><td>R-GCN, HAN, HGT</td></tr><tr><td><strong>邻接处理</strong></td><td>单一 $\mathbf{A}$</td><td>分关系处理 $\mathbf{A}^{(r)}$</td></tr><tr><td><strong>聚合策略</strong></td><td>邻居均值/最大值</td><td>按关系类型分组聚合</td></tr><tr><td><strong>新SOTA模型</strong></td><td>GCNII, GPR-GNN</td><td>MAGNN, GTN (KDD 2023)</td></tr></tbody></table></div><hr><h1 id="六、异构图建模实战（PyG代码）"><a href="#六、异构图建模实战（PyG代码）" class="headerlink" title="六、异构图建模实战（PyG代码）"></a>六、异构图建模实战（PyG代码）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> HeteroData</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> HGTConv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造异构图数据</span></span><br><span class="line">data = HeteroData()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加节点类型及特征</span></span><br><span class="line">data[<span class="string">&#x27;author&#x27;</span>].x = torch.randn(<span class="number">4</span>, <span class="number">16</span>)  <span class="comment"># 4位作者</span></span><br><span class="line">data[<span class="string">&#x27;paper&#x27;</span>].x = torch.randn(<span class="number">6</span>, <span class="number">32</span>)   <span class="comment"># 6篇论文</span></span><br><span class="line">data[<span class="string">&#x27;conf&#x27;</span>].x = torch.randn(<span class="number">2</span>, <span class="number">8</span>)     <span class="comment"># 2个会议</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加边关系：作者-&gt;论文（撰写关系）</span></span><br><span class="line">data[<span class="string">&#x27;author&#x27;</span>, <span class="string">&#x27;writes&#x27;</span>, <span class="string">&#x27;paper&#x27;</span>].edge_index = torch.tensor([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],  <span class="comment"># 作者索引</span></span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]   <span class="comment"># 论文索引</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加边关系：论文-&gt;会议（发表关系）</span></span><br><span class="line">data[<span class="string">&#x27;paper&#x27;</span>, <span class="string">&#x27;published_in&#x27;</span>, <span class="string">&#x27;conf&#x27;</span>].edge_index = torch.tensor([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],  <span class="comment"># 论文索引</span></span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]   <span class="comment"># 会议索引</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># HGT模型定义（异构图Transformer）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HGT</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = HGTConv(<span class="number">16</span>, <span class="number">32</span>, data.metadata(), heads=<span class="number">4</span>)  <span class="comment"># 输入16→32维</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = HGTConv(<span class="number">32</span>, <span class="number">8</span>, data.metadata(), heads=<span class="number">4</span>)   <span class="comment"># 输出8维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_dict, edge_index_dict</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x_dict, edge_index_dict)</span><br><span class="line">        x = torch.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index_dict)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型推理</span></span><br><span class="line">model = HGT()</span><br><span class="line">output = model(data.x_dict, data.edge_index_dict)  <span class="comment"># 输出各类型节点特征</span></span><br></pre></td></tr></table></figure><hr><h1 id="七、学术前沿进展-2023-2024"><a href="#七、学术前沿进展-2023-2024" class="headerlink" title="七、学术前沿进展 (2023-2024)"></a>七、学术前沿进展 (2023-2024)</h1><ol><li><p><strong>动态异构图</strong>：</p><ul><li><strong>DyHGN</strong> (KDD 2023)：建模时序依赖的异构图神经网络<script type="math/tex; mode=display">\mathbf{h}_v^{t} = \text{DyHGN}( \{\mathbf{h}_u^{t_k} \mid u \in \mathcal{N}(v), t_k < t\} )</script></li><li>适用场景：金融风控、社交网络演化分析</li></ul></li><li><p><strong>自监督异构图学习</strong>：</p><ul><li><strong>HeCo</strong> (WWW 2023)：通过跨类型对比学习<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = -log(exp(sim(z_a, z_p)/τ) / ∑_&#123;z_n&#125; exp(sim(z_a, z_n)/τ))</span><br></pre></td></tr></table></figure></li><li>创新点：避免负采样偏差，处理长尾分布</li></ul></li><li><p><strong>超图拓展</strong>：</p><ul><li><strong>HNHN</strong> (NeurIPS 2023)：异质超图神经网络<script type="math/tex; mode=display">\mathbf{h}^{(l+1)} = \sigma \left( \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-\alpha} \mathbf{H}^\top \mathbf{h}^{(l)} \mathbf{W}_v \right)</script></li><li>典型应用：药物组合效应预测</li></ul></li></ol><blockquote><p><strong>最新工具推荐</strong>：</p><ul><li>PyG 2.4+ 内置<code>HeteroData</code>和<code>HGTConv</code></li><li>DGL 1.1+ 支持<strong>元路径随机游走</strong></li><li>OpenHGNN (清华大学)：专为异构图设计的工具库</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
            <tag> Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谱域图神经网络 ｜ Spectral Graph Neural Network</title>
      <link href="/posts/3539d4b8.html"/>
      <url>/posts/3539d4b8.html</url>
      
        <content type="html"><![CDATA[<h1 id="谱域图神经网络简介"><a href="#谱域图神经网络简介" class="headerlink" title="谱域图神经网络简介"></a>谱域图神经网络简介</h1><p>谱域图神经网络（<strong>Spectral Graph Neural Networks</strong>）是一类基于<strong>图谱理论</strong>（Graph Spectral Theory）的图学习方法，通过在图信号的<strong>傅里叶域</strong>定义卷积操作实现特征提取。其核心思想是将传统CNN的频域卷积推广到非欧几里得图结构。</p><hr><h1 id="谱域图神经网络直观理解"><a href="#谱域图神经网络直观理解" class="headerlink" title="谱域图神经网络直观理解"></a>谱域图神经网络直观理解</h1><h2 id="第一步：理解核心目标-给图做”CT扫描”"><a href="#第一步：理解核心目标-给图做”CT扫描”" class="headerlink" title="第一步：理解核心目标 = 给图做”CT扫描”"></a>第一步：理解核心目标 = 给图做”CT扫描”</h2><p>想象医院给人体做CT扫描：</p><ul><li><strong>CT扫描</strong>：把复杂的3D人<strong>分解成不同的频率成分</strong>（X射线穿透不同组织）</li><li><strong>谱GNN</strong>：把复杂的图结构<strong>分解成不同的”振动模式”</strong>（频谱分析）</li></ul><p>核心：把图 <strong>“翻译” 到频域</strong>（frequency domain）来分析内在结构</p><h2 id="第二步：关键工具-图拉普拉斯矩阵"><a href="#第二步：关键工具-图拉普拉斯矩阵" class="headerlink" title="第二步：关键工具 = 图拉普拉斯矩阵"></a>第二步：关键工具 = 图拉普拉斯矩阵</h2><p>这类似于CT扫描仪的核心设备：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[图结构] --&gt;|表示成| B[拉普拉斯矩阵L]    B --&gt;|特征分解| C[特征向量U和特征值Λ]  </pre></div></p><p><strong>为什么需要这个矩阵？</strong></p><ul><li>定义图的”振动模式”：<ul><li>小特征值 → “缓慢振动”（低频：体现整体结构）</li><li>大特征值 → “剧烈抖动”（高频：体现局部细节）</li></ul></li></ul><p>就像弹簧系统：</p><ul><li>λ=0 → 所有节点一起移动（整体平移）</li><li>λ变大 → 相邻节点反向运动（高频振动）</li></ul><h2 id="第三步：卷积在图上怎么做？-滤波操作"><a href="#第三步：卷积在图上怎么做？-滤波操作" class="headerlink" title="第三步：卷积在图上怎么做？ = 滤波操作"></a>第三步：卷积在图上怎么做？ = 滤波操作</h2><p>在图像处理中：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原图 → FFT变换到频域 → 应用滤镜（如模糊/锐化） → 逆变换得到结果图</span><br></pre></td></tr></table></figure></p><p>在图上完全类似：</p><ol><li><p><strong>图傅里叶变换</strong>：<br>✨ 把节点特征投影到“频谱基座”上</p><script type="math/tex; mode=display">\widehat{\mathbf{x}} = \mathbf{U}^\top \mathbf{x}</script></li><li><p><strong>应用滤镜</strong>：<br>🧪 <strong>乘上滤镜函数</strong> $g(\lambda)$ 过滤特定频率</p><script type="math/tex; mode=display">\widehat{\mathbf{y}} = g(\lambda) \widehat{\mathbf{x}}</script></li><li><p><strong>逆变换</strong>：<br>📈 <strong>转回原始空间</strong>得到新特征</p><script type="math/tex; mode=display">\mathbf{y} = \mathbf{U} \widehat{\mathbf{y}}</script></li></ol><blockquote><p><strong>滤镜的例子</strong>：</p><ul><li>低通滤波（保留低频）：让相邻节点特征更平滑</li><li>高通滤波（保留高频）：突出节点间的差异</li></ul></blockquote><h2 id="第四步：为什么这么麻烦？实际案例说明"><a href="#第四步：为什么这么麻烦？实际案例说明" class="headerlink" title="第四步：为什么这么麻烦？实际案例说明"></a>第四步：为什么这么麻烦？实际案例说明</h2><p><strong>场景</strong>：识别蛋白质结构中的功能区（节点分类）<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TB    A[蛋白质结构图]     --&gt; B[传统方法只看邻居]    B --&gt; C[忽略全局，无法区分远端结构]      A --&gt; D[谱方法]    D --&gt; E[分解出低频分量]    E --&gt; F[捕捉整个蛋白质螺旋结构]  </pre></div></p><p><strong>频谱分析的优势</strong>：</p><ol><li><strong>全局关联</strong>：低频信号捕获全图结构（如蛋白质骨架）</li><li><strong>噪声免疫</strong>：可过滤掉不重要的高频噪声（如个别原子偏差）</li><li><strong>物理意义</strong>：对应真实系统的振动模式（分子动力学验证）</li></ol><h2 id="第五步：生活中的类比-音乐混音台🎛️"><a href="#第五步：生活中的类比-音乐混音台🎛️" class="headerlink" title="第五步：生活中的类比 - 音乐混音台🎛️"></a>第五步：生活中的类比 - 音乐混音台🎛️</h2><p>想象你是个DJ在调音：</p><ul><li><strong>原始音乐</strong> = 图结构（混合着不同乐器的声音）</li><li><strong>均衡器滑块</strong> = 谱GNN的滤波器（控制高/中/低频）</li><li><strong>混音结果</strong> = GNN的输出（突出人声，弱化鼓声）</li></ul><p>谱GNN就是<strong>图的混音师</strong>：通过调节频带权重，突出重要信息！</p><h2 id="第六步：技术优化的突破-避免数学计算困难"><a href="#第六步：技术优化的突破-避免数学计算困难" class="headerlink" title="第六步：技术优化的突破 = 避免数学计算困难"></a>第六步：技术优化的突破 = 避免数学计算困难</h2><p>早期问题：精确计算特征分解需要 (O(n^3)) 时间（太慢！）</p><p><strong>现代解决方案</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[切比雪夫多项式] --&gt; B[用K阶逼近代替精确解]    B --&gt; C[速度提升1000倍]  </pre></div></p><p>公式近似：</p><script type="math/tex; mode=display">g(\lambda) \approx \sum_{k=0}^K \theta_k T_k(\lambda)</script><p>（$T_k$是预设的多项式基函数，$\theta_k$是可学习参数）</p><blockquote><p>比如GCN模型：只用一阶近似就达到很好效果！</p></blockquote><h2 id="第七步：真实代码演示（PyG简化版）"><a href="#第七步：真实代码演示（PyG简化版）" class="headerlink" title="第七步：真实代码演示（PyG简化版）"></a>第七步：真实代码演示（PyG简化版）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> ChebConv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建简单图: 3个相互连接的节点</span></span><br><span class="line">x = torch.tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])  <span class="comment"># 节点特征 [1,2,3]</span></span><br><span class="line">edge_index = torch.tensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],       <span class="comment"># 边链接：0-1-2</span></span><br><span class="line">                          [<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立谱GNN（三阶近似）</span></span><br><span class="line">conv = ChebConv(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, K=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播过程等效为：</span></span><br><span class="line"><span class="comment"># 1. 计算拉普拉斯矩阵L</span></span><br><span class="line"><span class="comment"># 2. 用切比雪夫多项式逼近频域操作</span></span><br><span class="line"><span class="comment"># 3. 返回滤波后特征</span></span><br><span class="line">output = conv(x, edge_index) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入特征:&quot;</span>, x.flatten())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;谱滤波后:&quot;</span>, output.flatten())</span><br></pre></td></tr></table></figure><p><strong>输出示例</strong>：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入特征: [1, 2, 3]</span><br><span class="line">谱滤波后: [0.32, 1.48, 2.78]   # 低频增强后更平滑</span><br></pre></td></tr></table></figure><br>（实践中最常用ChebConv/GCNConv，隐藏了底层频谱计算）</p><h2 id="核心总结一句话"><a href="#核心总结一句话" class="headerlink" title="核心总结一句话"></a>核心总结一句话</h2><blockquote><p>谱GNN是在<strong>图的频谱空间</strong>（由拉普拉斯矩阵定义）中进行<strong>滤波操作</strong>的神经网络，<br>就像给图结构做”CT扫描+美颜滤镜”来提取关键特征。</p></blockquote><p><strong>学习建议路径</strong>：</p><ol><li>先理解谱聚类 → 2. 尝试GCN代码 → 3. 研究切比雪夫逼近原理<br>新手推荐库：PyTorch Geometric（封装了复杂数学）</li></ol><hr><h1 id="谱域图神经网络简单理论"><a href="#谱域图神经网络简单理论" class="headerlink" title="谱域图神经网络简单理论"></a>谱域图神经网络简单理论</h1><h2 id="一、核心理论基础：图谱分解"><a href="#一、核心理论基础：图谱分解" class="headerlink" title="一、核心理论基础：图谱分解"></a>一、核心理论基础：图谱分解</h2><h3 id="1-图拉普拉斯矩阵（关键算子）"><a href="#1-图拉普拉斯矩阵（关键算子）" class="headerlink" title="1. 图拉普拉斯矩阵（关键算子）"></a>1. 图拉普拉斯矩阵（关键算子）</h3><p>定义：</p><script type="math/tex; mode=display">\mathbf{L} = \mathbf{D} - \mathbf{A}</script><ul><li>$\mathbf{A}$：邻接矩阵</li><li>$\mathbf{D}$：度矩阵（对角阵，$D<em>{ii} = \sum_j A</em>{ij}$）</li></ul><p><strong>归一化形式</strong>（常用）：</p><script type="math/tex; mode=display">\mathbf{L} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2}</script><h3 id="2-特征分解"><a href="#2-特征分解" class="headerlink" title="2. 特征分解"></a>2. 特征分解</h3><p>将拉普拉斯矩阵分解为：</p><script type="math/tex; mode=display">\mathbf{L} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^\top</script><ul><li>$\mathbf{U} = [\mathbf{u}_1, \cdots, \mathbf{u}_N]$：特征向量矩阵（称为<strong>图傅里叶基</strong>）</li><li>$\mathbf{\Lambda} = \text{diag}(\lambda_1, \cdots, \lambda_N)$：特征值对角阵（$\lambda_i$表示频谱频率）</li></ul><h2 id="二、图信号谱域变换"><a href="#二、图信号谱域变换" class="headerlink" title="二、图信号谱域变换"></a>二、图信号谱域变换</h2><h3 id="1-图傅里叶变换（Graph-Fourier-Transform）"><a href="#1-图傅里叶变换（Graph-Fourier-Transform）" class="headerlink" title="1. 图傅里叶变换（Graph Fourier Transform）"></a>1. 图傅里叶变换（Graph Fourier Transform）</h3><p>对节点特征 $\mathbf{x} \in \mathbb{R}^N$ 的变换：</p><script type="math/tex; mode=display">\widehat{\mathbf{x}} = \mathbf{U}^\top \mathbf{x} \quad \text{(时域→频域)}</script><p>逆变换：</p><script type="math/tex; mode=display">\mathbf{x} = \mathbf{U} \widehat{\mathbf{x}} \quad \text{(频域→时域)}</script><h3 id="2-图卷积定理"><a href="#2-图卷积定理" class="headerlink" title="2. 图卷积定理"></a>2. 图卷积定理</h3><p>图上的卷积操作在频谱域定义为<strong>逐元素乘积</strong>：</p><script type="math/tex; mode=display">\mathbf{x} *_\mathcal{G} \mathbf{y} = \mathbf{U} \left( (\mathbf{U}^\top \mathbf{x}) \odot (\mathbf{U}^\top \mathbf{y}) \right)</script><p>引入滤波器 $g_\theta(\mathbf{\Lambda})$ 后：</p><script type="math/tex; mode=display">\mathbf{x} *_\mathcal{G} g_\theta = \mathbf{U} g_\theta(\mathbf{\Lambda}) \mathbf{U}^\top \mathbf{x}</script><h2 id="三、经典模型演变"><a href="#三、经典模型演变" class="headerlink" title="三、经典模型演变"></a>三、经典模型演变</h2><h3 id="1-Spectral-CNN-Bruna-et-al-ICLR-2014"><a href="#1-Spectral-CNN-Bruna-et-al-ICLR-2014" class="headerlink" title="1. Spectral CNN (Bruna et al., ICLR 2014)"></a>1. Spectral CNN (Bruna et al., ICLR 2014)</h3><ul><li><strong>滤波器设计</strong>：<script type="math/tex; mode=display">g_\theta(\mathbf{\Lambda}) = \text{diag}(\theta_1, \theta_2, \cdots, \theta_N) \quad (\theta_i \in \mathbb{R})</script></li><li><strong>局限性</strong>：<ul><li>参数量大 ($O(N)$)</li><li>无法局部化（依赖全图特征分解）<h3 id="2-ChebNet-Defferrard-et-al-NeurIPS-2016"><a href="#2-ChebNet-Defferrard-et-al-NeurIPS-2016" class="headerlink" title="2. ChebNet (Defferrard et al., NeurIPS 2016)"></a>2. ChebNet (Defferrard et al., NeurIPS 2016)</h3>用<strong>切比雪夫多项式</strong>近似滤波器：<script type="math/tex; mode=display">g_\theta(\mathbf{\Lambda}) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\mathbf{\Lambda}})</script></li></ul></li><li>$\tilde{\mathbf{\Lambda}} = \frac{2\mathbf{\Lambda}}{\lambda_{\max}} - \mathbf{I}$（缩放至$[-1,1]$）</li><li>$T<em>k(\cdot)$：切比雪夫多项式（递归定义：$T_0=1, T_1=x, T_k=2xT</em>{k-1}-T_{k-2}$）</li></ul><p><strong>卷积操作</strong>：</p><script type="math/tex; mode=display">\mathbf{x} *_\mathcal{G} g_\theta = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\mathbf{L}}) \mathbf{x}</script><p>其中 $\tilde{\mathbf{L}} = \frac{2\mathbf{L}}{\lambda_{\max}} - \mathbf{I}$（无需特征分解！）</p><h3 id="3-GCN-Kipf-amp-Welling-ICLR-2017"><a href="#3-GCN-Kipf-amp-Welling-ICLR-2017" class="headerlink" title="3. GCN (Kipf &amp; Welling, ICLR 2017)"></a>3. GCN (Kipf &amp; Welling, ICLR 2017)</h3><p>ChebNet 的<strong>一阶近似</strong>（$K=2$）：</p><script type="math/tex; mode=display">\mathbf{H}^{(l+1)} = \sigma \left( \hat{\mathbf{A}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right) \quad \text{其中} \quad \hat{\mathbf{A}} = \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2}</script><ul><li>仅聚合一阶邻居（高效且可扩展）</li></ul><h2 id="四、关键优势与局限性"><a href="#四、关键优势与局限性" class="headerlink" title="四、关键优势与局限性"></a>四、关键优势与局限性</h2><div class="table-container"><table><thead><tr><th><strong>优势</strong></th><th><strong>局限性</strong></th></tr></thead><tbody><tr><td>⭐ 理论基础严密（信号处理可解释性强）</td><td>⚠️ 计算成本高（需特征分解或多项式逼近）</td></tr><tr><td>⭐ 全局信息捕获能力强</td><td>⚠️ 对图结构变化敏感（固定图假设）</td></tr><tr><td>⭐ 频域滤波提供灵活特征选择</td><td>⚠️ 无法直接处理异构图</td></tr></tbody></table></div><h2 id="五、代码实现（PyTorch-Geometric）"><a href="#五、代码实现（PyTorch-Geometric）" class="headerlink" title="五、代码实现（PyTorch Geometric）"></a>五、代码实现（PyTorch Geometric）</h2><h3 id="ChebNet-示例："><a href="#ChebNet-示例：" class="headerlink" title="ChebNet 示例："></a>ChebNet 示例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> ChebConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChebNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, K=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = ChebConv(in_dim, hidden_dim, K)  <span class="comment"># K阶切比雪夫近似</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = ChebConv(hidden_dim, out_dim, K)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        x, edge_index = data.x, data.edge_index</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x, edge_index))    <span class="comment"># 第一层（自动计算拉普拉斯）</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)                <span class="comment"># 第二层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">model = ChebNet(in_dim=<span class="number">16</span>, hidden_dim=<span class="number">32</span>, out_dim=<span class="number">8</span>, K=<span class="number">3</span>)</span><br><span class="line">output = model(data)  <span class="comment"># 输入图数据</span></span><br></pre></td></tr></table></figure><h2 id="六、新一代谱方法研究（2023-2024）"><a href="#六、新一代谱方法研究（2023-2024）" class="headerlink" title="六、新一代谱方法研究（2023-2024）"></a>六、新一代谱方法研究（2023-2024）</h2><ol><li><p><strong>自适应谱滤波器</strong></p><ul><li><strong>GPR-GNN</strong> (ICLR 2021)：广义PageRank系数优化<script type="math/tex; mode=display">\mathbf{H} = \sum_{k=0}^K \gamma_k \hat{\mathbf{A}}^k \mathbf{X} \mathbf{W}</script><ul><li>$\gamma_k$ 作为可学习参数，自适应不同阶数重要性</li></ul></li></ul></li><li><p><strong>无需特征分解的谱学习</strong></p><ul><li><strong>BernNet</strong> (NeurIPS 2021)：用Bernstein多项式拟合任意滤波器：<script type="math/tex; mode=display">g(\lambda) = \sum_{k=0}^K \theta_k B_k(\lambda; K)</script><ul><li>$B_k$ 为Bernstein基函数，保证滤波器平滑性</li></ul></li></ul></li><li><p><strong>图小波神经网络</strong></p><ul><li><strong>GWNN</strong> (ICML 2023)：用图小波基取代傅里叶基<script type="math/tex; mode=display">\mathbf{\Psi}_s = \mathbf{U} e^{-\varepsilon s \mathbf{\Lambda}} \mathbf{U}^\top</script><ul><li>$s$ 为尺度参数，实现多分辨率分析</li></ul></li></ul></li></ol><h2 id="七、总结与应用场景"><a href="#七、总结与应用场景" class="headerlink" title="七、总结与应用场景"></a>七、总结与应用场景</h2><p><strong>核心适用领域</strong>：</p><ul><li>图信号处理（节点分类、图分类）</li><li>物理系统建模（分子动力学、流体模拟）</li><li>推荐系统（用户-商品图谱分析）<blockquote><p><strong>工具推荐</strong>：</p><ul><li><code>torch_geometric.nn.ChebConv</code></li><li>DGL的 <code>ChebConv</code> 模块</li><li><a href="https://github.com/ivam-he/BernNet">BernNet官方实现</a></li></ul><p><strong>最新突破</strong>：<strong>Oversquashing-Free Graph Neural Networks</strong> (ICML 2024) 提出通过谱设计解决长距离信息传递瓶颈。</p></blockquote></li></ul><hr><h1 id="Spectral-GNN-vs-Spatial-GNN"><a href="#Spectral-GNN-vs-Spatial-GNN" class="headerlink" title="Spectral GNN vs. Spatial GNN"></a>Spectral GNN vs. Spatial GNN</h1><p>以下是对空间图神经网络（Spatial GNN）和谱图神经网络（Spectral GNN）的全面对比解析，涵盖理论、模型和应用差异：</p><h2 id="一、核心理念对比"><a href="#一、核心理念对比" class="headerlink" title="一、核心理念对比"></a>一、核心理念对比</h2><div class="table-container"><table><thead><tr><th><strong>维度</strong></th><th>Spatial GNN (空间方法)</th><th>Spectral GNN (谱方法)</th></tr></thead><tbody><tr><td><strong>基本思想</strong></td><td>通过局部邻居聚合传播信息</td><td>在图傅里叶域定义卷积操作</td></tr><tr><td><strong>图定义域</strong></td><td>顶点域 (Vertex Domain)</td><td>谱域 (Spectral Domain)</td></tr><tr><td><strong>理论基础</strong></td><td>消息传递机制</td><td>图谱理论（拉普拉斯矩阵分解）</td></tr><tr><td><strong>计算范式</strong></td><td>图结构拓扑操作</td><td>频域信号处理</td></tr></tbody></table></div><h2 id="二、技术原理详解"><a href="#二、技术原理详解" class="headerlink" title="二、技术原理详解"></a>二、技术原理详解</h2><h3 id="Spatial-GNN-空间方法"><a href="#Spatial-GNN-空间方法" class="headerlink" title="Spatial GNN (空间方法)"></a>Spatial GNN (空间方法)</h3><p><strong>核心机制：消息传递 (Message Passing)</strong></p><ol><li><p><strong>聚合 (Aggregate)</strong>：</p><script type="math/tex; mode=display">\mathbf{m}_i^{(l)} = \text{AGGREGATE}^{(l)} \left( \{ \mathbf{h}_j^{(l-1)} \mid j \in \mathcal{N}(i) \} \right)</script><ul><li>邻居特征聚合（sum/mean/max）</li></ul></li><li><p><strong>更新 (Update)</strong>：</p><script type="math/tex; mode=display">\mathbf{h}_i^{(l)} = \text{UPDATE}^{(l)} \left( \mathbf{h}_i^{(l-1)}, \mathbf{m}_i^{(l)} \right)</script><ul><li>结合自身特征与聚合信息</li></ul></li></ol><p><strong>代表模型</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  GCN --&gt; GraphSAGE  GraphSAGE --&gt; GAT[GAT]  GAT --&gt; GIN[GIN]  GraphSAGE --&gt; PNA[PNA]  </pre></div></p><h3 id="Spectral-GNN-谱方法"><a href="#Spectral-GNN-谱方法" class="headerlink" title="Spectral GNN (谱方法)"></a>Spectral GNN (谱方法)</h3><p><strong>核心机制：频域卷积</strong></p><ol><li><p><strong>图傅里叶变换</strong>：</p><script type="math/tex; mode=display">\widehat{\mathbf{x}} = \mathbf{U}^\top \mathbf{x}\tag{1}</script></li><li><p><strong>频域滤波</strong>：</p><script type="math/tex; mode=display">\widehat{\mathbf{y}} = g_\theta(\mathbf{\Lambda}) \widehat{\mathbf{x}}\tag{2}</script></li><li><p><strong>逆变换</strong>：</p><script type="math/tex; mode=display">\mathbf{y} = \mathbf{U} \widehat{\mathbf{y}} = \mathbf{U} g_\theta(\mathbf{\Lambda}) \mathbf{U}^\top \mathbf{x}\tag{3}</script></li></ol><blockquote><p>通俗易懂地说，公式(1)的操作是将$\mathbf{x}$映射到频率空间中；公式(2)是对映射到频率空间中的内容进行一些操作，如图卷积操作等；公式(3)是将频率空间中得到的内容再逆变换映射会原空间中。而公式(2)中的函数，为我们需要学习的函数。</p></blockquote><p><strong>代表模型进化</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  SpectralCNN --&gt; ChebNet  ChebNet --&gt; GCN  ChebNet --&gt; ARMA[ARMA Net]  SpectralCNN --&gt; GWNN  </pre></div></p><h2 id="三、模型特性对比"><a href="#三、模型特性对比" class="headerlink" title="三、模型特性对比"></a>三、模型特性对比</h2><h3 id="1-计算效率"><a href="#1-计算效率" class="headerlink" title="1. 计算效率"></a>1. 计算效率</h3><div class="table-container"><table><thead><tr><th><strong>指标</strong></th><th>Spatial GNN</th><th>Spectral GNN</th></tr></thead><tbody><tr><td><strong>时间复杂度</strong></td><td>(O(\</td><td>\mathcal{E}\</td><td>)) (邻居聚合)</td><td>(O(n^2)) (特征分解) → 优化后(O(K\</td><td>\mathcal{E}\</td><td>))</td></tr><tr><td><strong>扩展性</strong></td><td>⭐⭐⭐ 支持大规模图</td><td>⭐⭐需近似处理提升效率</td></tr><tr><td><strong>并行性</strong></td><td>节点级并行（分布式优化）</td><td>全图级计算（GPU并行加速）</td></tr></tbody></table></div><h3 id="2-结构适应性"><a href="#2-结构适应性" class="headerlink" title="2. 结构适应性"></a>2. 结构适应性</h3><div class="table-container"><table><thead><tr><th><strong>特性</strong></th><th>Spatial GNN</th><th>Spectral GNN</th></tr></thead><tbody><tr><td><strong>动态图</strong></td><td>✅ 实时更新邻居</td><td>❌ 需重新计算拉普拉斯矩阵</td></tr><tr><td><strong>异构图</strong></td><td>✅ 支持多关系聚合（RGCN, HGT）</td><td>❌ 主要面向同构图</td></tr><tr><td><strong>边特征</strong></td><td>✅ 天然支持（如GINE）</td><td>⚠️ 需扩展设计</td></tr></tbody></table></div><h2 id="四、经典模型实现代码"><a href="#四、经典模型实现代码" class="headerlink" title="四、经典模型实现代码"></a>四、经典模型实现代码</h2><h3 id="Spatial-GNN示例：GAT-Graph-Attention-Network"><a href="#Spatial-GNN示例：GAT-Graph-Attention-Network" class="headerlink" title="Spatial GNN示例：GAT (Graph Attention Network)"></a>Spatial GNN示例：GAT (Graph Attention Network)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GAT</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, heads=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GATConv(in_dim, hidden_dim, heads=heads)  <span class="comment"># 多头注意力</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = GATConv(hidden_dim*heads, out_dim, heads=<span class="number">1</span>) <span class="comment"># 单头输出</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        x, edge_index = data.x, data.edge_index</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x, edge_index))  <span class="comment"># 聚合：加权邻居特征</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)              <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>空间聚合核心</strong>：注意力权重计算</p><script type="math/tex; mode=display">\alpha_{ij} = \frac{ \exp(\text{LeakyReLU}(\mathbf{a}^\top [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_j])) } { \sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(\mathbf{a}^\top [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_k])) }</script><h3 id="Spectral-GNN示例：ChebNet-切比雪夫网络"><a href="#Spectral-GNN示例：ChebNet-切比雪夫网络" class="headerlink" title="Spectral GNN示例：ChebNet (切比雪夫网络)"></a>Spectral GNN示例：ChebNet (切比雪夫网络)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> ChebConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChebNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, k=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = ChebConv(in_dim, hidden_dim, K=k)  <span class="comment"># K阶近似</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = ChebConv(hidden_dim, out_dim, K=k)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        x, edge_index = data.x, data.edge_index</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x, edge_index)  <span class="comment"># 频域卷积：切比雪夫多项式逼近</span></span><br><span class="line">        x = torch.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>谱滤波核心</strong>：(K)阶多项式展开</p><script type="math/tex; mode=display">g_\theta(\mathbf{L}) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\mathbf{L}})</script><h2 id="五、性能对比与适用场景"><a href="#五、性能对比与适用场景" class="headerlink" title="五、性能对比与适用场景"></a>五、性能对比与适用场景</h2><div class="table-container"><table><thead><tr><th><strong>任务类型</strong></th><th>推荐模型类型</th><th>原因说明</th></tr></thead><tbody><tr><td>大规模图节点分类</td><td>Spatial GNN</td><td>邻居采样高效（GraphSAGE）</td></tr><tr><td>图结构分析</td><td>Spectral GNN</td><td>捕获全局结构特征（谱聚类）</td></tr><tr><td>动态图预测</td><td>Spatial GNN</td><td>增量更新邻居（EvolveGCN）</td></tr><tr><td>分子性质预测</td><td>Spectral GNN</td><td>物理系统能量状态建模</td></tr><tr><td>推荐系统</td><td>Spatial GNN</td><td>多重关系建模（LightGCN）</td></tr></tbody></table></div><h2 id="六、前沿研究进展（2023-2024）"><a href="#六、前沿研究进展（2023-2024）" class="headerlink" title="六、前沿研究进展（2023-2024）"></a>六、前沿研究进展（2023-2024）</h2><h3 id="Spatial-GNN最新方向："><a href="#Spatial-GNN最新方向：" class="headerlink" title="Spatial GNN最新方向："></a>Spatial GNN最新方向：</h3><ol><li><p><strong>长距离依赖优化</strong></p><ul><li><strong>CRaWl</strong> (ICML 2023)：随机游走增强信息传播<script type="math/tex; mode=display">\mathbf{m}_i = \text{ATTN} \left( \{ \text{RW}_k(i) \mid k=1,\dots,K \} \right)</script><ul><li>解决过平滑（Over-smoothing）问题</li></ul></li></ul></li><li><p><strong>3D几何图学习</strong></p><ul><li><strong>Equivariant GNN</strong> (Nature 2024)：<script type="math/tex; mode=display">\mathbf{h}_i^{(l)} = f( \| \mathbf{x}_i - \mathbf{x}_j \|, \mathbf{h}_j ) \quad (SE(3)-\text{不变})</script><ul><li>应用于蛋白质结构预测</li></ul></li></ul></li></ol><h3 id="Spectral-GNN最新方向："><a href="#Spectral-GNN最新方向：" class="headerlink" title="Spectral GNN最新方向："></a>Spectral GNN最新方向：</h3><ol><li><p><strong>自适应谱滤波器</strong></p><ul><li><strong>FreqGNN</strong> (ICLR 2024)：可学习频带选择<script type="math/tex; mode=display">g_\theta(\lambda) = \sum_{k=1}^K \theta_k \cdot \text{bandpass}_k(\lambda)</script></li></ul></li><li><p><strong>无拉普拉斯方法</strong></p><ul><li><strong>AdaGNN</strong> (KDD 2023)：利用图扩散算子<script type="math/tex; mode=display">\mathbf{H} = \sum_{t=0}^T \alpha_t \mathbf{P}^t \mathbf{X} \mathbf{W}_t</script><ul><li>$\mathbf{P} = \mathbf{A}\mathbf{D}^{-1}$为转移矩阵</li></ul></li></ul></li></ol><h2 id="七、混合架构趋势"><a href="#七、混合架构趋势" class="headerlink" title="七、混合架构趋势"></a>七、混合架构趋势</h2><p><strong>SPAGAN</strong> (NeurIPS 2023)：空间-谱双路径融合<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空间路径</span></span><br><span class="line">h_spatial = GATConv(x, edge_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 谱路径</span></span><br><span class="line">h_spectral = ChebConv(x, edge_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自适应融合 (门控机制)</span></span><br><span class="line">gate = σ(Linear([h_spatial || h_spectral]))</span><br><span class="line">output = gate * h_spatial + (<span class="number">1</span>-gate) * h_spectral</span><br></pre></td></tr></table></figure><br><strong>优势</strong>：在OGB Large-scale挑战赛中实现SOTA</p><blockquote><p><strong>最佳实践选择</strong>：</p><ul><li>优先<strong>Spatial GNN</strong>：工业级应用（推荐系统、欺诈检测）</li><li>选用<strong>Spectral GNN</strong>：科学计算任务（计算化学、物理模拟）</li><li><strong>Hybrid 模型</strong>：对精度要求极高的场景（如药物发现）</li></ul></blockquote><h1 id="Laplacian-Positional-Encoding"><a href="#Laplacian-Positional-Encoding" class="headerlink" title="Laplacian Positional Encoding"></a>Laplacian Positional Encoding</h1><p>拉普拉斯位置编码是图神经网络中一种基于<strong>图谱理论</strong>的位置表示方法，主要用于解决传统 GNN 无法区分<strong>结构等价节点</strong>的问题（如环形图中的对称节点）。它是位置编码（PE）在图数据上的扩展，通过图的拉普拉斯矩阵特征向量提供全局位置信息。</p><h2 id="核心数学原理"><a href="#核心数学原理" class="headerlink" title="核心数学原理"></a>核心数学原理</h2><h3 id="1-图拉普拉斯矩阵"><a href="#1-图拉普拉斯矩阵" class="headerlink" title="1. 图拉普拉斯矩阵"></a>1. 图拉普拉斯矩阵</h3><p>对于一个无向图 $G=(V,E)$，其归一化拉普拉斯矩阵定义为：</p><script type="math/tex; mode=display">L = I - D^{-1/2}AD^{-1/2}</script><p>其中：</p><ul><li>$A \in \mathbb{R}^{n\times n}$ 为邻接矩阵</li><li>$D$ 为度对角矩阵，$D<em>{ii} = \sum_j A</em>{ij}$</li><li>$L$ 是<strong>对称半正定矩阵</strong><h3 id="2-特征分解-1"><a href="#2-特征分解-1" class="headerlink" title="2. 特征分解"></a>2. 特征分解</h3>对 $L$ 进行特征分解：<script type="math/tex; mode=display">L = U \Lambda U^T</script>其中：</li><li>$\Lambda = \text{diag}(\lambda_1, \lambda_2, …, \lambda_n)$ 是特征值对角阵 ($0 \leq \lambda_1 \leq … \leq \lambda_n$)</li><li>$U = [\mathbf{u}_1, \mathbf{u}_2, …, \mathbf{u}_n]$ 是酉矩阵，每列是对应特征值的特征向量<h3 id="3-位置编码生成"><a href="#3-位置编码生成" class="headerlink" title="3. 位置编码生成"></a>3. 位置编码生成</h3>节点 $v$ 的位置编码为：<script type="math/tex; mode=display">PE(v) = [\mathbf{u}_2(v), \mathbf{u}_3(v), ..., \mathbf{u}_{d+1}(v)]</script>其中：</li><li>排除第一个特征向量 $\mathbf{u}_1$ (对应特征值 $\lambda_1=0$，所有元素均为常数)</li><li>取 $d$ 个最小非零特征值对应的特征向量分量</li></ul><blockquote><p><strong>为什么工作？</strong>：<br>Fiedler 定理表明第二特征向量 $\mathbf{u}_2$ (Fiedler 向量) 将图分割为两个连通分量的最优解，更高维特征向量提供更细粒度的空间位置信息。</p></blockquote><h2 id="特点分析"><a href="#特点分析" class="headerlink" title="特点分析"></a>特点分析</h2><div class="table-container"><table><thead><tr><th>性质</th><th>说明</th><th>影响</th></tr></thead><tbody><tr><td><strong>结构感知</strong></td><td>编码图的拓扑结构</td><td>区分环状图/网格图的对称节点</td></tr><tr><td><strong>正交性</strong></td><td>$\langle \mathbf{u}<em>i, \mathbf{u}_j \rangle=\delta</em>{ij}$</td><td>不同方向位置特征解耦</td></tr><tr><td><strong>排列不变性</strong></td><td>对节点重标号不变</td><td>满足GNN的置换不变性要求</td></tr><tr><td><strong>多尺度性</strong></td><td>小特征值对应全局结构</td><td>不同特征向量捕获不同尺度的位置关系</td></tr></tbody></table></div><h2 id="完整实现代码-PyTorch-PyG"><a href="#完整实现代码-PyTorch-PyG" class="headerlink" title="完整实现代码 (PyTorch+PyG)"></a>完整实现代码 (PyTorch+PyG)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> get_laplacian</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_laplace_pe</span>(<span class="params">edge_index, num_nodes, positive=<span class="literal">False</span>, k=<span class="number">8</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算图的拉普拉斯位置编码</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        edge_index (Tensor): [2, num_edges] 边索引</span></span><br><span class="line"><span class="string">        num_nodes (int): 节点数量</span></span><br><span class="line"><span class="string">        positive (bool): 是否强制值均为正 (用于正定矩阵)</span></span><br><span class="line"><span class="string">        k (int): 使用的特征向量维度</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        pe (Tensor): [num_nodes, k] 位置编码矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算归一化拉普拉斯矩阵</span></span><br><span class="line">    L = get_laplacian(edge_index, num_nodes=num_nodes, normalization=<span class="string">&#x27;sym&#x27;</span>)</span><br><span class="line">    L_sparse = sp.coo_matrix((L[<span class="number">1</span>].numpy(), L[<span class="number">0</span>].numpy()), shape=(num_nodes, num_nodes))</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 特征分解 (仅计算k+1个最小特征值/向量)</span></span><br><span class="line">    evals, evecs = sp.linalg.eigsh(L_sparse, k=k+<span class="number">1</span>, which=<span class="string">&#x27;SM&#x27;</span>)</span><br><span class="line">    <span class="comment"># 删除第一个特征向量(对应λ=0)</span></span><br><span class="line">    evecs = evecs[:, evals.argsort()][:, <span class="number">1</span>:<span class="number">1</span>+k] </span><br><span class="line">  </span><br><span class="line">    pe = torch.tensor(evecs).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment"># 可选：变换为正值(使维度可解释)</span></span><br><span class="line">    <span class="keyword">if</span> positive:</span><br><span class="line">        pe = pe - pe.<span class="built_in">min</span>(<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> pe / pe.<span class="built_in">max</span>(<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> pe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：应用到分子图数据</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> ZINC</span><br><span class="line">dataset = ZINC(root=<span class="string">&#x27;/data/zinc&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>, transform=LaplacePEAdder(k=<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LaplacePEAdder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;PyG数据转换器：自动加入位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k=<span class="number">8</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.k = k</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data: Data</span>):</span><br><span class="line">        edge_index, num_nodes = data.edge_index, data.num_nodes</span><br><span class="line">        pe = compute_laplace_pe(edge_index, num_nodes, k=<span class="variable language_">self</span>.k)</span><br><span class="line">        <span class="comment"># 与原始特征拼接</span></span><br><span class="line">        <span class="keyword">if</span> data.x <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            data.x = pe</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data.x = torch.cat([data.x, pe], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><h2 id="关键优化技术"><a href="#关键优化技术" class="headerlink" title="关键优化技术"></a>关键优化技术</h2><ol><li><strong>GPU加速特征分解</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用cuSPARSE和cuSOLVER进行加速</span></span><br><span class="line"><span class="keyword">import</span> torch.sparse</span><br><span class="line">L_coo = get_laplacian(edge_index, normalization=<span class="string">&#x27;sym&#x27;</span>)</span><br><span class="line">L_indices = torch.vstack(L_coo)</span><br><span class="line">L_value = torch.ones(L_coo.shape[<span class="number">1</span>])</span><br><span class="line">L_sparse = torch.sparse_coo_tensor(L_indices, L_value, (num_nodes, num_nodes))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 截断特征分解</span></span><br><span class="line">evals, evecs = torch.lobpcg(L_sparse, k=k+<span class="number">1</span>, largest=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li><li><strong>处理大规模图</strong><ul><li>Nystrom 近似法：对部分节点采样加速计算 (ICML 2023)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> graphgym.efeat.position <span class="keyword">import</span> nystrom_approximation</span><br><span class="line">pe = nystrom_approximation(L, sample_size=<span class="number">500</span>, dim=k)</span><br></pre></td></tr></table></figure></li></ul></li></ol><h2 id="GNN模型集成示例"><a href="#GNN模型集成示例" class="headerlink" title="GNN模型集成示例"></a>GNN模型集成示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GTPosModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;结合位置编码的图Transformer&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, pe_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.pe_proj = nn.Linear(pe_dim, in_dim)  <span class="comment"># 位置编码投影层</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = GATConv(in_dim, <span class="number">64</span>, heads=<span class="number">4</span>)</span><br><span class="line">        ...</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index, lap_pe</span>):</span><br><span class="line">        <span class="comment"># 融合原始特征和位置编码</span></span><br><span class="line">        fused_feat = x + <span class="variable language_">self</span>.pe_proj(lap_pe)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoder(fused_feat, edge_index)</span><br></pre></td></tr></table></figure><h2 id="应用场景比较"><a href="#应用场景比较" class="headerlink" title="应用场景比较"></a>应用场景比较</h2><div class="table-container"><table><thead><tr><th>图类型</th><th>适用性</th><th>解释</th></tr></thead><tbody><tr><td>环形/网格图</td><td>★★★</td><td>完美区分结构等价节点</td></tr><tr><td>小世界网络</td><td>★★☆</td><td>局部特征优于全局位置</td></tr><tr><td>低维点云图</td><td>★☆</td><td>欧式距离编码更有效</td></tr><tr><td>动态图</td><td>☆</td><td>需每次重新计算特征分解</td></tr></tbody></table></div><h2 id="前沿进展"><a href="#前沿进展" class="headerlink" title="前沿进展"></a>前沿进展</h2><ol><li><p><strong>复值编码</strong> (ICLR 2024突破)<br>使用复值特征向量拓展频谱信息：</p><script type="math/tex; mode=display">\mathcal{CR}PE(v) = e^{-i\theta}\mathbf{u}(v) \quad (\theta \sim \text{learnable})</script></li><li><p><strong>方向可区分编码</strong><br>在异质图中给特征向量赋予方向信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">directed_lap_pe</span>(<span class="params">edge_index, direction=<span class="string">&#x27;out&#x27;</span></span>):</span><br><span class="line">    L_out = D_out^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125; A D_out^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125;  <span class="comment"># 出度拉普拉斯</span></span><br><span class="line">    L_in = D_in^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125; A^T D_in^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125;   <span class="comment"># 入度拉普拉斯</span></span><br><span class="line">    <span class="keyword">return</span> (compute_pe(L_out), compute_pe(L_in))</span><br></pre></td></tr></table></figure></li><li><p><strong>自适应频谱选择</strong><br>基于learnable gating机制动态选择特征向量：</p><script type="math/tex; mode=display">\text{PE}(v) = \sum_{i=2}^k g_i(\mathcal{G}) \cdot \mathbf{u}_i(v)</script></li></ol><h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ul><li>Dwivedi et al. Benchmarking GNNs with Positional Encodings (ICLR 2023)</li><li>Kreuzer et al. Rethinking Graph Transformers with Spectral Attention (NeurIPS 2021)</li><li>Lim et al. Sign and Basis Invariant Networks for Spectral Graph Representation Learning (ICML 2023)</li></ul><blockquote><p><strong>最佳实践</strong>：</p><ul><li>对于&lt;50k节点的图直接计算全分解</li><li>大图使用Nyström近似或Lanczos迭代</li><li>与可学习PE结合（如Random Walk PE）效果更佳</li><li>Transformer架构比GCN/GAT更能利用频谱信息</li></ul></blockquote><hr><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://epsilonzyj.github.io/posts/63fed347.html">图神经网络简介 | An Introduction to GNN</a></li><li><a href="https://www.bilibili.com/video/BV1Vw411R7Fj?spm_id_from=333.788.videopod.episodes&amp;vd_source=2e36fae16810615c2d859efc03aef1c4">图卷积神经网络（GCN）的数学原理详解——谱图理论和傅立叶变换初探-Bilibili</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Mac远程连接Windows WSL</title>
      <link href="/posts/e4aef2ca.html"/>
      <url>/posts/e4aef2ca.html</url>
      
        <content type="html"><![CDATA[<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>Windows与MacBook，且处于同一局域网下。安装WSL2的过程略。</p><h2 id="WSL中的配置"><a href="#WSL中的配置" class="headerlink" title="WSL中的配置"></a>WSL中的配置</h2><h3 id="安装配置SSH服务"><a href="#安装配置SSH服务" class="headerlink" title="安装配置SSH服务"></a>安装配置SSH服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install openssh-server</span><br></pre></td></tr></table></figure><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p>将注释的内容全部取消注释：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Port 22</span><br><span class="line">AddressFamily any</span><br><span class="line">ListenAddress 0.0.0.0</span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure><h3 id="启动SSH服务"><a href="#启动SSH服务" class="headerlink" title="启动SSH服务"></a>启动SSH服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ssh-keygen -A</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> /usr/sbin/service ssh start</span><br></pre></td></tr></table></figure><h2 id="Windows的配置"><a href="#Windows的配置" class="headerlink" title="Windows的配置"></a>Windows的配置</h2><p>由于电脑可能安装了杀毒软件，会导致Windows Defender中防火墙设置被篡改而使得部分功能变为灰色，从而不可用，因此使用Power Shell进行配置。注意，一定要使用管理员身份打开，否则会因为权限不足而无法完成操作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">New-NetFirewallRule -Name sshd -DisplayName <span class="string">&#x27;sshd for WSL&#x27;</span> -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22</span><br></pre></td></tr></table></figure><h2 id="端口转发"><a href="#端口转发" class="headerlink" title="端口转发"></a>端口转发</h2><p>使用管理员身份在Power Shell中运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">​​​​​​​netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=[PORT] connectaddress=[IP] connectport=[PORT]</span><br><span class="line"><span class="comment"># PORT 为你设置的端口，我这里为3333</span></span><br><span class="line"><span class="comment"># IP地址为wls linux子系统的ip地址，可通过ifconfig查看</span></span><br></pre></td></tr></table></figure><h2 id="使用Mac远程连接"><a href="#使用Mac远程连接" class="headerlink" title="使用Mac远程连接"></a>使用Mac远程连接</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh xxx@xxx.xxx.xxx.xxx -p 22</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Env </category>
          
          <category> WSL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WSL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zotero：A great research assistant</title>
      <link href="/posts/2286d822.html"/>
      <url>/posts/2286d822.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Research </tag>
            
            <tag> Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICS实验随笔</title>
      <link href="/posts/67ad26dd.html"/>
      <url>/posts/67ad26dd.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于右移的问题"><a href="#关于右移的问题" class="headerlink" title="关于右移的问题"></a>关于右移的问题</h2><h3 id="问题发现"><a href="#问题发现" class="headerlink" title="问题发现"></a>问题发现</h3><p>在上机进行ICS实验中，有一个函数要求产生highbit到lowbit全为1的数字。一个相当简单的想法是直接将0xFFFFFFFF右移hinghbit+1位后再 移回来，再取反，便可以得到第0位到第highbit位的数字，然后再进行后续操作。</p><p>然而在这个问题中，遇到了一些问题，在此记录一下。</p><h4 id="左右移的位数为负数"><a href="#左右移的位数为负数" class="headerlink" title="左右移的位数为负数"></a>左右移的位数为负数</h4><p><strong>负数移位</strong>（如x &gt;&gt; -1）属于<strong>未定义行为</strong>，不同平台和编译器会产生不同结果，部分会在编译阶段报错，而在一些特定运算下，如x &gt;&gt; (m-n)中，可能会因为某些特殊赋值而出现负数，应当进行避免。</p><h4 id="右移溢出"><a href="#右移溢出" class="headerlink" title="右移溢出"></a>右移溢出</h4><p>根据上面函数所需要求，一开始写出如下的函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((<span class="number">0xFFFFFFFF</span> &gt;&gt; (highbit + <span class="number">1</span>)) &lt;&lt; (highbit + <span class="number">1</span>));</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而，在测试调试时发现，当highbit==31时，函数值则会有问题。溯源后发现，highbit+1==32时，high的值一直为0xFFFFFFFF，并不符合预期的0x0。起初以为是自己逻辑有问题，于是进行了一下测试，首先把highbit+1替换为32：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((<span class="number">0xFFFFFFFF</span> &gt;&gt; <span class="number">32</span>) &lt;&lt; <span class="number">32</span>);</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而出乎意料的是，此时high的值变为正常的0x0，此时也开始报了warning，但显然不看warning是一个好习惯（经典笑话，笑）。则考虑到32与highbit+1的差异性，一开始没考虑常量和变量的问题，以为是32是unsigned类型而另一个是int类型，由于数据类型的问题可能会存在一些区别，虽然感觉这种思路似乎很离谱，且并没有什么道理可言，但是还是测试了一下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((<span class="number">0xFFFFFFFF</span> &gt;&gt; (<span class="type">unsigned</span>)(highbit + <span class="number">1</span>)) &lt;&lt; (<span class="type">unsigned</span>)(highbit + <span class="number">1</span>));</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个时候high又变成了0xFFFFFFFF，这有些出乎意料，一时就没什么头绪，不知道有什么问题，于是就开始怀疑编译器是否有编译优化的问题了。</p><h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><p>经过查找资料，发现右移时右侧操作数大于或等于左侧操作数的位数是属于未定义运算，而编译时确实存在一些编译优化。对于未定义行为，在编译时，分别发生了以下两件事：</p><ul><li>常量表达式：编译器在编译阶段进行常量求值，依据标准未定义的情况下可能直接优化成 0。</li><li>变量表达式：运行时生成的移位指令通常只取移位数的低 5 位（对于 32 位整数而言），因而 32 的低 5 位为 0，实际移位相当于“右移 0 位”，结果保持原值 0xFFFFFFFF。</li></ul><p>当表达式写为 0xFFFFFFFF &gt;&gt; (highbit+1) 时，虽然 (highbit+1) 计算结果为 32，但由于其为运行时求值的变量表达式，编译器生成的汇编指令遵循硬件实际移位机制。</p><p>在许多 CPU 架构（例如 x86）中，用于右移操作的指令（如 SHR 指令）通常只取移位数的低 5 位（对于 32 位操作数），即实际移位数 = 给定移位数 mod 32。</p><p>当 (highbit+1) 等于 32 时，其低 5 位为 0，故实际移位操作相当于“右移 0 位”，因此结果仍为原值 0xFFFFFFFF。</p><p>这种处理方式与硬件实现有关，虽然在 C 语言标准中两种写法的行为都是未定义的，但实际运行时硬件指令的“模 32”特性使得结果不同。</p><p>因此，对于上述问题，一种解决办法就是分开写，即移两次位。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((((<span class="number">0xFFFFFFFF</span> &gt;&gt; highbit) &gt;&gt; <span class="number">1</span>) &lt;&lt; highbit) &lt;&lt; <span class="number">1</span>);</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="寄存器保护"><a href="#寄存器保护" class="headerlink" title="寄存器保护"></a>寄存器保护</h2><h3 id="问题发现-1"><a href="#问题发现-1" class="headerlink" title="问题发现"></a>问题发现</h3><p>在某次实验中，根据实验要求，需要使用汇编语言编写其中的部分函数。当然这个实验主要的内容实在优化炫技（划掉），不过在实验中遇到了一个很奇怪的问题。</p><p><em>注意：实验环境为Visual Studio 2022，编译采用x86架，构。其中在生成依赖项时需要对项目进行设置，对项目名称右击后点击生成依赖项下的生成子定义，并勾选masm，这样可以实现汇编和C混合编程。其余均为默认设置。感兴趣的同学可以根据以上内容进行复现。</em></p><p>在实验中需要在Debug版本和Release版本下分别进行程序运行速度的比较，编译阶段并没有问题，Debug版本可以正常运行。但是在Release版本中，当运行即将退出主函数时，抛出了异常。（由于时间较久远，且笔者较懒，没有复现的图片，了解一下原理即可）</p><h3 id="问题解决-1"><a href="#问题解决-1" class="headerlink" title="问题解决"></a>问题解决</h3><p>根据反汇编调试，在return 0处打断点，在查看反汇编后，发现在退出主函数之前进行了一些安全检查，其中一项就是进行寄存器的检查。而在本次场景中，因为Release版本优化程度较高，在程序编译时默认不会使用到寄存器ebx，因而在程序退出时会检查ebx是否进行改动。很遗憾，笔者水平较差，在编写x86汇编的时候用到了ebx并且没有进行寄存器保护，因此由于篡改寄存器导致程序崩溃。</p><p>以下给出原始的代码（后期模拟出来的）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">.686P</span><br><span class="line">.model flat, c</span><br><span class="line">printf proto c :ptr sbyte, :vararg</span><br><span class="line">; includelib  libcmt.lib</span><br><span class="line">includelib  legacy_stdio_definitions.lib </span><br><span class="line"></span><br><span class="line">student  struct</span><br><span class="line">    sname   db   8 dup(0)</span><br><span class="line">    sid     db   11 dup(0)</span><br><span class="line">    align   2    ; 指明对齐方式，汇编语言默认是紧凑存放</span><br><span class="line">                 ; 可以实验一下，去掉对齐方式伪指令的结果</span><br><span class="line">    scores  dw   8  dup(0)</span><br><span class="line">    average dw   0</span><br><span class="line">student   ends</span><br><span class="line"></span><br><span class="line">.data</span><br><span class="line">   lpfmt  db &quot;%s %s %d %d&quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_string  db &quot;%s  &quot;,0</span><br><span class="line">   lpfmt_num  db &quot;%d  &quot;,0</span><br><span class="line">   lpfmt_size    db  &quot;size of struct %d  &quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_offset  db  0dh,0ah,&quot;offset of scores %d  &quot;,0dh,0ah,0</span><br><span class="line"></span><br><span class="line">.code</span><br><span class="line">;  显示学生信息</span><br><span class="line">;  sptr 学生数组的首地址</span><br><span class="line">;  num  学生人数</span><br><span class="line">;  注意， printf 中会用到一些寄存器，也没有保护</span><br><span class="line">;         即执行 printf前后，一个寄存器中的内容发生变化</span><br><span class="line"></span><br><span class="line">computeAverageScoreAsmOpt proc sptr: dword, num: dword</span><br><span class="line">    local s: dword</span><br><span class="line"></span><br><span class="line">    mov edx, sptr</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov ecx, 0</span><br><span class="line"></span><br><span class="line">LOOPI:</span><br><span class="line">                    ;int sum = 0</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov s, eax</span><br><span class="line">                    ;s[i].scores[0]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 014h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[1]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 016h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[2]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 018h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[3]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ah</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[4]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ch</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[5]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Eh</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[6]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 020h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx  </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[7]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 022h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;sum / 8</span><br><span class="line">    mov eax, s</span><br><span class="line">    sar eax, 3</span><br><span class="line">                    ;s[i].average = sum/8</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    mov word ptr[edx+ebx+024h], ax</span><br><span class="line">                    ;i ++</span><br><span class="line">    inc ecx</span><br><span class="line">                    ;i == num ?</span><br><span class="line">    cmp ecx,num</span><br><span class="line">    jne LOOPI</span><br><span class="line"></span><br><span class="line">    ret</span><br><span class="line">computeAverageScoreAsmOpt endp</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>寄存器保护其实是一个非常简单非常常见的想法，但是由于在编写函数时，笔者原想要减少指令以换取（似乎）更快的速度，同时也是懒得写（划掉，怎么能把最重要的原因写出来），因此并没有做任何寄存器保护的办法。</p><p>接下来就简单讲一下寄存器保护思路：</p><ul><li>对于eax等寄存器，直接压入栈中即可</li><li>对于ebp，将ebp压入栈中，同时将esp赋值给ebp（即让ebp指向原始栈顶，方便后续esp和ebp等在函数返回时/后恢复原来的值）</li><li>对于esp，esp减少一部分值，即指向更小的地址，建立一个新栈</li></ul><p>而在实验中，由于并没有（直接）使用ebp、esp等寄存器，因而就对这两个不做特殊的保护了，而将其余寄存器全部压入栈中，在函数结束时再弹出。由于懒（划掉），笔者将所有的可能用到的寄存器均压入了栈中，而不是将此函数中用到的寄存器压入栈中（反正结果对了就行了，划掉）。</p><p>以下是修改后的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line">.686P</span><br><span class="line">.model flat, c</span><br><span class="line">printf proto c :ptr sbyte, :vararg</span><br><span class="line">; includelib  libcmt.lib</span><br><span class="line">includelib  legacy_stdio_definitions.lib </span><br><span class="line"></span><br><span class="line">student  struct</span><br><span class="line">    sname   db   8 dup(0)</span><br><span class="line">    sid     db   11 dup(0)</span><br><span class="line">    align   2    ; 指明对齐方式，汇编语言默认是紧凑存放</span><br><span class="line">                 ; 可以实验一下，去掉对齐方式伪指令的结果</span><br><span class="line">    scores  dw   8  dup(0)</span><br><span class="line">    average dw   0</span><br><span class="line">student   ends</span><br><span class="line"></span><br><span class="line">.data</span><br><span class="line">   lpfmt  db &quot;%s %s %d %d&quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_string  db &quot;%s  &quot;,0</span><br><span class="line">   lpfmt_num  db &quot;%d  &quot;,0</span><br><span class="line">   lpfmt_size    db  &quot;size of struct %d  &quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_offset  db  0dh,0ah,&quot;offset of scores %d  &quot;,0dh,0ah,0</span><br><span class="line"></span><br><span class="line">.code</span><br><span class="line">;  显示学生信息</span><br><span class="line">;  sptr 学生数组的首地址</span><br><span class="line">;  num  学生人数</span><br><span class="line">;  注意， printf 中会用到一些寄存器，也没有保护</span><br><span class="line">;         即执行 printf前后，一个寄存器中的内容发生变化</span><br><span class="line"></span><br><span class="line">computeAverageScoreAsmOpt proc sptr: dword, num: dword</span><br><span class="line">    local s: dword</span><br><span class="line"></span><br><span class="line">    push eax</span><br><span class="line">    push ebx</span><br><span class="line">    push ecx</span><br><span class="line">    push edx</span><br><span class="line">    push esi</span><br><span class="line">    push edi</span><br><span class="line"></span><br><span class="line">    mov edx, sptr</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov ecx, 0</span><br><span class="line"></span><br><span class="line">LOOPI:</span><br><span class="line">                    ;int sum = 0</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov s, eax</span><br><span class="line">                    ;s[i].scores[0]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 014h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[1]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 016h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[2]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 018h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[3]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ah</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[4]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ch</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[5]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Eh</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[6]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 020h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx  </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[7]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 022h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;sum / 8</span><br><span class="line">    mov eax, s</span><br><span class="line">    sar eax, 3</span><br><span class="line">                    ;s[i].average = sum/8</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    mov word ptr[edx+ebx+024h], ax</span><br><span class="line">                    ;i ++</span><br><span class="line">    inc ecx</span><br><span class="line">                    ;i == num ?</span><br><span class="line">    cmp ecx,num</span><br><span class="line">    jne LOOPI</span><br><span class="line"></span><br><span class="line">    pop edi</span><br><span class="line">    pop esi</span><br><span class="line">    pop edx</span><br><span class="line">    pop ecx</span><br><span class="line">    pop ebx</span><br><span class="line">    pop eax</span><br><span class="line"></span><br><span class="line">    ret</span><br><span class="line">computeAverageScoreAsmOpt endp</span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Course </category>
          
          <category> ICS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HUST </tag>
            
            <tag> ICS </tag>
            
            <tag> C/C++ </tag>
            
            <tag> Assembly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUST仓库</title>
      <link href="/posts/c723aac6.html"/>
      <url>/posts/c723aac6.html</url>
      
        <content type="html"><![CDATA[<p>本博客用于记录作者本科时期的课程实验或课程设计。</p><ol><li><a href="https://github.com/EpsilonZYJ/HUST-Material/tree/main/计算机科学与技术/IAC语言程序设计/code">C语言程序设计实验</a></li><li><a href="https://github.com/EpsilonZYJ/HUST-Material/tree/main/计算机科学与技术/IA计算思维">计算思维</a></li><li><a href="https://github.com/EpsilonZYJ/DataStructureExp">数据结构实验</a></li><li><a href="https://github.com/EpsilonZYJ/Sudoku">基于SAT问题的数独求解（程序综合课程设计）</a></li><li><a href="https://github.com/EpsilonZYJ/AlgorithmExperiment">算法设计实验</a></li><li><a href="https://github.com/EpsilonZYJ/CppExperiement">C++实验</a></li><li><a href="https://github.com/EpsilonZYJ/Digital-Circuit-Experiment">数字电路与逻辑设计实验</a></li><li><a href="https://github.com/EpsilonZYJ/IntroductionToAI">人工智能导论课程作业</a></li><li><a href="https://github.com/EpsilonZYJ/DataBaseExperiement/tree/main">数据库系统原理实验</a></li><li><a href="https://github.com/EpsilonZYJ/Introduction-To-Machine-Learning/tree/main">机器学习导论实验与课设</a></li><li><a href="https://github.com/EpsilonZYJ/HUST-Java">Java实验</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> HUST </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HUST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN经典模型 | GNN Basic Models</title>
      <link href="/posts/4a93988c.html"/>
      <url>/posts/4a93988c.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
            <tag> GNN </tag>
            
            <tag> GNN With Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图神经网络简介 | An Introduction to GNN</title>
      <link href="/posts/63fed347.html"/>
      <url>/posts/63fed347.html</url>
      
        <content type="html"><![CDATA[<h2 id="图的表示"><a href="#图的表示" class="headerlink" title="图的表示"></a>图的表示</h2><p>对于图，有</p><ul><li>V：Vertex(or node) attributes</li><li>E：Edge(or link) attributes and directions</li><li>U：Global(or master node) attributes</li></ul><p>分别可以使用向量embedding来进行表示。</p><h3 id="图片转化为图"><a href="#图片转化为图" class="headerlink" title="图片转化为图"></a>图片转化为图</h3><p>将第$(i, j)$位置的像素映射为第$(i, j)$位置的节点（可以将节点按照类似像素排序的规则排序，即每行每列个数均固定。对于图采用邻接矩阵进行存储。对于某个像素的邻居，为上下左右和对角线上至多8个节点，进而表示为图</p><h3 id="文本转化为图"><a href="#文本转化为图" class="headerlink" title="文本转化为图"></a>文本转化为图</h3><p>可以将每一个词表示为一个顶点，上一个词和下一个词之间有一个有向的边。</p><h3 id="其它问题转化为图"><a href="#其它问题转化为图" class="headerlink" title="其它问题转化为图"></a>其它问题转化为图</h3><h4 id="分子转化为图"><a href="#分子转化为图" class="headerlink" title="分子转化为图"></a>分子转化为图</h4><p>可以使每个原子表示一个节点，原子之间有边相连则表示一条边。</p><h4 id="社交网络图"><a href="#社交网络图" class="headerlink" title="社交网络图"></a>社交网络图</h4><p>如同一场景中出现过的人，可以将其对应的节点连一条边。</p><h4 id="引用图"><a href="#引用图" class="headerlink" title="引用图"></a>引用图</h4><p>如文章A引用文章B，则可以连接A指向B的有向边。</p><h3 id="图机器学习的任务"><a href="#图机器学习的任务" class="headerlink" title="图机器学习的任务"></a>图机器学习的任务</h3><ul><li>图层面的任务，如识别有几个环进而对图进行分类</li><li>顶点层面的任务，如对两个复杂的社交网络图，对其中以两个人为核心的两个不同阵营，判断每个人属于哪个阵营</li><li>边层面的任务，预测边的属性</li></ul><h3 id="机器学习用于图上的挑战"><a href="#机器学习用于图上的挑战" class="headerlink" title="机器学习用于图上的挑战"></a>机器学习用于图上的挑战</h3><p><em>（此处机器学习特指神经网络）</em></p><p>图上有四种信息：顶点的属性、边的属性、全局信息和连接性。前三种均可用向量表示，但连接性表示比较困难。</p><p>一种朴素想法是使用邻接矩阵，连接则用1表示，未连接用0表示。但是这种方法表示的矩阵会非常大。如果使用稀疏矩阵，在存储上可行，但要高效计算或者在GPU上计算较为困难。此外，由于交换任意行和任意列不会产生影响，这意味着交换任意行或任意列后的图放进神经网络，出来的结果应该与原先相同。</p><p>因此我们可以采用如下的形式。使用一个向量来表示节点，每个节点的属性使用一个标量来表示；用一个向量来表示边，每个边的属性也使用一个标量来表示；使用邻接链表来表示连接性。如下图所示：</p><p><img src="/img/GNN1.png" alt=""></p><h2 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h2><p>图神经网络是对图上所有的属性，包括顶点、边、上下文等，进行一个可以优化的变换，而这个变换可以保持住图的对称信息（顶点重新排序后结果不变）。此处采用信息传递的神经网络。图神经网络输入为图，输出也是图，对图的顶点、边和全局上下文进行变换，但不对连接性产生改变。</p><h3 id="最简单的GNN"><a href="#最简单的GNN" class="headerlink" title="最简单的GNN"></a>最简单的GNN</h3><p>使用上文所提出的数据结构，对全局上下问信息、顶点和边分别建立多层感知机，从而获得一个新的图。</p><h3 id="GNN-Predictions-by-Pooling-Information"><a href="#GNN-Predictions-by-Pooling-Information" class="headerlink" title="GNN Predictions by Pooling Information"></a>GNN Predictions by Pooling Information</h3><p>若没有顶点点向量，则使用汇聚/池化（Pooling）来得到节点向量。将与该节点的边的向量和全局向量一起相加得到新的向量（此处假设全局向量和边的向量维度相同，如果不同则需要进行投影），得到的新的向量作为节点的向量。最后进入全连接层得到顶点的输出。</p><p><img src="/img/GNN2.png" alt=""></p><p>同样的，如果只有顶点向量和全局向量，则将顶点向量和全局向量汇聚到边上，然后进入边向量的输出层，最后得到边的输出。</p><p>如果没有全局向量，则可以把所有的顶点向量加起来得到一个全局向量，并进入全局的输出层得到全局的输出。</p><p>因此最简单的GNN为如下的结构：给定输入的图，进入一系列的GNN层，每个层有三个MLP对应三种不同的属性。最后输出得到保持整个图结构的输出，但里面所有的属性发生了变化，而根据要对哪个属性做预测则添加合适的输出层，如果有信息缺失的话则加入合适的汇聚层即可。这样就可以完成一个简单的预测。</p><p><img src="/img/GNN3.png" alt=""></p><p>然而这种方式有所欠缺，因为将三种属性割裂开，并不能有效地融合和利用整个图的信息，顶点与边分开单独计算。因此需要一种其它方式。</p><h3 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h3><p>在顶点输入MLP时，不再只是单纯输入顶点向量，而是采用将顶点向量与此顶点连接的顶点的向量相加组成的向量输入MLP进行顶点的更新，即聚合步与更新步。当叠加很多层，可以实现顶点信息长距离的传递。</p><p>其中顶点周围距离为1的邻居成为1-近邻。上述步骤即$\rho_{V_n \rightarrow V_n}$</p><p><img src="/img/GNN4.png" alt=""></p><h3 id="Learning-edge-representation"><a href="#Learning-edge-representation" class="headerlink" title="Learning edge representation"></a>Learning edge representation</h3><p>对于如何将顶点的信息传递给边，将边的信息传递给顶点，有如下的方式：</p><ul><li><p>首先通过$\rho_{V_n \rightarrow E_n}$将顶点的向量传递给边，若维度不同则进行投影再传递或使用concat将两个向量并在一起。</p></li><li><p>然后再进行$\rho_{E_n \rightarrow V_n}$将边的信息再传递给顶点。</p></li></ul><p>也可以反过来：</p><ul><li><p>先做顶点的更新$\rho_{E_n \rightarrow V_n}$</p></li><li><p>再做边的更新$\rho_{V_n \rightarrow E_n}$</p></li></ul><p>以上两种方法会出现不同的结果，且没有孰优孰劣之分。此外还可以进行交替更新。</p><p><img src="/img/GNN5.png" alt=""></p><h3 id="Adding-global-representations"><a href="#Adding-global-representations" class="headerlink" title="Adding global representations"></a>Adding global representations</h3><p><em>全局信息的更新</em></p><p>可以增加一个虚拟的顶点，称为<strong>master node</strong>或<strong>context vector</strong>，这个顶点与所有的V和E里面的内容均相连。当把顶点的信息汇聚给边的时候，会把U的信息也汇聚过来；当汇聚顶点时，也会把U汇聚过来；当汇聚U的时候，会把顶点和边的信息一起汇聚到U上，再做更新。</p><p><img src="/img/GNN6.png" alt=""></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>GNN对超参数较为敏感，通常参数有四种：网络有多少层，每个属性的嵌入（embedding）维度有多高，汇聚（pooling）的操作是什么类型（最大值、均值等），怎么做信息传递（是否做信息传递，哪些属性之间做信息传递）。</p><h2 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h2><h3 id="采样（batching）"><a href="#采样（batching）" class="headerlink" title="采样（batching）"></a>采样（batching）</h3><p>如果对整个图进行计算，可能最终中间结果会非常大，因而要进行采样。常见的采样方法如下：</p><ol><li>随机采样一些点，将这些点点最近的邻居找出来，在计算时在这个子图上进行计算；</li><li>随机游走：随机在图上找一条边，沿着这条边走到下一个节点，沿着这个图随机走，规定最多随机走多少步，从而得到一个子图；</li><li>结合上面两种，随机走三步，然后把这三步中的每个邻居的节点全部找出来；</li><li>随机选一个点，找出第1近邻，2近邻…k近邻，即做宽度遍历得到子图。</li></ol><h3 id="Inductive-bias"><a href="#Inductive-bias" class="headerlink" title="Inductive bias"></a>Inductive bias</h3><p>此模型假设了在神经网络中保持了图的对称性。</p><h3 id="汇聚操作的比较"><a href="#汇聚操作的比较" class="headerlink" title="汇聚操作的比较"></a>汇聚操作的比较</h3><p>汇聚操作可以求和、求平均、求最大值，然而并没有哪种特别理想。</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://distill.pub/2021/gnn-intro/">Sanchez-Lengeling, et al., “A Gentle Introduction to Graph Neural Networks”, Distill, 2021.</a></li><li><a href="https://www.bilibili.com/video/BV1iT4y1d7zP">零基础多图详解图神经网络</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224W | Machine Learning with Graphs</title>
      <link href="/posts/c6b8d3b6.html"/>
      <url>/posts/c6b8d3b6.html</url>
      
        <content type="html"><![CDATA[<h2 id="传统基于特征的方法｜Traditional-Feature-base-Methods"><a href="#传统基于特征的方法｜Traditional-Feature-base-Methods" class="headerlink" title="传统基于特征的方法｜Traditional Feature-base Methods"></a>传统基于特征的方法｜Traditional Feature-base Methods</h2><h3 id="节点中心性｜Node-Centrality"><a href="#节点中心性｜Node-Centrality" class="headerlink" title="节点中心性｜Node Centrality"></a>节点中心性｜Node Centrality</h3><h4 id="特征向量中心度｜Eigenvector-centrality"><a href="#特征向量中心度｜Eigenvector-centrality" class="headerlink" title="特征向量中心度｜Eigenvector centrality"></a>特征向量中心度｜Eigenvector centrality</h4><p>衡量节点重要性的一种中心性指标，不仅考虑节点的直接连接数量，还考虑其相邻节点的重要性。特征向量中心性通过计算图的邻接矩阵的主导特征向量（最大特征值对应的特征向量）来确定，每个节点的中心性得分与其邻居的得分成正比。这种方法可用于识别网络中影响力较大的节点，如社交网络中的关键人物或传播网络中的核心节点。</p><p>思想：对于节点v，如果v被重要的邻居节点$u\in N(v)$包围，那么这个节点就相对重要</p><script type="math/tex; mode=display">c_v = \frac{1}{\lambda} \sum_{u \in N(v)}c_u \longleftrightarrow \lambda \mathbf{c} = \mathbf{Ac}</script><p>其中：</p><ul><li>$\lambda$是某个正常量</li><li>A是邻接矩阵，且若$u\in N(v), A_{uv} = 1$</li><li>c为中心性向量(Centrality vector)</li></ul><p>最大的特征值$\lambda_{max}$总是正数且唯一</p><p>主导特征向量 $C_max$用于衡量图的中心性。</p><h4 id="中介中心性｜Betweenness-Centrality"><a href="#中介中心性｜Betweenness-Centrality" class="headerlink" title="中介中心性｜Betweenness Centrality"></a>中介中心性｜Betweenness Centrality</h4><p>衡量一个节点在图中充当其他节点之间“桥梁”作用的程度。具体而言，它表示该节点出现在所有最短路径中的频率，数值越高，说明该节点在信息传递或网络流动中的重要性越大。</p><p>思想：如果许多的最短路径都经过这个节点，那么这个节点就相对重要</p><p>公式具体如下：</p><script type="math/tex; mode=display">c_v = \sum_{s\neq v \neq t}\frac{在s和t之间包含节点v的最短路径数}{在s和t中的最短路径数}</script><h4 id="接近中心性｜Closeness-Centrality"><a href="#接近中心性｜Closeness-Centrality" class="headerlink" title="接近中心性｜Closeness Centrality"></a>接近中心性｜Closeness Centrality</h4><p>衡量一个节点与图中所有其他节点的平均最短路径距离的倒数。数值越高，表示该节点更容易接触到图中的其他节点，信息传播效率更高，通常用于评估节点在网络中的传播能力。</p><p>思想：如果一个节点到所有的节点的距离最小，那么这个节点相对重要</p><p>公式如下：</p><script type="math/tex; mode=display">c_v = \frac{1}{\sum_{u\neq v}u和v之间最短路径的长度}</script><h4 id="聚类系数｜Clustering-Coefficient"><a href="#聚类系数｜Clustering-Coefficient" class="headerlink" title="聚类系数｜Clustering Coefficient"></a>聚类系数｜Clustering Coefficient</h4><p>针对单个节点，定义为该节点的邻居之间实际存在的边数与可能存在的最大边数之比</p><p>公式：</p><script type="math/tex; mode=display">e_v = \frac{v节点的邻居节点之间的边数}{C_{k_v}^{2}}</script><p>其中，分母表示节点v的邻居节点两两相连得到的边数。</p><h4 id="图元频率向量｜Graphlet-Degree-Vector"><a href="#图元频率向量｜Graphlet-Degree-Vector" class="headerlink" title="图元频率向量｜Graphlet Degree Vector"></a>图元频率向量｜Graphlet Degree Vector</h4><h5 id="图元"><a href="#图元" class="headerlink" title="图元"></a>图元</h5><p>图元（Graphlets）：指小规模的、无方向或有方向的连通子图，通常用于分析复杂网络的局部结构。图元可以帮助识别网络的模式、节点的结构角色，并用于特征提取、网络比较和生物网络分析。</p><ul><li>度数计算的是节点接触多少条边</li><li>聚类系数计算的是节点接触多少个三角形</li><li>GDV计算的是节点接触多少个图元</li></ul><h5 id="图元度向量的作用"><a href="#图元度向量的作用" class="headerlink" title="图元度向量的作用"></a>图元度向量的作用</h5><p>考虑由2到5个节点组成的图元时：</p><ul><li>通过分析2到5个节点的所有可能连通子图，构建一个包含73维度的向量，该向量被视为节点的特征签名，用于描述其局部拓扑结构。</li><li>该向量不仅考虑节点的直接邻居，还能捕捉其在最多4跳范围内的互连关系</li><li>图元度向量提供了比单纯的节点度（Degree）或聚类系数（Clustering Coefficient）更丰富的结构信息</li></ul><h3 id="链接级预测任务｜Link-Level-Prediction-Task"><a href="#链接级预测任务｜Link-Level-Prediction-Task" class="headerlink" title="链接级预测任务｜Link-Level Prediction Task"></a>链接级预测任务｜Link-Level Prediction Task</h3><p>在此任务中，是基于现有的链接预测新的链接。在测试阶段，对所有的未连接节点对进行排序，并且预测最高的K个节点对。<strong>关键是设计一对节点的特征</strong>。</p><p>两种链接预测的形式：</p><ul><li>随机链接缺失边：随机移除一些链接，并且尝试去预测它们</li><li>随时间进行链接：<ul><li>对于给定的图$G[t_0,t_0’]$，图的边在时间$t_0’$之前，需要产生一个对在原图中不存在的链接的排序的表，其中这些链接是被预测在$G[t_1,t_1’]$中出现</li><li>评估：<ul><li>$n=|E_{new}|$：在测试阶段$[t_1,t_1’]$新出现的边</li><li>取出L中前n个元素并且计算正确的边数</li></ul></li></ul></li></ul><h4 id="通过相似性的链接预测"><a href="#通过相似性的链接预测" class="headerlink" title="通过相似性的链接预测"></a>通过相似性的链接预测</h4><p>方法：</p><ol><li>对于每一对节点$(x,y)$，计算相似性分数$c(x,y)$<ul><li>例如，可以计算相同的邻居数目</li></ul></li><li>按照相似性分数降序排列节点对</li><li>预测分数最高的n对作为新的链接</li><li>查看这些链接中哪些真实存在于图$G[t_1,t_1’]$中</li></ol><h4 id="基于距离的特征｜Distance-Based-Features"><a href="#基于距离的特征｜Distance-Based-Features" class="headerlink" title="基于距离的特征｜Distance-Based Features"></a>基于距离的特征｜Distance-Based Features</h4><p>使用两个节点之间最短路径的长度</p><p>缺陷：不能捕获领域重叠的信息</p><h4 id="局部邻域重叠｜Local-Neighborhood-Overlap"><a href="#局部邻域重叠｜Local-Neighborhood-Overlap" class="headerlink" title="局部邻域重叠｜Local Neighborhood Overlap"></a>局部邻域重叠｜Local Neighborhood Overlap</h4><p>捕获两个节点$v_1,v_2$之间共享的邻居节点</p><ul><li>Common neighbors: $|N(v_1)\cap N(v_2)|$</li><li>Jaccard’s coefficient: $\frac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}$</li><li>Adamic-Adar index: $\sum_{u\in N(v_1)\cap N(v_2)}\frac{1}{log(k_u)}, k_u: degrees\quad of\quad node\quad u$</li></ul><p>缺陷：当两个节点没有任何公共邻居时总为0，但是这两个节点在未来仍有可能被链接</p><h4 id="全局邻域重叠｜Global-Neighborhood-Overlap"><a href="#全局邻域重叠｜Global-Neighborhood-Overlap" class="headerlink" title="全局邻域重叠｜Global Neighborhood Overlap"></a>全局邻域重叠｜Global Neighborhood Overlap</h4><p>Katz index：计算一个给定的节点对中所有长度的路径的数目</p><p>方法：<strong>使用邻接矩阵的幂可以计算各种长度的路径数</strong></p><ul><li>$A_{uv}$表示节点u和v之间长度为1的路径</li><li>$A_{uv}^{l}$表示节点u和v之间长度为l的路径</li></ul><p>对于$v_1,v_2$的Katz index计算如下：</p><script type="math/tex; mode=display">S_{v_1v_2} = \sum_{l=1}^{\infty}\beta^{l}A_{v_1v_2}^{l}, 0<\beta<1:discount\quad factor</script><p>Katz index矩阵闭式计算：</p><script type="math/tex; mode=display">\mathbf{S} = \sum_{i=1}^{\infty}\beta^i\mathbf{A}^i = (\mathbf{I} - \beta\mathbf{A})^{-1}-\mathbf{I}</script><h3 id="图级别的特征｜Graph-Level-Features"><a href="#图级别的特征｜Graph-Level-Features" class="headerlink" title="图级别的特征｜Graph-Level Features"></a>图级别的特征｜Graph-Level Features</h3><h4 id="背景：核方法｜Kernel-Methods"><a href="#背景：核方法｜Kernel-Methods" class="headerlink" title="背景：核方法｜Kernel Methods"></a>背景：核方法｜Kernel Methods</h4><p>核方法被广泛应用于传统机器学习中的图级别预测任务。</p><p>核心思想: 设计核函数而非特征向量</p><ul><li>核函数$K(G, G’)\in R$衡量数据之间的相似性</li><li>核矩阵$K=(K(G, G’))_{G,G’}$必须始终保持半正定(即具有非负特征值)</li><li>存在特征表示$\Phi(·)$使得$K(G,G’)=\Phi(G)^T\Phi(G’)$</li><li>一旦定义好核函数，就可以直接使用现成的机器学习模型(如核支持向量机)进行预测</li></ul><h4 id="图核｜Graph-Kernels"><a href="#图核｜Graph-Kernels" class="headerlink" title="图核｜Graph Kernels"></a>图核｜Graph Kernels</h4><p>核心思想是设计核函数来衡量图之间的相似性，而不是直接使用特征向量。图核方法允许在不显式构造高维特征向量的情况下，利用核函数将图映射到一个高维特征空间，并应用标准的机器学习模型（如支持向量机 SVM）进行预测。</p><ul><li><a href="#图元核graphlet-kernel">Graphlet Kernel</a></li><li><a href="#weisfeiler-lehman-kernel">Weisfeiler-Lehman Kernel</a></li><li>Random-walk kernel</li><li>Shortest-path graph kernel</li></ul><h4 id="词袋｜Bag-of-Words"><a href="#词袋｜Bag-of-Words" class="headerlink" title="词袋｜Bag-of-Words"></a>词袋｜Bag-of-Words</h4><p>为图设计特征向量$\phi (G)$，一种方法就是设计词袋（Bag-of-Words，BoW）。</p><p>词袋（BoW）：对于文本，直接对出现的单词进行计数并作为其特征，其中并没有考虑顺序的因素。</p><p>对于拓展到图上的朴素的思想就是将节点视作单词。</p><p>e.g.对于向量[1, 4, 0]，可以表示为度为1，2，3的节点分别有1，4，0个。</p><h4 id="图元核｜Graphlet-Kernel"><a href="#图元核｜Graphlet-Kernel" class="headerlink" title="图元核｜Graphlet Kernel"></a>图元核｜Graphlet Kernel</h4><p>核心思想：计算在图中的不同图元的数目，将得到的特征向量记为$\mathbf{f}_G$</p><p>对给定的图$G$，以及一个图元表</p><script type="math/tex; mode=display">G_{k}=(g_{1}, g_{2},...,g_{n_k})</script><p>我们将图元计数得到的向量$\mathbf{f}_{G}\in R^{n_k}$定义为</p><script type="math/tex; mode=display">(\mathbf{f}_G)_i = (g_i \subseteq G),i = 1,2,...,n_k</script><p>对于给定的两个图G,G’，图元核可以通过以下的方式进行计算：</p><script type="math/tex; mode=display">K(G,G') = \mathbf{f}_{G}^{T}\mathbf{f}_{G'}</script><p><strong>问题：</strong>如果G和G’拥有不同的大小，那么值将会有偏移</p><p><strong>解决方法：</strong>将每个特征向量进行归一化</p><script type="math/tex; mode=display">\mathbf{h}_{G} = \frac{\mathbf{f}_{G}}{Sum(\mathbf{f}_{G})}</script><script type="math/tex; mode=display">K(G, G') = \mathbf{h}_{G}^{T}\mathbf{h}_{G'}</script><p><strong>缺陷：</strong>计算图元开销很大。对于计算一个大小为n的图中大小为k的图元，使用枚举法需要$n^k$次。计算某图是否是另一个图的子图是NP难问题。但对于节点度数上限为$d$的图，计算大小为k的图元数目，现在存在一个$O(nd^{k-1})$的算法。</p><h4 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h4><p>思想：使用领域的结构来丰富节点的表示。</p><p>从Bag of node degrees提取出的信息是1-邻域的信息，使用算法Color refinement可以提取多领域信息。</p><h5 id="颜色细化｜Color-Refinement"><a href="#颜色细化｜Color-Refinement" class="headerlink" title="颜色细化｜Color Refinement"></a>颜色细化｜Color Refinement</h5><p>输入：图G和节点集合V</p><ul><li>对于每个节点v，给定一个初试颜色$c^{(0)}(v)$</li><li>通过以下公式迭代更新节点颜色，其中HASH将不同的输入映射到不同颜色：</li></ul><script type="math/tex; mode=display">c^{(k+1)}(v) = HASH(\{c^{(k)}(v),\{c^{(k)}(u)\}_{u\in N(v)}\})</script><ul><li>通过K步颜色更新，$c^{K}(v)$将提取出K邻域的结构信息</li></ul><h5 id="WL-Kernel的优势"><a href="#WL-Kernel的优势" class="headerlink" title="WL Kernel的优势"></a>WL Kernel的优势</h5><p>WL kernel计算效率很高，因为每次更新的时间复杂度都是线性的。而计算核值的时候，只有同时出现在两个图中的颜色才会被追踪。因此，颜色数目最多不会超过节点数目。计算颜色的数目的时间复杂度为线性的。因此，总的时间复杂度为线性的。</p><h2 id="节点嵌入｜Node-Embedding"><a href="#节点嵌入｜Node-Embedding" class="headerlink" title="节点嵌入｜Node Embedding"></a>节点嵌入｜Node Embedding</h2><p>假设目前拥有一个图$G$，其中</p><ul><li>$V$是节点集</li><li>$A$是邻接矩阵（假设为0-1邻接矩阵）</li><li>为了简便起见，假设没有节点信息和额外信息</li><li>假设是无向图</li></ul><p>目标：在原始网络中的相似性<script type="math/tex">similarity(u, v)\approx\mathbf{z_{v}^{T}z_{u}}</script></p><p>节点嵌入学习：</p><ol><li>编码器将节点映射为嵌入</li><li>定义节点相似性函数（例如用于衡量原始网络中的相似性）</li><li>解码器DEC将嵌入映射为相似性分数</li><li>优化编码器的参数使得<script type="math/tex">similarity(u, v)\approx\mathbf{z_{v}^{T}z_{u}}</script></li></ol><h3 id="编码器｜Encoder"><a href="#编码器｜Encoder" class="headerlink" title="编码器｜Encoder"></a>编码器｜Encoder</h3><p>将每个节点映射到一个低维向量</p><script type="math/tex; mode=display">ENC(v) = \mathbf{z}_v</script><h4 id="最简单的编码"><a href="#最简单的编码" class="headerlink" title="最简单的编码"></a>最简单的编码</h4><p>最简单的编码方法：编码仅为一个嵌入的查找</p><script type="math/tex; mode=display">ENC(v) = \mathbf{z}_{v} = \mathbf{Z}\cdot v</script><script type="math/tex; mode=display">\mathbf{Z}\in R^{d\times |V|}：矩阵，每一列是一个节点嵌入（我们需要学习/优化的目标）</script><script type="math/tex; mode=display">v\in \mathrm{II}^{|V|}：指示向量，在一列中只有0和1，用于指示节点v</script><p>缺陷：参数过多，矩阵大小与节点数成正比。节点数较多的情况下运算很慢。</p><h3 id="相似性函数｜Similarity-function"><a href="#相似性函数｜Similarity-function" class="headerlink" title="相似性函数｜Similarity function"></a>相似性函数｜Similarity function</h3><p>指定在向量空间中的关系如何映射到原始网络中的关系</p><script type="math/tex; mode=display">similarity(u,v)\approx\mathbf{z}_{v}^{T}\mathbf{z}_{u}</script><h3 id="随机游走嵌入｜Random-Walk-Embedding"><a href="#随机游走嵌入｜Random-Walk-Embedding" class="headerlink" title="随机游走嵌入｜Random-Walk Embedding"></a>随机游走嵌入｜Random-Walk Embedding</h3><ol><li>估计使用随机游走策略的决策$R$，从节点$u$出发访问节点$v$的概率</li><li>优化嵌入方式来编码这些随机游走的概率</li></ol><p>优化的特征学习：</p><ul><li>给定图$G=(V,E)$</li><li>目标是学习一个映射$f:u\rightarrow R^{d}: f(u) = z_{u}$</li><li>对数相似性目标：<script type="math/tex; mode=display"> max_{f}\sum_{u\in V}logP(N_{R}(u)|\mathbf{z}_{u}),N_{R}(u)是使用策略R的节点u的一个邻居</script></li></ul><p>对于给定的节点u，我们想要学习在随机游走的邻域$N_{R}(u)$中可预测的节点的特征表示。</p><h4 id="随机游走优化算法"><a href="#随机游走优化算法" class="headerlink" title="随机游走优化算法"></a>随机游走优化算法</h4><ol><li>从每个节点u开始在图中使用随机游走策略R走固定长度的短路径</li><li>对于每个节点u得到可重集合$N_{R}(u)$（记录使用从u开始的随机游走策略访问的节点）</li><li>根据给定节点$u$预测邻域$N_{R}(u)$来优化嵌入</li></ol><p>等价的，优化目标变为</p><script type="math/tex; mode=display">L = \sum_{u\in V}\sum_{v\in N_{R}(u)}-log(P(v|\mathbf{z}_u))</script><p>目标：优化嵌入$z_u$以最大化随机游走的可能性</p><p>使用softmax参数化$P(v|\mathbf{z}_u)$:</p><script type="math/tex; mode=display">P(v|\mathbf{z}_u) = \frac{exp(\mathbf{z}_{u}^{T}\mathbf{z}_{v})}{\sum_{n\in V}exp(\mathbf{z}_{u}^{T}\mathbf{z}_{n})}</script><p>因此，最终的优化的目标为（时间复杂度为$O(|V|^{2})）$：</p><script type="math/tex; mode=display">L = \sum_{u\in V}\sum_{v\in N_{R}(u)}-log(\frac{exp(\mathbf{z}_{u}^{T}\mathbf{z}_{v})}{\sum_{n\in V}exp(\mathbf{z}_{u}^{T}\mathbf{z}_{n})})</script><h4 id="负采样｜Negative-Sampling"><a href="#负采样｜Negative-Sampling" class="headerlink" title="负采样｜Negative Sampling"></a>负采样｜Negative Sampling</h4><p>由于原来需要优化的目标时间复杂度过高，需要对目标进近似，此处采用负采样的方法：</p><script type="math/tex; mode=display">log(\frac{exp(\mathbf{z}_{u}^{T}\mathbf{z}_{v})}{\sum_{n\in V}exp(\mathbf{z}_{u}^{T}\mathbf{z}_{n})}) \\approx log(\sigma(\mathbf{z}_{u}^{T}\mathbf{z}_{v})) - \sum_{i=1}^{k}log(\sigma(\mathbf{z}_{u}^{T}\mathbf{z_{n_{i}}})),n_{i}\sim P_{V}</script><p>在此过程中，对于每个探测随机采样k个负节点</p><p>其中更大的k给出的估计越具有鲁棒性，同时也会在负样例中给出更高的偏移。</p><p>实际中k取5到20。</p><p>优化方法使用<strong>随机梯度下降法</strong>。</p><hr><h2 id="参考文献｜References"><a href="#参考文献｜References" class="headerlink" title="参考文献｜References"></a>参考文献｜References</h2><ol><li>CS224W-Video:<a href="https://www.bilibili.com/video/BV1RZ4y1c7Co">CN</a>,<a href="https://www.youtube.com/watch?v=JAB_plj2rbA">EN</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> Graph ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Graph ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>软件与工具集 | Fancy Tools</title>
      <link href="/posts/ebcd0e10.html"/>
      <url>/posts/ebcd0e10.html</url>
      
        <content type="html"><![CDATA[<h2 id="跨平台工具"><a href="#跨平台工具" class="headerlink" title="跨平台工具"></a>跨平台工具</h2><h3 id="AI工具"><a href="#AI工具" class="headerlink" title="AI工具"></a>AI工具</h3><h4 id="API中转"><a href="#API中转" class="headerlink" title="API中转"></a>API中转</h4><ol><li><a href="https://cloud.siliconflow.cn/models">SiliconFlow</a>：国产厂商，优势在于没有汇率等影响而相对较便宜，缺点就是不含国外模型厂商的API，当需要使用Claude等在某些场景下表现更好的国外模型时有明显缺陷</li><li><a href="https://openrouter.ai">OpenRouter</a>：国外大模型API中转平台，可免费调用一些比较好的模型，但因为有汇率因素导致部分模型价格对学生等群体来说比较高</li></ol><h4 id="AI工具及模型"><a href="#AI工具及模型" class="headerlink" title="AI工具及模型"></a>AI工具及模型</h4><p><em>写在前面：如果是用于做研究或者是考虑深入某一领域等，国产大模型很多似乎都表现得比较拉垮，因此笔者也懒得去尝试一些模型。就身边的同学等使用体验来看，对于国产大模型，在下文中提到的相对比较高质量，其它的模型甚至BAT的似乎更多的是提供情绪价值，并没有多少辅助工作学习的实际作用。当然，考虑到国产大模型其实免费的偏多，并且可以不依赖镜像站等直接访问，相对来说还是有优势的。因此，如果要深入某一领域，还是需要自行寻找一下比较好的模型，从而达到事半功倍的效果。</em></p><ol><li><a href="https://iflow.cn">心流AI</a>：可用于查找论文，阅读论文（论文翻译）等</li><li><a href="https://ai-bot.cn">AI工具集</a>：比较全的AI工具集，查找一些想要的AI工具在这个平台上都能找到</li><li><a href="https://claude.ai/">Claude</a>：写代码全靠Claude捞😋</li><li><a href="https://chatgpt.com">ChatGPT</a>：通用模型，特殊领域表现并不如其它模型</li><li><a href="https://chat.deepseek.com">DeepSeek</a>：数学推理等比较强，可以作为通用模型，文本处理比较厉害，多模态（如图片识别之类的任务）能力较弱，写代码能力还可以。当然，建议使用深度求索公司直接提供的模型，其它厂商自己训练的模型由于数据集等问题，训练效果良莠不齐。</li><li><a href="https://www.kimi.com">Kimi AI</a>：能力还算可以，可以用于当作通用模型，可用于搜索一些内容，多模态能力比较强</li><li><a href="https://manus.im/app">Manus</a>：校友产品支持一波～～可以进行一些复杂任务</li></ol><p><em>注：当前国产AI对于学术和开发友好的模型，笔者及周围测试下来觉得不错的有DeepSeek R1-0528，Kimi K2，Qwen3 Coder.</em></p><h4 id="AI杂项-前端"><a href="#AI杂项-前端" class="headerlink" title="AI杂项/前端"></a>AI杂项/前端</h4><ol><li><a href="https://github.com/CherryHQ/cherry-studio?tab=readme-ov-file">Cherry Studio</a>：可以集成多种大语言模型的客户端</li></ol><h3 id="计算机学术工具"><a href="#计算机学术工具" class="headerlink" title="计算机学术工具"></a>计算机学术工具</h3><ol><li><a href="https://arxiv.org">arXiv</a>：未发表或已发表的文章均有，可以查找最新的科研进展</li><li><a href="https://dblp.org">dblp</a>：CS论文</li><li><a href="https://scholar.google.com">Google Scholar</a>：各种论文</li><li><a href="https://paperswithcode.com">Paper with code</a>：主要是关于机器学习和数据科学的论文和开源代码</li><li><a href="https://snip.mathpix.com/home">Mathpix</a></li><li><a href="https://simpletex.cn/ai/latex_ocr">Simpleletex</a>：Mathpix的平替</li><li><a href="https://app.diagrams.net">Draw IO</a>：在线绘图工具（流程图等），<a href="https://github.com/jgraph/drawio-desktop/tree/v26.2.15">客户端</a></li><li><a href="https://typora.io">Typora</a>：Markdown编辑器</li><li><a href="https://obsidian.md">Obsidian</a>：类似Markdown Pro，用于整理知识、笔记等内容，同时有助于一些使用Markdown进行博客部署的框架进行博客的快速部署</li><li><a href="https://www.zotero.org">Zotero</a>：文献管理</li></ol><h3 id="服务器等"><a href="#服务器等" class="headerlink" title="服务器等"></a>服务器等</h3><ol><li><a href="https://www.autodl.com/home">AutoDL</a>：深度学习租卡网站</li></ol><h3 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h3><ol><li><a href="https://github.com/Eugeny/tabby">Tabby终端</a>：开源免费的终端</li></ol><h3 id="IDE-编辑器"><a href="#IDE-编辑器" class="headerlink" title="IDE/编辑器"></a>IDE/编辑器</h3><ol><li><a href="https://code.visualstudio.com">VS Code</a></li><li><a href="https://www.jetbrains.com/zh-cn/">Jetbrains全家桶</a></li><li><a href="https://www.cursor.com">Cursor</a>：AI编辑器</li><li><a href="https://kiro.dev">Kiro</a>：AI编辑器，由AWS开发</li></ol><h2 id="macOS"><a href="#macOS" class="headerlink" title="macOS"></a>macOS</h2><h3 id="必推"><a href="#必推" class="headerlink" title="必推"></a>必推</h3><ol><li><a href="https://github.com/Homebrew/brew/releases">HomeBrew</a>：macOS上（或许）最好的软件包管理器</li><li><a href="https://github.com/iina/iina/tree/v1.4.0-beta1">IINA播放器</a>：macOS上较好的播放器</li><li><a href="https://pilotmoon.com/scrollreverser/">Scroll Reverser</a>：macOS上鼠标滚轮反向，不使用鼠标则无需使用，<a href="https://github.com/pilotmoon/Scroll-Reverser">源码</a></li><li><a href="https://iterm2.com">iTerm2</a>：macOS上最好的终端（认真）。之前用过一段时间Tabby，但后来改用iTerm2后才知道有多强。配置方法直接参照知乎上的大佬配置就行了，<a href="https://zhuanlan.zhihu.com/p/550022490">文章链接</a>。</li></ol><h3 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h3><ol><li><a href="https://developer.apple.com/xcode/">XCode</a>：macOS上的开发工具，iOS开发必备</li><li><a href="https://github.com/nicklockwood/SwiftFormat">SwiftFormat for Xcode</a>：Swift代码格式化工具</li><li><a href="https://apps.apple.com/us/app/devcleaner-for-xcode/id1388020431">DevCleaner</a>：macOS上的垃圾清理工具</li><li><a href="https://github.com/github/copilot-extension">Github Copilot for Xcode</a>：Xcode上的Github Copilot插件，虽然不是很好用，但是能用就行</li></ol><h3 id="系统工具-杂项"><a href="#系统工具-杂项" class="headerlink" title="系统工具/杂项"></a>系统工具/杂项</h3><p>得益于Mac强大的社区，我们总是可以找到一些很有意思的工具或者是插件。此部分很多东西都来源于Github，在此也需要感谢每一位无私用心付出的开源社区作者。</p><ol><li><a href="https://www.parallels.cn/products/desktop/">Parallels Desktop</a>：macOS上的虚拟机，可以运行Windows，还有很多附加的工具，虽然不是很好用，但是也还算可以</li><li><a href="https://apps.apple.com/us/app/hidden-bar/id1452453066">Hidden Bar</a>：macOS上隐藏菜单栏的工具</li><li><a href="https://cleanmymac.com">CleanMyMac</a>：macOS上的垃圾清理工具，很贵但很好用</li><li><a href="https://www.macbartender.com">Bartender</a>：和Hidden Bar类似，但是功能更加强大，可以隐藏菜单栏，Dock栏，Finder栏等，但是很贵</li><li><a href="https://theunarchiver.com">The Unarchiver</a>：macOS自带解压，但对于一些除了zip之外其它的压缩包格式并不支持，而这个软件支持几乎所有压缩包格式的解压，而且免费</li><li><a href="https://theboring.name">boringNotch</a>：没什么比较大的实际用处，只是将MacBook上的摄像头的一部分变为和iPhone一样的灵动岛，装饰使用</li><li><a href="https://icemenubar.app">IceMenubar</a>：和Hidden Bar类似，开源，免费，功能强大，比较好用</li><li><a href="https://apps.apple.com/cn/app/amphetamine/id937984704?mt=12">Amphetamine</a>：用于设置防止系统睡眠，适合开发、编译、进行深度学习等需要不熄屏的场景，并且可以个性化设置，无需将整个系统设置为不熄屏</li><li><a href="https://www.keka.io/en/">Keka</a>：Mac上最强大的压缩软件之一，可以去除Mac自带的系统文件。在官网上可以免费下载，但有条件可以在App Store上购买支持良心作者。</li><li><a href="https://apps.apple.com/us/app/pixelstyle-photo-editor/id1244649277?mt=12">PixelStyle</a>：Mac上比较好的图片编辑软件，相比Adobe Photoshop等的优势就是免费</li><li><a href="https://apps.apple.com/cn/app/irightmenu-右键新建文件菜单/id1542347829?mt=12">iRightMenu</a>：为macOS创造类似于Windows系统的右键菜单</li></ol><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><p><del>Windows没有必要推荐工具。</del>笔者太懒了，以后再来写吧。如果一定要推荐，那就推荐WSL吧。</p><ol><li><a href="https://learn.microsoft.com/en-us/windows/wsl/install">WSL文档</a>：<del>不知道放什么，就放个文档链接吧。</del>既想用Linux又想用Windows的用户的福音，同时共享同一个文件系统，当然还可以直接用来做深度学习。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Tools </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Java环境配置</title>
      <link href="/posts/f7539049.html"/>
      <url>/posts/f7539049.html</url>
      
        <content type="html"><![CDATA[<h2 id="MacOS"><a href="#MacOS" class="headerlink" title="MacOS"></a>MacOS</h2><h3 id="使用Homebrew安装JDK"><a href="#使用Homebrew安装JDK" class="headerlink" title="使用Homebrew安装JDK"></a>使用Homebrew安装JDK</h3><ul><li>配置Homebrew</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;</span><br></pre></td></tr></table></figure><h4 id="安装OpenJDK"><a href="#安装OpenJDK" class="headerlink" title="安装OpenJDK"></a>安装OpenJDK</h4><p>查询jdk版本信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew search jdk</span><br></pre></td></tr></table></figure><p>安装特定版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install openjdk@17</span><br></pre></td></tr></table></figure><h4 id="配置JDK"><a href="#配置JDK" class="headerlink" title="配置JDK"></a>配置JDK</h4><p>注意，运行完以上命令后，终端会出现如下的信息：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">For the system Java wrappers to find this JDK, symlink it with</span><br><span class="line">  sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk</span><br><span class="line"></span><br><span class="line">openjdk@17 is keg-only, which means it was not symlinked into /opt/homebrew,</span><br><span class="line">because this is an alternate version of another formula.</span><br><span class="line"></span><br><span class="line">If you need to have openjdk@17 first in your PATH, run:</span><br><span class="line">  echo &#x27;export PATH=&quot;/opt/homebrew/opt/openjdk@17/bin:$PATH&quot;&#x27; &gt;&gt; ~/.zshrc</span><br><span class="line"></span><br><span class="line">For compilers to find openjdk@17 you may need to set:</span><br><span class="line">  export CPPFLAGS=&quot;-I/opt/homebrew/opt/openjdk@17/include&quot;</span><br><span class="line">==&gt; Summary</span><br><span class="line">🍺  /opt/homebrew/Cellar/openjdk@17/17.0.14: 636 files, 304.3MB</span><br><span class="line">==&gt; Running `brew cleanup openjdk@17`...</span><br><span class="line">Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.</span><br><span class="line">Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).</span><br></pre></td></tr></table></figure><p>根据指示，运行相应的命令。首先运行如下命令使得系统可以找到当前下载的JDK：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk</span><br></pre></td></tr></table></figure><p>可以通过tree命令来检查是否成功。</p><ul><li>若没有安装过tree，则运行以下命令：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tree</span><br></pre></td></tr></table></figure><p>检查是否成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree /Library/Java</span><br></pre></td></tr></table></figure><h4 id="检查当前JDK以及Java环境"><a href="#检查当前JDK以及Java环境" class="headerlink" title="检查当前JDK以及Java环境"></a>检查当前JDK以及Java环境</h4><p>执行以下命令可以查看当前系统使用的JDK版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/libexec/java_home</span><br></pre></td></tr></table></figure><p>执行以下命令可查看当前系统使用的Java版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java --version</span><br></pre></td></tr></table></figure><h4 id="多版本JDK管理"><a href="#多版本JDK管理" class="headerlink" title="多版本JDK管理"></a>多版本JDK管理</h4><p>先要查找到JDK的地址，采用如下的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/libexec/java_home -V</span><br></pre></td></tr></table></figure><p>若没有建过.bash_profile文件，则在根目录下建此配置文件，并打开此配置文件。如已经有，则添加即可。其中地址均使用上面命令查找出来的地址。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#配置JDK路径</span><br><span class="line">export JAVA_11_HOME=/Library/Java/JavaVirtualMachines/jdk-11.jdk/Contents/Home</span><br><span class="line">export JAVA_17_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home</span><br><span class="line"> </span><br><span class="line"># 设置默认JDK版本，默认使用 JDK17</span><br><span class="line">export JAVA_HOME=$JAVA_17_HOME</span><br><span class="line">CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:.</span><br><span class="line"> </span><br><span class="line"># 配置alias命令动态切换JDK版本  </span><br><span class="line">alias jdk11=&quot;export JAVA_HOME=$JAVA_11_HOME&quot;</span><br><span class="line">alias jdk17=&quot;export JAVA_HOME=$JAVA_17_HOME&quot;</span><br><span class="line"> </span><br><span class="line">export JAVA_HOME</span><br><span class="line">export CLASSPATH</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>若嫌麻烦可以跳过查看地址那一步，直接在上述.bash_profile中编辑：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=$(/usr/libexec/java_home -v11)</span><br><span class="line">export JAVA_8_HOME=$(/usr/libexec/java_home -v1.8)</span><br><span class="line">export JAVA_11_HOME=$(/usr/libexec/java_home -v11)</span><br><span class="line"></span><br><span class="line">alias java8=&#x27;export JAVA_HOME=$JAVA_8_HOME&#x27;</span><br><span class="line">alias java11=&#x27;export JAVA_HOME=$JAVA_11_HOME&#x27;</span><br></pre></td></tr></table></figure><p>保存配置文件，在终端中输出如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source .bash_profile</span><br></pre></td></tr></table></figure><p>可以通过如下命令查看是否配置成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $JAVA_HOME</span><br></pre></td></tr></table></figure><h4 id="JDK版本切换"><a href="#JDK版本切换" class="headerlink" title="JDK版本切换"></a>JDK版本切换</h4><p>在终端中输入命令：jdk/java版本号 即可，如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdk17</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java17</span><br></pre></td></tr></table></figure><p>注意看清楚编辑时alias后面的命令到底是jdk还是java</p><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><h3 id="下载并安装JDK"><a href="#下载并安装JDK" class="headerlink" title="下载并安装JDK"></a>下载并安装JDK</h3><p>上<a href="https://www.oracle.com/java/technologies/downloads/?er=221886">官网</a>下载所需的JDK，运行相应的安装程序即可完成安装。若从未注册过账号，需要在官网注册账号后才能下载。</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以上的JDK目录，在系统变量中，配置为<strong>JAVA_HOME</strong>，路径为JDK路径，如：C:\Program Files\Java\jdk1.8.0_191。</p><p>然后，在Path中添加两个路径，分别为</p><ul><li><p><strong>%JAVA_HOME%\bin</strong></p></li><li><p><strong>%JAVA_HOME%\jre\bin</strong></p></li></ul><p>此外，还需要配置<strong>CLASSPATH</strong>变量。若本来存在这个变量，则进行编辑，若无则新建。变量值为</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</span><br></pre></td></tr></table></figure><p>可以通过以下命令来检查是否安装成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java</span><br></pre></td></tr></table></figure><h2 id="Java运行"><a href="#Java运行" class="headerlink" title="Java运行"></a>Java运行</h2><p>最方便和最简单的当然是直接拿IDEA一键运行啦，当然稍微麻烦点的就是配置VSCode。但如果除去这两个，就需要依靠运行脚本来运行Java程序了。</p><p>下面将展示脚本编写示例。</p><h3 id="Windows脚本"><a href="#Windows脚本" class="headerlink" title="Windows脚本"></a>Windows脚本</h3><p>在当前工程目录下，创建run.bat脚本文件（文件名是什么随便起就行，不一定要为run.bat）。假设我们的工程目录是如下的情况，我们可以在脚本文件中对应编辑如下内容：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">JavaDemo</span><br><span class="line">├── .idea</span><br><span class="line">├── bin</span><br><span class="line">│   └── production</span><br><span class="line">│       └── JavaDemo</span><br><span class="line">│           └── hust</span><br><span class="line">│               └── cs</span><br><span class="line">│                   └── javacourse</span><br><span class="line">│                       └── ch1</span><br><span class="line">│                           └── HelloWorld.class</span><br><span class="line">├── src</span><br><span class="line">│   └── hust.cs.javacourse.ch1</span><br><span class="line">│       └── HelloWorld.java</span><br><span class="line">└── JavaDemo.iml</span><br></pre></td></tr></table></figure><figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> JAVA_HOME=D:\jdk1.<span class="number">8</span>.<span class="number">0</span>_231_64bit</span><br><span class="line"><span class="built_in">set</span> PROJECT_HOME=D:\IdeaWorkspace\JavaDemo</span><br><span class="line"><span class="built_in">set</span> <span class="built_in">path</span>=<span class="variable">%path%</span>;<span class="variable">%JAVA_HOME%</span>\bin</span><br><span class="line"><span class="built_in">set</span> classpath=<span class="variable">%classpath%</span>;<span class="variable">%PROJECT_HOME%</span>\bin\production\JavaDemo</span><br><span class="line"></span><br><span class="line">java -classpath <span class="variable">%classpath%</span> hust.cs.javacourse.ch1.HelloWorld</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><p>JAVA_HOME设置JAVA_HOME环境变量</p></li><li><p>PROJECT_HOME设置PROJECT_HOME环境变量</p></li><li><p>path设置把JAVA_HOME目录的子目录bin加到环境变量PATH</p></li><li><p>classpath把PROJECT_HOME目录的子目录bin\production\JavaDemo加到环境变量CLASSPATH，这个目录是类HelloWorld所属包的顶级目录</p></li><li><p>最后一行为运行指令，启动类时，用类的完全限定名（带包名限定），并且带-classspath选项</p></li></ul><p>运行时直接命令行输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./run.bat</span><br></pre></td></tr></table></figure><hr><p>参考资料：</p><ol><li><p><a href="https://blog.csdn.net/Jarvs/article/details/134669580">Mac JDK环境变量配置 及 JDK多版本切换</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/298535991">新 Mac 如何优雅地配置 Java 开发环境</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Env </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello Hexo</title>
      <link href="/posts/4a17b156.html"/>
      <url>/posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="More"><a href="#More" class="headerlink" title="More"></a>More</h2><p>可以参考安知鱼的<a href="https://blog.anheyu.com/posts/sdxhu.html#配置自定义-css">博客</a>和<a href="https://www.bilibili.com/video/BV1CG41157fr?spm_id_from=333.788.player.switch&amp;vd_source=2e36fae16810615c2d859efc03aef1c4">Bilibili</a></p>]]></content>
      
      
      <categories>
          
          <category> 本站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/google7e7b55ce6b603fc5.html"/>
      <url>/google7e7b55ce6b603fc5.html</url>
      
        <content type="html"><![CDATA[google-site-verification: google7e7b55ce6b603fc5.html]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/manifest.json"/>
      <url>/manifest.json</url>
      
        <content type="html"><![CDATA[{"name":"EpsilonZ `Blog","short_name":"EpsilonZ","theme_color":"#3b70fc","background_color":"#3b70fc","display":"standalone","scope":"/","start_url":"/","icons":[{"src":"/img/siteicon/16.png","sizes":"16x16","type":"image/png"},{"src":"/img/siteicon/32.png","sizes":"32x32","type":"image/png"},{"src":"/img/siteicon/48.png","sizes":"48x48","type":"image/png"},{"src":"/img/siteicon/64.png","sizes":"64x64","type":"image/png"},{"src":"/img/siteicon/128.png","sizes":"128x128","type":"image/png"},{"src":"/img/siteicon/144.png","sizes":"144x144","type":"image/png"},{"src":"/img/siteicon/512.png","sizes":"512x512","type":"image/png"}],"splash_pages":null}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>简 介 ｜ Who am I</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于我-｜-About-me"><a href="#关于我-｜-About-me" class="headerlink" title="关于我 ｜ About me"></a>关于我 ｜ About me</h2><p>目前就读于华中科技大学计算机科学与技术学院，喜欢的方向很杂，做做机器学习，偶尔也做做系统。</p><p>I am currently an undergraduate student in Huazhong University of Science and Technology, majoring in computer science and technology. I am interested in various fields, including machine learning and computer system, especially in deep learning.</p><hr><h2 id="兴趣领域-｜-Interests"><a href="#兴趣领域-｜-Interests" class="headerlink" title="兴趣领域 ｜ Interests"></a>兴趣领域 ｜ Interests</h2><p>当下研究方向为图神经网络，同时也做AI4Science，并应用图神经网络。空余时间会进行基于Swift的iOS应用开发。</p><p>I am now studying about Graph Neural Network, in detail GNN with Transformer. Besides, I also try AI for Science, where GNN can be widely applied.</p><hr><h2 id="项目-｜-Projects"><a href="#项目-｜-Projects" class="headerlink" title="项目 ｜ Projects"></a>项目 ｜ Projects</h2><ul><li><p>AI辅助新材料的发现(GNN, ReactJS, AI4Science)</p><ul><li>使用<strong>图神经网络</strong>开发了一个模型，用于辅助新材料的发现</li><li>采用<strong>ReactJS</strong>框架搭建前端，用于输入和可视化等</li><li>创建<strong>分布式相似性检索系统</strong>，用于搜索相似的材料分子</li></ul></li><li><p>AI-Assisted New Material Discovery(GNN, ReactJS, AI4Science)</p><ul><li>Develop a model to assist in the discovery of new materials by applying <strong>Graph Neural Network</strong>.</li><li>Develop a web application for the front development, namely, the inputs and visualization of the model with <strong>ReactJS</strong>.</li><li>Create <strong>a distributed similarity search system</strong> of searching for material molecules.</li></ul></li></ul><hr><h2 id="技能-｜-Skills"><a href="#技能-｜-Skills" class="headerlink" title="技能 ｜ Skills"></a>技能 ｜ Skills</h2><ul><li>编程语言：C/C++, Java, SQL, python, Swift, JavaScript</li><li><p>技能标签：深度学习, 机器学习, iOS开发, ReactJS</p></li><li><p>Programming language: C/C++, Java, SQL, python, Swift, JavaScript</p></li><li>Skill tags: Deep Learning, Machine Learning, iOS App Development, ReactJS</li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类 ｜ Categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[/* @font-face {  font-family: Candyhome;  src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/Candyhome.ttf);  font-display: swap;  font-weight: lighter;} */@font-face {    font-family: ZhuZiAYuanJWD;    src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/ZhuZiAWan.woff2);    font-display: swap;    font-weight: lighter;}div#menus {    font-family: "ZhuZiAYuanJWD";}h1#site-title {    font-family: ZhuZiAYuanJWD;    font-size: 3em !important;}a.article-title,a.blog-slider__title,a.categoryBar-list-link,h1.post-title {    font-family: ZhuZiAYuanJWD;}.iconfont {    font-family: "iconfont" !important;    font-size: 3em;    /* 可以定义图标大小 */    font-style: normal;    -webkit-font-smoothing: antialiased;    -moz-osx-font-smoothing: grayscale;}/* 时间轴生肖icon */svg.icon {    /* 这里定义svg.icon，避免和Butterfly自带的note标签冲突 */    width: 1em;    height: 1em;    /* width和height定义图标的默认宽度和高度*/    vertical-align: -0.15em;    fill: currentColor;    overflow: hidden;}.icon-zhongbiao::before {    color: #f7c768;}/* bilibli番剧插件 */#article-container .bangumi-tab.bangumi-active {    background: var(--anzhiyu-theme);    color: var(--anzhiyu-ahoverbg);    border-radius: 10px;}a.bangumi-tab:hover {    text-decoration: none !important;}.bangumi-button:hover {    background: var(--anzhiyu-theme) !important;    border-radius: 10px !important;    color: var(--anzhiyu-ahoverbg) !important;}a.bangumi-button.bangumi-nextpage:hover {    text-decoration: none !important;}.bangumi-button {    padding: 5px 10px !important;}a.bangumi-tab {    padding: 5px 10px !important;}svg.icon.faa-tada {    font-size: 1.1em;}.bangumi-info-item {    border-right: 1px solid #f2b94b;}.bangumi-info-item span {    color: #f2b94b;}.bangumi-info-item em {    color: #f2b94b;}/* 解决artitalk的图标问题 */#uploadSource>svg {    width: 1.19em;    height: 1.5em;}/*top-img黑色透明玻璃效果移除，不建议加，除非你执着于完全一图流或者背景图对比色明显 */#page-header:not(.not-top-img):before {    background-color: transparent !important;}/* 首页文章卡片 */#recent-posts>.recent-post-item {    background: rgba(255, 255, 255, 0.9);}/* 首页侧栏卡片 */#aside-content .card-widget {    background: rgba(255, 255, 255, 0.9);}/* 文章页面正文背景 */div#post {    background: rgba(255, 255, 255, 0.9);}/* 分页页面 */div#page {    background: rgba(255, 255, 255, 0.9);}/* 归档页面 */div#archive {    background: rgba(255, 255, 255, 0.9);}/* 标签页面 */div#tag {    background: rgba(255, 255, 255, 0.9);}/* 分类页面 */div#category {    background: rgba(255, 255, 255, 0.9);}/*夜间模式伪类遮罩层透明*/[data-theme="dark"] #recent-posts>.recent-post-item {    background: #121212;}[data-theme="dark"] .card-widget {    background: #121212 !important;}[data-theme="dark"] div#post {    background: #121212 !important;}[data-theme="dark"] div#tag {    background: #121212 !important;}[data-theme="dark"] div#archive {    background: #121212 !important;}[data-theme="dark"] div#page {    background: #121212 !important;}[data-theme="dark"] div#category {    background: #121212 !important;}[data-theme="dark"] div#category {    background: transparent !important;}/* 页脚透明 */#footer {    background: transparent !important;}/* 头图透明 */#page-header {    background: transparent !important;}#rightside>div>button {    border-radius: 5px;}/* 滚动条 */::-webkit-scrollbar {    width: 10px;    height: 10px;}::-webkit-scrollbar-thumb {    background-color: #3b70fc;    border-radius: 2em;}::-webkit-scrollbar-corner {    background-color: transparent;}::-moz-selection {    color: #fff;    background-color: #3b70fc;}/* 音乐播放器 *//* .aplayer .aplayer-lrc {  display: none !important;} */.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {    left: -66px !important;    transition: all 0.3s;    /* 默认情况下缩进左侧66px，只留一点箭头部分 */}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {    left: 0 !important;    transition: all 0.3s;    /* 鼠标悬停是左侧缩进归零，完全显示按钮 */}.aplayer.aplayer-fixed {    z-index: 999999 !important;}/* 评论框  */.vwrap {    box-shadow: 2px 2px 5px #bbb;    background: rgba(255, 255, 255, 0.3);    border-radius: 8px;    padding: 30px;    margin: 30px 0px 30px 0px;}/* 设置评论框 */.vcard {    box-shadow: 2px 2px 5px #bbb;    background: rgba(255, 255, 255, 0.3);    border-radius: 8px;    padding: 30px;    margin: 30px 0px 0px 0px;}/* md网站下划线 */#article-container a:hover {    text-decoration: none !important;}#article-container #hpp_talk p img {    display: inline;}/* 404页面 */#error-wrap {    position: absolute;    top: 40%;    right: 0;    left: 0;    margin: 0 auto;    padding: 0 1rem;    max-width: 1000px;    transform: translate(0, -50%);}#error-wrap .error-content {    display: flex;    flex-direction: row;    justify-content: center;    align-items: center;    margin: 0 1rem;    height: 18rem;    border-radius: 8px;    background: var(--card-bg);    box-shadow: var(--card-box-shadow);    transition: all 0.3s;}#error-wrap .error-content .error-img {    box-flex: 1;    flex: 1;    height: 100%;    border-top-left-radius: 8px;    border-bottom-left-radius: 8px;    background-color: #3b70fc;    background-position: center;    background-size: cover;}#error-wrap .error-content .error-info {    box-flex: 1;    flex: 1;    padding: 0.5rem;    text-align: center;    font-size: 14px;    font-family: Titillium Web, "PingFang SC", "Hiragino Sans GB", "Microsoft JhengHei", "Microsoft YaHei", sans-serif;}#error-wrap .error-content .error-info .error_title {    margin-top: -4rem;    font-size: 9em;}#error-wrap .error-content .error-info .error_subtitle {    margin-top: -3.5rem;    word-break: break-word;    font-size: 1.6em;}#error-wrap .error-content .error-info a {    display: inline-block;    margin-top: 0.5rem;    padding: 0.3rem 1.5rem;    background: var(--btn-bg);    color: var(--btn-color);}#body-wrap.error .aside-list {    display: flex;    flex-direction: row;    flex-wrap: nowrap;    bottom: 0px;    position: absolute;    padding: 1rem;    width: 100%;    overflow: scroll;}#body-wrap.error .aside-list .aside-list-group {    display: flex;    flex-direction: row;    flex-wrap: nowrap;    max-width: 1200px;    margin: 0 auto;}#body-wrap.error .aside-list .aside-list-item {    padding: 0.5rem;}#body-wrap.error .aside-list .aside-list-item img {    width: 100%;    object-fit: cover;    border-radius: 12px;}#body-wrap.error .aside-list .aside-list-item .thumbnail {    overflow: hidden;    width: 230px;    height: 143px;    background: var(--anzhiyu-card-bg);    display: flex;}#body-wrap.error .aside-list .aside-list-item .content .title {    -webkit-line-clamp: 2;    overflow: hidden;    display: -webkit-box;    -webkit-box-orient: vertical;    line-height: 1.5;    justify-content: center;    align-items: flex-end;    align-content: center;    padding-top: 0.5rem;    color: white;}#body-wrap.error .aside-list .aside-list-item .content time {    display: none;}/* 代码框主题 */#article-container figure.highlight {    border-radius: 10px;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签 | Tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>

<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>论文阅读：Primphormer | EpsilonZ's Blog</title><meta name="author" content="EpsilonZ"><meta name="copyright" content="EpsilonZ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Primphormer:Efficient Graph Transformers with Primal Representations"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读：Primphormer"><meta property="og:url" content="https://epsilonzyj.github.io/posts/Primphormer.html"><meta property="og:site_name" content="EpsilonZ&#39;s Blog"><meta property="og:description" content="Primphormer:Efficient Graph Transformers with Primal Representations"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://epsilonzyj.github.io/img/GNNCover.png"><meta property="article:published_time" content="2025-10-25T05:20:01.000Z"><meta property="article:modified_time" content="2025-10-29T13:48:29.811Z"><meta property="article:author" content="EpsilonZ"><meta property="article:tag" content="AI"><meta property="article:tag" content="Graph ML"><meta property="article:tag" content="Graph Transformer"><meta property="article:tag" content="Linear Transformer"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://epsilonzyj.github.io/img/GNNCover.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "论文阅读：Primphormer",
  "url": "https://epsilonzyj.github.io/posts/Primphormer.html",
  "image": "https://epsilonzyj.github.io/img/GNNCover.png",
  "datePublished": "2025-10-25T05:20:01.000Z",
  "dateModified": "2025-10-29T13:48:29.811Z",
  "author": [
    {
      "@type": "Person",
      "name": "EpsilonZ",
      "url": "https://epsilonzyj.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/128.ico"><link rel="canonical" href="https://epsilonzyj.github.io/posts/Primphormer.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="manifest" href="/manifest.json"><meta name="msapplication-TileColor" content="#3b70fc"><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/128.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/siteicon/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/siteicon/16.png"><link rel="mask-icon" href="/img/siteicon/128.png" color="#5bbad5"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"论文阅读：Primphormer",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload='this.media="all"'><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="EpsilonZ's Blog" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image:url(/img/IMG_3821.jpg)"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_2179.JPG" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/GNNCover.png)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">EpsilonZ's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">论文阅读：Primphormer</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i> <span>关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读：Primphormer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-25T05:20:01.000Z" title="发表于 2025-10-25 13:20:01">2025-10-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-29T13:48:29.811Z" title="更新于 2025-10-29 21:48:29">2025-10-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/">Graph ML</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/Graph-Transformer/">Graph Transformer</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/Graph-Transformer/Linear-Transformer/">Linear Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">8.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>35分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><hr><h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><ul><li>作者: Mingzhen He, Ruikai Yang, Hanling Tian, Youmei Qiu, Xiaolin Huang</li><li>出处: ICML</li><li>日期: 2025</li><li>PDF: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=fMAihjfJij">https://openreview.net/pdf?id=fMAihjfJij</a></li></ul><hr><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h1 id="论文摘要翻译"><a href="#论文摘要翻译" class="headerlink" title="论文摘要翻译"></a>论文摘要翻译</h1><p>图Transformer(GT)已成为图表示学习的一种有前景的方法。尽管取得了成功，但由于GT需要进行成对(pair-wise)计算，其二阶复杂度限制了在大规模图上的可扩展性。为了从根本上减少GT的计算负担，我们提出了一种原始-对偶(primal-dual)框架，将图上的自注意力机制解释为对偶表示(dual representation)。基于这一框架，我们开发出了Primphormer，这是一种高效GT，利用具有线性复杂度的原始表示(primal representation)。理论分析表明，Primphormer既是序列和图上函数的通用近似器(universal approximator)，又保留了对非同构图(non-isomorphic graphs)的判别能力。在各种图基准上的广泛实验证明，Primphormer取得了具有竞争力的经验结果，同时保持了更友好的内存和计算成本。</p><h1 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h1><p>本文主要解决了图Transformer(GTs)在实际应用中的计算效率问题。具体研究的问题可以概括为以下几个方面：</p><h2 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h2><p>传统图Transformer面临二次方(O(N²))的计算复杂度，这是由于其自注意力机制中需要计算每对节点之间的相似度，导致在大规模图上的可扩展性受到严重限制。</p><h2 id="现有解决方案的局限性"><a href="#现有解决方案的局限性" class="headerlink" title="现有解决方案的局限性"></a>现有解决方案的局限性</h2><p>作者分析了之前解决这一问题的方法及其不足：</p><ol><li><strong>线性注意力模型</strong>：如Performer和BigBird虽然减少了计算复杂度，但引入了额外的计算开销，成为中等规模图的主要计算瓶颈</li><li><strong>稀疏注意力机制</strong>：如Exphormer利用图结构的稀疏性，但当图变得更密集时，复杂度又退化为二次方</li><li><strong>序列特定方法</strong>：如Primal-Atten等方法虽然解决了序列数据中的表示问题，但不适用于图数据，因为图中的节点没有自然顺序属性<h2 id="理论挑战"><a href="#理论挑战" class="headerlink" title="理论挑战"></a>理论挑战</h2>关键的技术挑战是注意力分数本身具有非对称性(κ(x,y)≠κ(y,x))，这违反了Mercer条件，使得经典的原始-对偶讨论无法直接应用。虽然最近的研究已经在探索不对称核机器中的原始-对偶关系，但这些方法主要是在序列数据上设计的，不适用于图网络。<h2 id="创新切入点"><a href="#创新切入点" class="headerlink" title="创新切入点"></a>创新切入点</h2>为了从根本上增强GTs的可扩展性，作者寻求避免成对(pair-wise)计算的方法，从核机器中的原始-对偶关系获取灵感。他们注意到传统的支持向量机(Cortes &amp; Vapnik, 1995)、最小二乘支持向量机(Suykens &amp; Vandewalle, 1999)和核主成分分析(Mika et al., 1999)等模型展示了如何通过在原始空间中表示来避免二次复杂度。<h2 id="针对图数据的具体挑战"><a href="#针对图数据的具体挑战" class="headerlink" title="针对图数据的具体挑战"></a>针对图数据的具体挑战</h2>对于图数据应用原始-对偶关系时，作者遇到了一个基本问题：与不同节点对上的序列不同，图中的节点没有明确指定的顺序或排列，这使得讨论图上自注意力的原始-对偶关系成为一个开放性问题。<br>综上所述，本文主要研究问题是如何开发一种高效的图Transformer架构，它能够：</li><li>规避成对计算，将计算复杂度从二次方降低到线性</li><li>保留区分非同构图的表达能力</li><li>在保证性能的同时，提供更友好的内存和计算成本Primphormer的研究问题主要集中在解决图Transformer在处理大规模图时的计算效率限制。传统图Transformer(GTs)由于自注意力机制中的成对(pair-wise)计算，具有二次方(O(N²))的计算复杂度，这严重限制了它们在大规模图上的可扩展性。<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2>He等人（2025） 提出的Primphormer旨在解决图Transformer（GTs）在大规模图应用中的计算效率问题。传统GTs的自注意力机制由于需要进行成对计算，导致计算复杂度为O(N²)，严重限制了其在大型图上的可扩展性。为解决这一问题，作者提出了一种基于原始表示（primal representation）的框架，通过原始-对偶关系将自注意力机制重构为线性复杂度模型，同时保持模型的表达能力。<h2 id="Primphormer核心方法"><a href="#Primphormer核心方法" class="headerlink" title="Primphormer核心方法"></a>Primphormer核心方法</h2><h3 id="1-问题与动机"><a href="#1-问题与动机" class="headerlink" title="1. 问题与动机"></a>1. 问题与动机</h3>图Transformer在处理长程依赖时表现出色，但其自注意力机制需要计算所有节点对之间的注意力分数：<script type="math/tex;mode=display">\kappa(x_i, x_j) = \sigma(\langle q(x_i), k(x_j) \rangle), \quad o_i = \sum_{j=1}^{N} v(x_j)\kappa(x_i, x_j)</script>其中$q(\cdot)$、$k(\cdot)$和$v(\cdot)$分别是查询(query)、键(key)和值(value)的投影函数，$\sigma$是激活函数。这种成对计算导致$O(N²)$复杂度，限制了实际应用。<h3 id="2-原始-对偶框架"><a href="#2-原始-对偶框架" class="headerlink" title="2. 原始-对偶框架"></a>2. 原始-对偶框架</h3>论文引入核机器中的原始-对偶关系，将自注意力机制解释为对偶表示，并通过空间转换实现原始表示：<h4 id="2-1-不对称核技巧"><a href="#2-1-不对称核技巧" class="headerlink" title="2.1 不对称核技巧"></a>2.1 不对称核技巧</h4>由于注意力分数$\kappa(x, y) \neq \kappa(y, x)$违反Mercer条件，作者采用定义2.1的不对称核技巧：<script type="math/tex;mode=display">\kappa(x, z) = \langle \phi_q(x), \phi_k(z) \rangle</script>其中$\phi_q$和$\phi_k$是查询和键的特征映射。<h4 id="2-2-虚拟节点与全局聚合"><a href="#2-2-虚拟节点与全局聚合" class="headerlink" title="2.2 虚拟节点与全局聚合"></a>2.2 虚拟节点与全局聚合</h4>为保持图的置换等变性，引入虚拟节点<script type="math/tex">f_X = F + BX_1^N 1_{N_s}^{\top}</script>，其中：</li></ol><ul><li>$B, F$是可学习权重</li><li>$X_1^N$是节点特征矩阵</li><li>$1_{N_s}$是长度为$N_s$的全1向量（$N_s \ll N$）<br>公式详细理解见<a href="#附录A">附录A</a>.<h4 id="2-3-优化问题构建"><a href="#2-3-优化问题构建" class="headerlink" title="2.3 优化问题构建"></a>2.3 优化问题构建</h4>基于原始表示定义优化问题：<script type="math/tex;mode=display">\min_{\Theta} J = \frac{1}{2}\sum_{i=1}^{N} e_i^\top \Lambda e_i + \frac{1}{2}\sum_{j=1}^{N} r_j^\top \Lambda r_j - \text{Tr}(W_e^\top W_r)</script>约束条件：<script type="math/tex;mode=display">e_i = f_X W_e \phi_q(x_i), \quad r_j = f_X W_r \phi_k(x_j)</script>其中$\Theta = {W_e, W_r, e_i, r_j}$是参数集，$\Lambda$是对角正则化矩阵，$W_e, W_r \in \mathbb{R}^{N_s \times p}$。<br>此优化问题本质是进行正则化，对于变分原理优化及KKT条件，见<a href="#附录D">附录D</a>.<h4 id="2-4-对偶问题与原始表示"><a href="#2-4-对偶问题与原始表示" class="headerlink" title="2.4 对偶问题与原始表示"></a>2.4 对偶问题与原始表示</h4>定理2.2的KKT条件给出对偶问题：<script type="math/tex;mode=display">K H_r F_X = H_e \Sigma, \quad K^\top H_e F_X = H_r \Sigma</script>通过优化推导出原始表示与对偶表示的关系：</li><li><strong>原始表示</strong>:<script type="math/tex;mode=display">e(x) = f_X W_e \phi_q(x), \quad r(x) = f_X W_r \phi_k(x)</script></li><li><strong>对偶表示</strong>:<script type="math/tex;mode=display">e(x) = \sum_{j=1}^{N} \tilde{h}_{rj} \kappa(x, x_j), \quad r(x) = \sum_{i=1}^{N} \tilde{h}_{ei} \kappa(x_i, x)</script>其中<script type="math/tex">\tilde{h}_{rj} = F_X h_{rj}</script>和<script type="math/tex">\tilde{h}_{ei} = F_X h_{ei}</script>形成数据自适应基。<br>对于优化问题的详细推导和理解见<a href="#附录B">附录B</a><h4 id="2-5-输出"><a href="#2-5-输出" class="headerlink" title="2.5 输出"></a>2.5 输出</h4>得到$e$（查询特征映射）和$r$（键特征映射）后，采用拼接进行输出：<script type="math/tex;mode=display">o = \mathbf{W}_c[\mathbf{e};\mathbf{r}]</script><h3 id="3-实现与架构"><a href="#3-实现与架构" class="headerlink" title="3. 实现与架构"></a>3. 实现与架构</h3><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./Primphormer/Primphormer.png" alt=""><br>Primphormer模型架构为$T_{Pri} = \text{FFN}(X + \text{Prim}(X))$，其中：</li></ul><ol><li>使用虚拟节点聚合全局图信息</li><li>通过最小化额外损失实现优化：<script type="math/tex;mode=display">\mathcal{L} = \mathcal{L}_{\text{task}} + \eta \sum_l J_l^2</script>$\eta$是正则化系数，$J_l$是第$l$层的原始目标损失<h2 id="理论贡献"><a href="#理论贡献" class="headerlink" title="理论贡献"></a>理论贡献</h2><h3 id="1-通用近似定理"><a href="#1-通用近似定理" class="headerlink" title="1. 通用近似定理"></a>1. 通用近似定理</h3></li></ol><ul><li><strong>定理3.2</strong>：Primphormer是置换等序列函数的通用近似器</li><li><strong>定理3.3</strong>：添加位置编码后，Primphormer可近似任意连续序列函数<h3 id="2-表达能力保留"><a href="#2-表达能力保留" class="headerlink" title="2. 表达能力保留"></a>2. 表达能力保留</h3></li><li><strong>定理3.4</strong>：Primphormer能模拟1-Weisfeiler-Lehman(1-WL)测试，证明其与标准Transformer区分非同构图的能力相当：<script type="math/tex;mode=display">C_{1,t}(v) = C_{1,t}(w) \iff X^{(t)}_{\text{Pri}}(v) = X^{(t)}_{\text{Pri}}(w)</script><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2>Primphormer在多个基准数据集上验证了其高效性与有效性：</li></ul><ol><li><strong>LRGB数据集</strong>（表1）：在5个数据集中，Primphormer在4个上超越基线</li><li><strong>GNN基准</strong>（表2）：在MNIST上达到98.56%准确率，优于其他GT模型</li><li><strong>效率对比</strong>（表4）：计算复杂度从O(N²)降至O(Nps)，内存占用显著降低，例如在MalNet-Tiny上仅需2.86GB（远低于Exphormer的10.38GB）<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>Primphormer通过以下创新解决了GTs的计算效率问题：</li><li>引入原始-对偶框架，将二次复杂度转换为线性复杂度</li><li>设计虚拟节点机制保持图的置换等变性</li><li>理论证明其通用近似能力和表达能力</li><li>实验验证在保持性能的同时大幅降低计算和内存开销<br>这一方法为图Transformer在大规模图上的应用提供了新思路，未来可进一步探索边特征集成和高效微调方法。</li></ol><hr><p>参考：<br>He, M., Yang, R., Tian, H., Qiu, Y., &amp; Huang, X. (2025). Primphormer: Efficient Graph Transformers with Primal Representations. <em>Proceedings of the 42nd International Conference on Machine Learning</em>.</p><hr><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="附录A"><a href="#附录A" class="headerlink" title="附录A"></a>附录A</h2><p>在论文的Primphormer模型中，虚拟节点全局聚合机制是通过<strong>平均池化后经过可学习的线性层</strong>实现的，具体分析如下：</p><h3 id="虚拟节点的实现方式"><a href="#虚拟节点的实现方式" class="headerlink" title="虚拟节点的实现方式"></a>虚拟节点的实现方式</h3><p>从论文第2.3节的定义可以看到，虚拟节点<script type="math/tex">f_X</script>被定义为：<script type="math/tex">f_X := F + BX_1^N 1_{N_s}^{\top}</script>其中：</p><ul><li>$F \in \mathbb{R}^{s \times N_s}$ 和 $B \in \mathbb{R}^{s \times d}$ 是可学习权重</li><li>$X_1^N \in \mathbb{R}^{d \times N}$ 是节点特征矩阵</li><li>$1_{N_s}$ 是长度为 $N_s$ 的全1向量<h3 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h3>结合理论和代码，虚拟节点的聚合过程可以表示为： $f_X^{(l+1)} = \text{FFN}(\text{global_mean_pool}(h^{(l)})) + f_X^{(l)}$ 其中：</li><li>global_mean_pool() 是平均池化操作</li><li>FFN() 代表一个可学习的线性层</li><li>$f_X^{(l)}$ 是第 $l$ 层的虚拟节点表示</li></ul><h2 id="附录B"><a href="#附录B" class="headerlink" title="附录B"></a>附录B</h2><p>本文提出的优化问题在公式(2.5)中定义如下：</p><script type="math/tex;mode=display">J = \frac{1}{2}\sum_{i=1}^N e_i^\top \Lambda e_i + \frac{1}{2}\sum_{j=1}^N r_j^\top \Lambda r_j - \text{Tr}(W_e^\top W_r)</script><p>约束条件为：</p><script type="math/tex;mode=display">e_i = f_X W_e \phi_q(x_i), \quad i \in [N]</script><script type="math/tex;mode=display">r_j = f_X W_r \phi_k(x_j), \quad j \in [N]</script><h3 id="为什么公式这样构建"><a href="#为什么公式这样构建" class="headerlink" title="为什么公式这样构建"></a>为什么公式这样构建</h3><p>这个优化问题的构建基于以下几个关键原因：</p><ol><li><strong>处理自注意力的不对称性</strong>：传统图Transformer中的自注意力机制是$O(N²)$复杂度的且存在不对称性，违反了Mercer条件。该优化问题通过引入变分原理，使Primphormer能够处理非对称核。</li><li><strong>保持置换等变性</strong>：通过虚拟节点$f_X$聚合全局信息，保证图神经网络所需的置换等变性，这对于无序的图数据至关重要。</li><li><strong>降低计算复杂度</strong>：通过在原始空间中进行优化，避免了传统自注意力机制中需要计算所有节点对的问题。</li><li><strong>形成数据自适应基</strong>：通过将全局信息整合到投影权重中，而非特征映射中，构建双空间中的数据自适应基，增强了模型灵活性。（因此使用参数矩阵构建，而不是采用全局信息与特征输出相加）</li></ol><h3 id="各项含义"><a href="#各项含义" class="headerlink" title="各项含义"></a>各项含义</h3><p>目标函数$J$中的各项含义如下：</p><ol><li><strong>正则化项</strong>：<script type="math/tex">\frac{1}{2}\sum_{i=1}^N e_i^\top \Lambda e_i + \frac{1}{2}\sum_{j=1}^N r_j^\top \Lambda r_j</script><ul><li>这两项是对投影得分$e_i$和$r_j$的正则化，通过正定矩阵$\Lambda$防止过拟合</li><li>$\Lambda$是对角正则化系数矩阵，控制不同维度上的正则化强度</li></ul></li><li><strong>迹项</strong>：$-\text{Tr}(W_e^\top W_r)$<ul><li>这一项鼓励$W_e$和$W_r$之间的一致性</li><li>在KKT条件下，当满足优化条件时，与前两项达到平衡</li></ul></li><li><strong>约束条件中的参数</strong>：<ul><li>$W_e, W_r \in \mathbb{R}^{N_s \times p}$：可学习权重矩阵，其中$N_s \ll N$</li><li>$e_i, r_j \in \mathbb{R}^s$：投影得分</li><li>$\phi_q(\cdot), \phi_k(\cdot) : \mathbb{R}^d \to \mathbb{R}^p$：查询和键的特征映射</li><li><script type="math/tex">f_X\in\mathbb{R}^{s\times N_s}</script>：数据依赖投影，定义为<script type="math/tex">f_X := F + BX_1^N 1_{N_s}^\top</script>，作为虚拟节点聚合全局信息</li></ul></li></ol><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>数学推导过程主要包括以下步骤：</p><h4 id="1-拉格朗日函数"><a href="#1-拉格朗日函数" class="headerlink" title="1. 拉格朗日函数"></a>1. 拉格朗日函数</h4><p>首先构建拉格朗日函数：</p><script type="math/tex;mode=display">\mathcal{L}(W_e, W_r, e_i, r_j, h_e^i, h_r^j) = J - \sum_{i=1}^N h_e^{i\top}(e_i - f_X W_e \phi_q(x_i)) - \sum_{j=1}^N h_r^{j\top}(r_j - f_X W_r \phi_k(x_j))</script><p>其中$h_e^i, h_r^j \in \mathbb{R}^s$是与投影得分$e_i$和$r_j$相关的对偶变量向量。</p><h4 id="2-KKT条件"><a href="#2-KKT条件" class="headerlink" title="2. KKT条件"></a>2. KKT条件</h4><p>通过对拉格朗日函数求偏导，得到KKT条件：</p><ul><li><script type="math/tex;mode=display">\frac{\partial \mathcal{L}}{\partial W_e} = 0 \Rightarrow W_r = \sum_{i=1}^N f_X^\top h_e^i \phi_q(x_i)^\top</script></li><li><script type="math/tex;mode=display">\frac{\partial \mathcal{L}}{\partial W_r} = 0 \Rightarrow W_e = \sum_{j=1}^N f_X^\top h_r^j \phi_k(x_j)^\top</script></li><li><script type="math/tex;mode=display">\frac{\partial \mathcal{L}}{\partial e_i} = 0 \Rightarrow \Lambda e_i = h_e^i, \quad i \in [N]</script></li><li><script type="math/tex;mode=display">\frac{\partial \mathcal{L}}{\partial r_j} = 0 \Rightarrow \Lambda r_j = h_r^j, \quad j \in [N]</script></li><li><script type="math/tex;mode=display">\frac{\partial \mathcal{L}}{\partial h_e^i} = 0 \Rightarrow e_i = f_X W_e \phi_q(x_i), \quad i \in [N]</script></li><li><script type="math/tex;mode=display">\frac{\partial \mathcal{L}}{\partial h_r^j} = 0 \Rightarrow r_j = f_X W_r \phi_k(x_j), \quad j \in [N]</script></li></ul><h4 id="3-对偶问题"><a href="#3-对偶问题" class="headerlink" title="3. 对偶问题"></a>3. 对偶问题</h4><p>通过消除原始变量$W_e$和$W_r$，可以得到以下广义特征值问题：</p><script type="math/tex;mode=display">KH_r F_X = H_e \Sigma</script><script type="math/tex;mode=display">K^\top H_e F_X = H_r \Sigma</script><p>其中<script type="math/tex">F_X := f_X f_X^\top \in \mathbb{S}^{s \times s}_+</script>是自相关矩阵，<script type="math/tex">H_e := [h_e^1, \ldots, h_e^N]^\top \in \mathbb{R}^{N \times s}</script>和<script type="math/tex">H_r := [h_r^1, \ldots, h_r^N]^\top \in \mathbb{R}^{N \times s}</script>是对偶变量，<script type="math/tex">\Sigma := \Lambda^{-1}</script>，<script type="math/tex">K</script>是由注意力分数诱导的核矩阵。</p><h4 id="4-原始表示与对偶表示"><a href="#4-原始表示与对偶表示" class="headerlink" title="4. 原始表示与对偶表示"></a>4. 原始表示与对偶表示</h4><p>基于KKT条件，论文推导出自注意力的原始表示和对偶表示：<br><strong>原始表示</strong>：</p><script type="math/tex;mode=display">e(x) = f_X W_e \phi_q(x)</script><script type="math/tex;mode=display">r(x) = f_X W_r \phi_k(x)</script><p><strong>对偶表示</strong>：</p><script type="math/tex;mode=display">e(x) = \sum_{j=1}^N \tilde{h}_r^j \kappa(x, x_j)</script><script type="math/tex;mode=display">r(x) = \sum_{i=1}^N \tilde{h}_e^i \kappa(x_i, x)</script><p>其中$F_X := f_X f_X^\top$包含全局信息，$\tilde{h}_r^j := F_X h_r^j$和$\tilde{h}_e^i := F_X h_e^i$是数据自适应基，$\kappa(x_i, x_j) := \langle \phi_q(x_i), \phi_k(x_j) \rangle$是核函数。</p><h4 id="5-零值目标引理"><a href="#5-零值目标引理" class="headerlink" title="5. 零值目标引理"></a>5. 零值目标引理</h4><p>论文证明了在对偶空间中满足KKT条件的解会导致原始空间中的目标值为零（引理2.3）：</p><script type="math/tex;mode=display">J = \frac{1}{2}\text{Tr}(H_e \Sigma H_e^\top) + \frac{1}{2}\text{Tr}(H_r \Sigma H_r^\top) - \text{Tr}(K H_r F_X H_e^\top) = 0</script><p>这一结果为后续实现提供了理论基础，使得Primphormer可以通过简单地将原始目标值作为额外损失项来优化，而不需要直接求解复杂的对偶问题。<br>通过这种优化设计和推导，Primphormer能够高效地实现图Transformer，避免O(N²)的复杂度，同时保持置换等变性和表达能力。</p><h2 id="附录C"><a href="#附录C" class="headerlink" title="附录C"></a>附录C</h2><p>根据Primphormer论文中的定义，满足置换等变性后，节点重新排序不影响输出值，只是输出的排列顺序也会相应变化。</p><h3 id="置换等变性的正式定义"><a href="#置换等变性的正式定义" class="headerlink" title="置换等变性的正式定义"></a>置换等变性的正式定义</h3><p>论文中给出了置换等变性的明确定义（定义3.1）：</p><blockquote><p>一个连续的序列到序列函数 $f: X^N \to Y^N$ 如果对于每个排列 $\pi: [N] \to [N]$ 满足以下条件，则称其对序列中元素的顺序具有等变性：</p><script type="math/tex;mode=display">f([x_{\pi(1)}, \ldots, x_{\pi(N)}]) = [f_{\pi(1)}(X), \ldots, f_{\pi(N)}(X)]</script><p>其中 $X = [x_1, \ldots, x_N]$ 是包含 $N$ 个令牌的序列。</p></blockquote><h3 id="对图神经网络的重要性"><a href="#对图神经网络的重要性" class="headerlink" title="对图神经网络的重要性"></a>对图神经网络的重要性</h3><p>论文特别强调了图数据中置换等变性的重要性：</p><blockquote><p>“First, unlike sequences, nodes in a graph are unordered, meaning the sampling operation may break permutation equivariance, i.e., any permutation of the nodes could result in a different output.”</p></blockquote><p>这是因为图中的节点本质上是无序的(unlike sequences)，与序列数据不同。如果图神经网络不具有置换等变性，那么当对节点进行不同的排序时，可能会得到不一致的输出结果，这与图结构的基本特性相违背。</p><h3 id="Primphormer如何保持置换等变性"><a href="#Primphormer如何保持置换等变性" class="headerlink" title="Primphormer如何保持置换等变性"></a>Primphormer如何保持置换等变性</h3><p>论文指出，Primphormer通过引入虚拟节点(virtual node)机制来保持置换等变性：</p><blockquote><p>“We collect graph information by introducing a virtual node (Cai et al., 2023) that aggregates global information.”<br>“The global aggregation<script type="math/tex">f_X</script>preserves permutation equivariance.”</p></blockquote><p>这种全局聚合方式<script type="math/tex">f_X := F + BX_1^N 1^⊤_{N_s}</script>本质上是一个对称操作，对节点排列保持不变，从而确保了模型对节点重排的不变性。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>“满足置换等变性后节点重新排序不影响输出”的理解是正确的。更准确地说，满足置换等变性意味着：</p><ol><li>当输入序列中的元素进行重新排列时，输出序列也会以相同的方式进行排列</li><li>输出序列中元素本身的值不会发生改变</li><li>对于图神经网络，这意味着无论图中节点的顺序如何变化，只要图的结构（边和节点特征）保持不变，模型的处理结果就会保持一致<br>这一性质对于图神经网络至关重要，因为它尊重了图作为无序数据结构的基本特性。</li></ol><h2 id="附录D"><a href="#附录D" class="headerlink" title="附录D"></a>附录D</h2><p>KKT条件（Karush-Kuhn-Tucker条件）是优化理论中的一组重要条件，用于解决约束优化问题。这些条件是由Karush (1939)、Kuhn和Tucker (1951)提出的，是非线性规划领域中求解约束优化问题最优解的必要条件。</p><h3 id="在Primphormer论文中的应用"><a href="#在Primphormer论文中的应用" class="headerlink" title="在Primphormer论文中的应用"></a>在Primphormer论文中的应用</h3><p>在He等(2025)的论文中，KKT条件被用来建立优化问题(2.5)的对偶问题，这是Primphormer理论基础的关键部分。论文中定义了原始优化问题的拉格朗日函数：</p><script type="math/tex;mode=display">\mathcal{L}(W_e, W_r, e_i, r_j, h_{ei}, h_{rj}) = \frac{1}{2}\sum_{i=1}^{N} e_i^T \Lambda e_i + \frac{1}{2}\sum_{j=1}^{N} r_j^T \Lambda r_j - \text{Tr}(W_e^T W_r) - \sum_{i=1}^{N} h_{ei}^T (e_i - f_X W_e \phi_q(x_i)) - \sum_{j=1}^{N} h_{rj}^T (r_j - f_X W_r \phi_k(x_j))</script><p>通过求拉格朗日函数的偏导数并设为零，论文得到了KKT条件：</p><script type="math/tex;mode=display">\begin{cases}
\frac{\partial\mathcal{L}}{\partial W_e} = 0 &\Rightarrow W_r = \sum_{i=1}^{N} f_X^T h_{ei} \phi_q(x_i)^T \\
\frac{\partial\mathcal{L}}{\partial W_r} = 0 &\Rightarrow W_e = \sum_{j=1}^{N} f_X^T h_{rj} \phi_k(x_j)^T \\
\frac{\partial\mathcal{L}}{\partial e_i} = 0 &\Rightarrow \Lambda e_i = h_{ei}, \quad i \in [N] \\
\frac{\partial\mathcal{L}}{\partial r_j} = 0 &\Rightarrow \Lambda r_j = h_{rj}, \quad j \in [N] \\
\frac{\partial\mathcal{L}}{\partial h_{ei}} = 0 &\Rightarrow e_i = f_X W_e \phi_q(x_i), \quad i \in [N] \\
\frac{\partial\mathcal{L}}{\partial h_{rj}} = 0 &\Rightarrow r_j = f_X W_r \phi_k(x_j), \quad j \in [N]
\end{cases}</script><h3 id="KKT条件的主要内容"><a href="#KKT条件的主要内容" class="headerlink" title="KKT条件的主要内容"></a>KKT条件的主要内容</h3><p>KKT条件通常包括：</p><ol><li><strong>原始可行性条件</strong>：满足原始问题的约束条件</li><li><strong>对偶可行性条件</strong>：满足对偶变量的符号约束（对于不等式约束）</li><li><strong>互补松弛条件</strong>：原始约束和对应的对偶变量的乘积为零</li><li><strong>梯度条件</strong>：原始问题和对偶问题的梯度条件<h3 id="在Primphormer中的意义"><a href="#在Primphormer中的意义" class="headerlink" title="在Primphormer中的意义"></a>在Primphormer中的意义</h3><script type="math/tex;mode=display">K H_r F_X = H_e \Sigma, \quad K^T H_e F_X = H_r \Sigma</script>其中Σ = Λ⁻¹，He和Hr是对偶变量，K是由注意力分数形成的矩阵。这种原始-对偶关系证明了Primphormer的自注意力机制可以通过原始表示有效实现，同时避免了二次复杂度计算。<br>KKT条件在Primphormer中不仅为理论证明提供了基础，还在实际实现中起到了指导作用（引理2.3表明，当达到KKT点时，原始空间的目标函数值为零）。</li></ol><h2 id="附录E"><a href="#附录E" class="headerlink" title="附录E"></a>附录E</h2><h3 id="论文中提出的定理"><a href="#论文中提出的定理" class="headerlink" title="论文中提出的定理"></a>论文中提出的定理</h3><h4 id="定理-2-2-Duality"><a href="#定理-2-2-Duality" class="headerlink" title="定理 2.2 (Duality)"></a>定理 2.2 (Duality)</h4><p>He 等 (2025) 提出的这个定理描述了优化问题 (2.5) 在 KKT 条件下的对偶问题，即：</p><script type="math/tex;mode=display">KH_r F_X = H_e \Sigma, \quad K^T H_e F_X = H_r \Sigma</script><p>其中 $\Sigma = \Lambda^{-1}$，$H_e$ 和 $H_r$ 是对偶变量，$K$ 是由注意力分数诱导的矩阵。该定理建立了原始空间（Primal）与对偶空间（Dual）之间的关系，证明了 Primphormer 可以通过原始表示实现，从而避免二次复杂度计算。</p><h4 id="引理-2-3-Zero-valued-objective-with-stationary-solutions"><a href="#引理-2-3-Zero-valued-objective-with-stationary-solutions" class="headerlink" title="引理 2.3 (Zero-valued objective with stationary solutions)"></a>引理 2.3 (Zero-valued objective with stationary solutions)</h4><p>该引理指出，在对偶空间 (2.6) 中 $H_e, H_r, \Sigma$ 的解会导致原始空间 (2.5) 中目标值 $J$ 为零。这个结果支持了 Primphormer 的实现方式：通过最小化额外的目标损失函数有效逼近 KKT 点。</p><h5 id="定理2-3的证明"><a href="#定理2-3的证明" class="headerlink" title="定理2.3的证明"></a>定理2.3的证明</h5><p>定理2.3（引理2.3）表明：对偶空间(2.6)中He、Hr、Σ的解会导致原始空间(2.5)中目标值J为零。</p><h6 id="证明过程"><a href="#证明过程" class="headerlink" title="证明过程"></a>证明过程</h6><p>根据KKT条件（C2）和优化问题(2.6)，目标函数在平稳点上的值为：</p><script type="math/tex;mode=display">J = (1/2)∑_{i=1}^N e_i^T Λe_i + (1/2)∑_{j=1}^N r_j^T Λr_j - Tr(W_e^T W_r)</script><p>根据KKT条件中的<script type="math/tex">∂L/∂e_i = 0 ⇒ Λe_i = h_{ei}</script>和<script type="math/tex">∂L/∂r_j = 0 ⇒ Λr_j = h_{rj}</script>，我们有：</p><script type="math/tex;mode=display">J = (1/2)∑_{i=1}^N (h_{ei}^T Λ^{-1} h_{ei}) + (1/2)∑_{j=1}^N (h_{rj}^T Λ^{-1} h_{rj}) - Tr(W_e^T W_r)</script><p>令 $Σ = Λ^{-1}$ ，则上式可写为：</p><script type="math/tex;mode=display">J = (1/2)∑_{i=1}^N h_{ei}^T Σ h_{ei} + (1/2)∑_{j=1}^N h_{rj}^T Σ h_{rj} - Tr(W_e^T W_r)</script><p>根据KKT条件中的<script type="math/tex">∂L/∂W_e = 0 ⇒ W_r = ∑_{j=1}^N f_X^T h_{rj} φ_k(x_j)^T</script>和<script type="math/tex">∂L/∂W_r = 0 ⇒ W_e = ∑_{i=1}^N f_X^T h_{ei} φ_q(x_i)^T</script>，我们可以将迹项展开：</p><script type="math/tex;mode=display">J = (1/2) Tr(H_e Σ H_e^T) + (1/2) Tr(H_r Σ H_r^T) - Tr[∑_{i,j} φ_k(x_j) h_{rj}^T f_X f_X^T h_{ei} φ_q(x_i)^T]</script><p>其中<script type="math/tex">H_e := [h_{e1}, ..., h_{eN}]^T ∈ R^{N×s}</script>，<script type="math/tex">H_r := [h_{r1}, ..., h_{rN}]^T ∈ R^{N×s}</script>。<br>定义注意力矩阵 $K$，其中 $K_{ij} = ⟨φ_q(x_i), φ_k(x_j)⟩ = φ_q(x_i)^T φ_k(x_j)$，则：</p><script type="math/tex;mode=display">J = (1/2) Tr(H_e Σ H_e^T) + (1/2) Tr(H_r Σ H_r^T) - Tr[∑_{i,j} K_{ij} h_{rj}^T F_X h_{ei}]</script><p>其中 $F_X = f_X f_X^T$ 。由于 $KH_r F_X = H_e Σ$ 和 $K^T H_e F_X = H_r Σ$ ，我们可以将 $J$ 重写为：</p><script type="math/tex;mode=display">J = (1/2) Tr(KH_r F_X H_e^T) + (1/2) Tr(K^T H_e F_X H_r^T) - Tr(KH_r F_X H_e^T)</script><p>简化后：</p><script type="math/tex;mode=display">J = (1/2) Tr(K^T H_e F_X H_r^T) - (1/2) Tr(KH_r F_X H_e^T)
= (1/2) Tr[H_e F_X H_r^T K^T - KH_r F_X H_e^T]
= (1/2) Tr[M - M^T]  其中 M = H_e F_X H_r^T K^T
= 0</script><p>对于任何矩阵 $M$ ，都有 $Tr[M - M^T] = 0$ ，因为 $M - M^T$ 是斜对称矩阵，其迹为零。</p><h5 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h5><p>因此，我们证明了：在对偶空间(2.6)中 $H_e$、$H_r$ 、$Σ$ 的解会导致原始空间(2.5)中目标值 $J$ 为零。这个结果支持了Primphormer的实现方式：通过最小化额外的目标损失函数有效逼近KKT点。</p><h4 id="定理-3-2-Universal-approximation-for-permutation-equivariant-sequence-to-sequence-functions"><a href="#定理-3-2-Universal-approximation-for-permutation-equivariant-sequence-to-sequence-functions" class="headerlink" title="定理 3.2 (Universal approximation for permutation equivariant sequence-to-sequence functions)"></a>定理 3.2 (Universal approximation for permutation equivariant sequence-to-sequence functions)</h4><p>该定理证明了 Primphormer 作为置换等变序列函数的通用逼近器：<br>对于任意函数<script type="math/tex">f \in \mathcal{FE}_q^N(\mathcal{X}, \mathcal{Y})</script>和每个<script type="math/tex">\varepsilon > 0</script>，存在一个 Primphormer<script type="math/tex">T_{Pri}</script>，使得</p><script type="math/tex;mode=display">\sup_{X \in \mathcal{X}^N} \|f(X) - T_{Pri}(X)\|_\infty < \varepsilon</script><h5 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h5><p>我们将证明分为两个部分：首先用Sumformer逼近f，然后用Primphormer逼近Sumformer。证明基于三角不等式：</p><script type="math/tex;mode=display">\sup_{X\in X^N} \|f (X) - T_{Pri}(X)\|_\infty \leq \sup_{X\in X^N} \|f (X) - S(X)\|_\infty + \sup_{X\in X^N} \|S(X) - T_{Pri}(X)\|_\infty</script><h6 id="第一步：用Sumformer逼近f"><a href="#第一步：用Sumformer逼近f" class="headerlink" title="第一步：用Sumformer逼近f"></a>第一步：用Sumformer逼近f</h6><p>根据引理C.2（Alberti等, 2023）：</p><blockquote><p>引理C.2：对于每个函数 $f ∈ Fe^N_q(X, Y)$ 和每个 $ε &gt; 0$ ，存在一个Sumformer S，使得</p><script type="math/tex;mode=display">\sup_{X\in X^N} \|f (X) - S(X)\|_\infty < \varepsilon</script></blockquote><p>我们选择一个Sumformer S，使得<script type="math/tex">\sup_{X\in X^N} \|f (X) - S(X)\|_\infty < \varepsilon/2</script>。Sumformer的定义如下：</p><blockquote><p>定义C.1（Sumformer）：设d’ ∈ N且有两个函数<script type="math/tex">ξ : X → R^{d'}</script>，<script type="math/tex">ψ : X × R^{d'} → Y</script>。Sumformer是一个序列到序列函数<script type="math/tex">S : X^N → Y^N</script>，首先计算：</p><script type="math/tex;mode=display">\Xi := \sum_{k=1}^N \xi(x_k)</script><p>然后，$S([x_1, …, x_N]) := [ψ(x_1, Ξ), …, ψ(x_N, Ξ)]$ 。</p></blockquote><h6 id="第二步：用Primphormer逼近Sumformer"><a href="#第二步：用Primphormer逼近Sumformer" class="headerlink" title="第二步：用Primphormer逼近Sumformer"></a>第二步：用Primphormer逼近Sumformer</h6><p>现在我们构造一个Primphormer $T_{Pri}$ 来近似Sumformer S：</p><ol><li>首先将输入X通过前馈层变换：<script type="math/tex;mode=display">[x_1 ... x_N; x_1 ... x_N] \in R^{2d \times N}</script></li><li>构造一个两层前馈网络，在前N个分量上执行恒等变换，同时近似函数ξ：<script type="math/tex;mode=display">[x_1 ... x_N; \xi(x_1) ... \xi(x_N)] \in R^{(d+d') \times N}</script></li><li>添加线性映射产生输出：<script type="math/tex;mode=display">[1 ... 1; x_1 ... x_N; \xi(x_1) ... \xi(x_N)] \in R^{(1+d+d') \times N}</script></li><li><p>使用注意力机制表示 $Ξ = ∑_{i=1}^N ξ(x_i)$ ：</p><ul><li>设置 $W_q = W_k = [e_1, 0^{(1+d+2d’)×(d+2d’)}]$ ，其中 $e_1 = [1, 0^{1×(d+2d’)}]^T$</li><li>数据依赖投影<script type="math/tex">f(X) = BX1_N1^T_{N_s}</script>，其中<script type="math/tex">B = [0^{d'×1}, 0^{d'×d}, I_{d'}, 0^{d'×d'}]</script></li><li>得到投影分数：$[Ξ, …, Ξ] ∈ R^{d’×N}$</li></ul></li><li><p>连接投影分数并通过兼容矩阵 $W_c$ 产生最终输出，应用残差连接：</p><script type="math/tex;mode=display">[1 ... 1; x_1 ... x_N; \xi(x_1) ... \xi(x_N); \Xi ... \Xi] \in R^{(1+d+2d') \times N}</script><p>通过这种架构，Primphormer的注意力模块被设计用来计算Sumformer的聚合，而其余部分保持不变。因此，我们可以构造Primphormer<script type="math/tex">T_{Pri}</script>，使得<script type="math/tex">\sup_{X\in X^N} \|S(X) - T_{Pri}(X)\|_\infty < \varepsilon/2</script>。</p></li></ol><h5 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h5><p>利用三角不等式，我们得到：</p><script type="math/tex;mode=display">\sup_{X\in X^N} \|f (X) - T_{Pri}(X)\|_\infty < \varepsilon/2 + \varepsilon/2 = \varepsilon</script><p>这就完成了定理3.2的证明，表明Primphormer是置换等变序列到序列函数的通用逼近器。<br>值得注意的是，证明的关键在于将问题分解为两个可管理的部分，并分别用Sumformer和Primphormer的通用逼近性质来实现整体近似。这种方法受到Alberti等(2023)工作的启发，并结合了Primphormer的特定架构特性。</p><h4 id="定理-3-3-Universal-approximation-for-arbitrary-continuous-sequence-functions"><a href="#定理-3-3-Universal-approximation-for-arbitrary-continuous-sequence-functions" class="headerlink" title="定理 3.3 (Universal approximation for arbitrary continuous sequence functions)"></a>定理 3.3 (Universal approximation for arbitrary continuous sequence functions)</h4><p>该定理证明带有位置编码的 Primphormer 能近似任意连续序列函数：<br>对于任意连续函数 $f : [0,1]^{d \times N} \rightarrow \mathbb{R}^{d \times N}$ 和每个 $\varepsilon &gt; 0$，存在一个带有位置编码 $E$ 的 Primphormer $T^{PE}$，使得</p><script type="math/tex;mode=display">\sup_{X \in \mathcal{X}^N} \|f(X) - T^{PE}(X)\|_\infty < \varepsilon</script><h5 id="证明-1"><a href="#证明-1" class="headerlink" title="证明"></a>证明</h5><h6 id="第一步：引入位置编码与分片常数函数近似"><a href="#第一步：引入位置编码与分片常数函数近似" class="headerlink" title="第一步：引入位置编码与分片常数函数近似"></a>第一步：引入位置编码与分片常数函数近似</h6><p>由于目标函数 $f$ 是连续的，其分量函数<script type="math/tex">g(x_k, {x_i | i≠k})</script>也是连续的。考虑<script type="math/tex">X ∈ [0,1]^{d×N}</script>作为紧致集，<script type="math/tex">f</script>在紧集上一致连续。给定<script type="math/tex">ε > 0</script>，存在<script type="math/tex">δ > 0</script>使得当<script type="math/tex">∥X - X'∥ < δ</script>时，有<script type="math/tex">∥f(X) - f(X')∥_{∞} < ε/2</script>。<br>定义网格：</p><script type="math/tex;mode=display">G_\delta = \{0, \delta, 2\delta, \ldots, 1-\delta\}^{d \times N}</script><p>将空间划分为超立方体网格<script type="math/tex">C_P = [P_1, P_1+δ) × ⋯ × [P_{d×N}, P_{d×N}+δ)</script>，其中<script type="math/tex">P ∈ G_δ</script>。构造分片常数函数：</p><script type="math/tex;mode=display">\tilde{g}(X) = \sum_{P \in G_\delta} g(P) \cdot \mathbf{1}_{X \in C_P}</script><p>由于g的一致连续性，对于足够小的δ，有：</p><script type="math/tex;mode=display">\sup_{X} \|g(X) - \tilde{g}(X)\|_\infty < \frac{\varepsilon}{4}</script><p>引入位置编码<script type="math/tex">E ∈ R^{d×N}</script>：</p><script type="math/tex;mode=display">E = \begin{bmatrix}
0 & 1 & 2 & \cdots & N-1 & 0 & 1 & \cdots & N-1 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 1 & 2 & \cdots & N-1 & 0 & 1 & \cdots & N-1
\end{bmatrix}</script><p>使得X+E的第k个列向量位于区间[k-1, k)内，不同token位于互不相交的区间。</p><h6 id="第二步：构建Sumformer-S"><a href="#第二步：构建Sumformer-S" class="headerlink" title="第二步：构建Sumformer S"></a>第二步：构建Sumformer S</h6><p>定义映射 $l: R^d → R$ ，对于列向量 $H_i ∈ R^d$：</p><script type="math/tex;mode=display">u = \left(\frac{1-\delta}{N}, \delta^{-1}, \ldots, \delta^{1-d}\right)^\top \in R^d</script><script type="math/tex;mode=display">l(H_i) = u^\top H_i</script><p>该映射将不同列映射到不同区间：<script type="math/tex">0 ≤ l(H_1) < l(H_2) < ⋯ < l(H_N) < 1</script>，且对任意排列π满足<script type="math/tex">l(H_π(1)) < l(H_π(2)) < ⋯ < l(H_π(N))</script>。<br>利用Kolmogorov-Arnold表示定理（Khesin &amp; Tabachnikov, 2014），定义从位置标识<script type="math/tex">b = [l(H_i)|i∈[N]]</script>到网格索引的函数<script type="math/tex">μ: [0,1]^N → N</script>：</p><script type="math/tex;mode=display">\mu(b) = \rho\left(\sum_{n=1}^N \lambda_n \phi(b_n)\right)</script><p>其中<script type="math/tex">ρ: R^{2N+1} → R</script>和<script type="math/tex">φ: R → R^{2N+1}</script>是连续函数。<br>定义聚合函数：</p><script type="math/tex;mode=display">\Xi = \sum_{n=1}^N \xi(b_n) = \sum_{n=1}^N \lambda_n \phi(b_n)</script><p>并定义：</p><script type="math/tex;mode=display">\psi(x_k, \Xi) = \tilde{g}(\iota(\chi^{-1} \circ \mu^{-1} \circ \rho(\Xi)) - E)</script><p>其中<script type="math/tex">ι: P → (P_k, P_{i≠k})</script>将网格点分解为当前token与其他token，χ将网格点映射到位置标识b。<br>构建Sumformer S:</p><script type="math/tex;mode=display">S([x_1, \ldots, x_N]) = [\psi(x_1, \Xi), \ldots, \psi(x_N, \Xi)]</script><p>由于<script type="math/tex">g̃</script>精确表示网格点上的值且位置编码保持顺序，有：</p><script type="math/tex;mode=display">\sup_{X} \|f(X+E) - S(X+E)\|_\infty < \frac{\varepsilon}{2}</script><h6 id="第三步：用Primphormer逼近Sumformer"><a href="#第三步：用Primphormer逼近Sumformer" class="headerlink" title="第三步：用Primphormer逼近Sumformer"></a>第三步：用Primphormer逼近Sumformer</h6><p>根据定理3.2的证明思路（Alberti等, 2023），Primphormer可以逼近置换等变函数。尽管此处S不是置换等变的，但位置编码E固定了顺序，使得：</p><script type="math/tex;mode=display">\sup_{X} \|S(X+E) - T_{Pri}(X+E)\|_\infty < \frac{\varepsilon}{2}</script><p>其中 $T_{Pri}$ 为Primphormer，通过以下方式构建：</p><ol><li>前馈变换：$[x_1 \cdots x_N; x_1 \cdots x_N] \in R^{2d \times N}$</li><li>两层前馈网络保持前N个分量不变，同时近似ξ函数</li><li>添加线性映射与数据依赖投影计算$\Xi = \sum_{i=1}^N \xi(x_i)$</li><li>通过兼容矩阵 $W_c$ 输出最终结果<h6 id="第四步：整合结果"><a href="#第四步：整合结果" class="headerlink" title="第四步：整合结果"></a>第四步：整合结果</h6>由三角不等式：<script type="math/tex;mode=display">\sup_{X} \|f(X) - T_{PE}(X)\|_\infty = \sup_{X} \|f(X+E) - T_{PE}(X+E)\|_\infty</script><script type="math/tex;mode=display">\leq \sup_{X} \|f(X+E) - S(X+E)\|_\infty + \sup_{X} \|S(X+E) - T_{PE}(X+E)\|_\infty</script><script type="math/tex;mode=display">< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon</script>其中<script type="math/tex">T_{PE}(X) = T_{Pri}(X+E)</script>。<h5 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h5>定理3.3证明完成：对于任意连续函数f和ε &gt; 0，存在位置编码E和Primphormer TPE，使得在 $[0,1]^{d×N}$ 上一致逼近f的误差小于ε。这一结果证明了Primphormer在添加适当位置编码后，可以作为任意连续序列到序列函数的通用逼近器（Alberti等, 2023）。<h4 id="定理-3-4-Expressiveness-in-terms-of-1-WL"><a href="#定理-3-4-Expressiveness-in-terms-of-1-WL" class="headerlink" title="定理 3.4 (Expressiveness in terms of 1-WL)"></a>定理 3.4 (Expressiveness in terms of 1-WL)</h4>该定理证明了 Primphormer 的表达能力与 1 维 Weisfeiler-Lehman 算法（1-WL）相当：<br>设 $G = (V, E, l)$ 是有 $N$ 个节点的标记图，节点特征矩阵 $X^{(0)} := H \in \mathbb{R}^{d \times N}$ 与标签 $l$ 一致。那么对于所有迭代 $t \geq 0$，存在 Primphormer 的参数化，使得<script type="math/tex;mode=display">C_{1,t}(v) = C_{1,t}(w) \Leftrightarrow X^{(t)}(v) = X^{(t)}(w)</script>对于所有节点 $v, w \in V$，其中 $C^t_{1}$ 是 1-WL 测试在第 $t$ 次迭代中的着色函数。<h5 id="证明-2"><a href="#证明-2" class="headerlink" title="证明"></a>证明</h5><h6 id="1-初始化"><a href="#1-初始化" class="headerlink" title="1. 初始化"></a>1. 初始化</h6>根据引理C.7，存在一个初始化$X^{(0)}$的参数化，使得对于每个顶点$v \in V$：<script type="math/tex;mode=display">X^{(0)}(v) = [H'(v); 0; \deg'(v); P'(v)],</script>并且满足：<script type="math/tex;mode=display">H(v) = H(w) \iff H'(v) = H'(w),</script><script type="math/tex;mode=display">\deg(v) = \deg(w) \iff \deg'(v) = \deg'(w),</script><script type="math/tex;mode=display">P(v) = P(w) \iff P'(v) = P'(v),</script>其中$d = 2s + r + k$。我们使用归纳法进行证明。<br>首先，根据1-WL测试的定义，我们有：<script type="math/tex;mode=display">C_{1,0}(v) = C_{1,0}(w) \iff H(v) = H(w).</script>令$H^{(t)}(v)$表示迭代$t$时节点$v$的颜色表示。设$D^{\text{emb}} \in \mathbb{R}^{r\times N}$，使得对于第$i$列$D^{\text{emb}}_i = \deg’(v_i)$，其中$v_i$是某个固定但任意的节点排序中的第$i$个节点。那么$X^{(0)}$可以写为：<script type="math/tex;mode=display">X^{(0)} = [H^{(0)}; 0; D^{\text{emb}}; P'] \in \mathbb{R}^{d\times N}.</script><h6 id="2-归纳假设"><a href="#2-归纳假设" class="headerlink" title="2. 归纳假设"></a>2. 归纳假设</h6>假设命题在迭代$t$时成立，即存在Primphormer的参数化，使得：<script type="math/tex;mode=display">C_{1,t}(v) = C_{1,t}(w) \iff H^{(t)}(v) = H^{(t)}(w).</script>现在我们证明命题在$t+1$时也成立。为此，我们需要：<script type="math/tex;mode=display">C_{1,t+1}(v) = C_{1,t+1}(w) \iff H^{(t+1)}(v) = H^{(t+1)}(w).</script>这意味着$X^{(t+1)}$的第一个元素应该匹配1-WL等价的聚合：<script type="math/tex;mode=display">X^{(t+1)} = [H^{(t+1)}; 0; D^{\text{emb}}; P'] \in \mathbb{R}^{d\times N}.</script><h6 id="3-1-WL等价聚合"><a href="#3-1-WL等价聚合" class="headerlink" title="3. 1-WL等价聚合"></a>3. 1-WL等价聚合</h6>根据引理C.4，我们知道1-WL等价聚合遵循：<script type="math/tex;mode=display">H^{(t+1)} := \text{FFN}_{\text{WL}}[H^{(t)} + 2H^{(t)}A(G)],</script>其中$\text{FFN}_{\text{WL}}$是更新颜色的前馈层。因此，我们需要证明Primphormer能够模拟这一聚合过程。<h6 id="4-Primphormer参数化"><a href="#4-Primphormer参数化" class="headerlink" title="4. Primphormer参数化"></a>4. Primphormer参数化</h6>考虑Primphormer的输出$o(x) = W_c [e(x); r(x)]$，其中：<script type="math/tex;mode=display">e(x) = f_X W_e \phi_q(x),</script><script type="math/tex;mode=display">r(x) = f_X W_r \phi_k(x),</script><script type="math/tex;mode=display">f_X = F + BX1_N1^\top_{N_s}.</script>通过适当设置参数，我们可以将Primphormer参数化为$o(x) = e(x) = W_e \phi_q(x)$。设$\phi_q(x) := q(x)/|q(x)|_2$和$\phi_k(x) := k(x)/|k(x)|_2$，其中$q(x) = W_qx$和$k(x) = W_kx$。<br>将$W_q$和$W_k$进行分解：<script type="math/tex;mode=display">W_q = [W_{1,q}, W_{2,q}, W_{3,q}, W_{4,q}] \in \mathbb{R}^{d\times d},</script><script type="math/tex;mode=display">W_k = [W_{1,k}, W_{2,k}, W_{3,k}, W_{4,k}] \in \mathbb{R}^{d\times d},</script>其中子矩阵维度分别为<script type="math/tex">W_{q1}, W_{1,k} \in \mathbb{R}^{d\times s}</script>,<script type="math/tex">W_{q2}, W_{2,k} \in \mathbb{R}^{d\times s}</script>,<script type="math/tex">W_{q3}, W_{3,k} \in \mathbb{R}^{d\times r}</script>,<script type="math/tex">W_{q4}, W_{4,k} \in \mathbb{R}^{d\times k}</script>。<br>根据KKT条件，我们可以在行空间中重新参数化<script type="math/tex">W_e</script>：<script type="math/tex;mode=display">W_e = H\phi_k(X^{(t)})^\top,</script>其中$H$是一个由权重向量构成的矩阵。因此，Primphormer的输出可以表示为：<script type="math/tex;mode=display">o(X^{(t)}) = H\phi_k(X^{(t)})^\top\phi_q(X^{(t)}).</script><h6 id="5-模拟图拉普拉斯算子"><a href="#5-模拟图拉普拉斯算子" class="headerlink" title="5. 模拟图拉普拉斯算子"></a>5. 模拟图拉普拉斯算子</h6>通过设置合适的参数，我们可以使Primphormer模拟图拉普拉斯算子的作用。根据引理C.7，结构嵌入$P’$可以恢复(归一化)图拉普拉斯算子，即$P’^\top P’ = L$。<br>经过一系列变换，Primphormer的输出可以重写为：<script type="math/tex;mode=display">o(X^{(t)}) = HD^{-1/2}LD^{-1/2} = H[I - D^{-1/2}A(G)D^{-1/2}],</script>其中$L = D - A$是图拉普拉斯算子，$D$是度矩阵，$A$是邻接矩阵。<h6 id="6-完成归纳步骤"><a href="#6-完成归纳步骤" class="headerlink" title="6. 完成归纳步骤"></a>6. 完成归纳步骤</h6>最后，我们得到Primphormer的输出为：<script type="math/tex;mode=display">\text{Prim}(X^{(t)}) = [0; H^{(t)}(D^{1/2} - A(G)D^{-1/2}); 0; 0].</script>结合模型结构 $\text{FFN}(X + \text{Prim}(X))$ ，Primphormer计算下一个表示：<script type="math/tex;mode=display">X^{(t+1)} = \text{FFN}[X^{(t)} + \text{Prim}(X^{(t)})].</script>通过定义合适的函数<script type="math/tex">f_{\text{FFN}}, f_{\text{lin2}}, f_{\text{lin1}}, f_{\text{deg}}</script>，我们可以使Primphormer精确地模拟1-WL的聚合过程，得到：<script type="math/tex;mode=display">X^{(t+1)} = [H^{(t+1)}; 0; D^{\text{emb}}; P'].</script>其中$H^{(t+1)} = \text{FFN}_{\text{WL}}[H^{(t)} + 2H^{(t)}A(G)]$正是1-WL测试在第$t+1$次迭代的特征。<br>因此，我们证明了存在Primphormer的参数化，使得：<script type="math/tex;mode=display">C_{1,t+1}(v) = C_{1,t+1}(w) \iff H^{(t+1)}(v) = H^{(t+1)}(w).</script>根据引理C.5和定理C.8，我们知道Transformer和Primphormer在区分非同构图方面都能够模拟1-WL测试，表明了Primphormer保持了与标准Transformer相同的表达能力。<h4 id="推论-3-5-Expressiveness-comparison-with-Transformer"><a href="#推论-3-5-Expressiveness-comparison-with-Transformer" class="headerlink" title="推论 3.5 (Expressiveness comparison with Transformer)"></a>推论 3.5 (Expressiveness comparison with Transformer)</h4>该推论是定理 3.4 的延伸，指出 Transformer 和 Primphormer 在区分非同构图方面具有相同的表达能力：<br>设 $G = (V, E, l)$ 是有 $N$ 个节点的标记图，节点特征矩阵 $X^{(0)} := H \in \mathbb{R}^{d \times N}$ 与标签 $l$ 一致。那么对于所有迭代 $t \geq 0$，存在 Transformer 和 Primphormer 的参数化以及位置编码，使得<script type="math/tex;mode=display">X^{(t)}_T(v) = X^{(t)}_T(w) \Leftrightarrow X^{(t)}_{Pri}(v) = X^{(t)}_{Pri}(w)</script>对于所有节点 $v, w \in V$。<h3 id="论文中引用的其他重要定理"><a href="#论文中引用的其他重要定理" class="headerlink" title="论文中引用的其他重要定理"></a>论文中引用的其他重要定理</h3><h4 id="定义-2-1-Asymmetric-kernel-trick"><a href="#定义-2-1-Asymmetric-kernel-trick" class="headerlink" title="定义 2.1 (Asymmetric kernel trick)"></a>定义 2.1 (Asymmetric kernel trick)</h4>引用自 Wright &amp; Gonzalez (2021) 、Lin et al. (2022) 、He et al. (2023a) 和 Chen et al. (2023) ，定义了来自再生核巴拿赫空间（RKBS）的非对称核技巧：<script type="math/tex;mode=display">\kappa(x, z) = \langle \phi_q(x), \phi_k(z) \rangle</script><h4 id="表示定理-Representer-theorem"><a href="#表示定理-Representer-theorem" class="headerlink" title="表示定理 (Representer theorem)"></a>表示定理 (Representer theorem)</h4>引用自 Kimeldorf &amp; Wahba (1971) ，描述了原始空间和对偶空间之间的最优解关系：<script type="math/tex;mode=display">g(\xi_i) = \sum_j \alpha_j \kappa(\xi_i, \xi_j) = \langle w, \phi(\xi_i) \rangle</script><h4 id="引理-C-2-Universal-approximation-of-Sumformer"><a href="#引理-C-2-Universal-approximation-of-Sumformer" class="headerlink" title="引理 C.2 (Universal approximation of Sumformer)"></a>引理 C.2 (Universal approximation of Sumformer)</h4>引用自 Alberti et al. (2023) ，证明了 Sumformer 可以作为置换等变函数的通用逼近器。<h4 id="引理-C-3-Kolmogorov-Arnold-representation"><a href="#引理-C-3-Kolmogorov-Arnold-representation" class="headerlink" title="引理 C.3 (Kolmogorov-Arnold representation)"></a>引理 C.3 (Kolmogorov-Arnold representation)</h4>引用自 Khesin &amp; Tabachnikov (2014) 和 Zaheer et al. (2017) ，证明了任意多元连续函数有特定表示形式：<script type="math/tex;mode=display">f(x_1, \cdots, x_N) = \rho\left(\sum_{n=1}^N \lambda_n \varphi(x_n)\right)</script><h4 id="引理-C-4-Theorem-VIII-4-in-Grohe-2021"><a href="#引理-C-4-Theorem-VIII-4-in-Grohe-2021" class="headerlink" title="引理 C.4 (Theorem VIII.4 in Grohe, 2021)"></a>引理 C.4 (Theorem VIII.4 in Grohe, 2021)</h4>引用自 Grohe (2021) ，说明了 GNN 如何模拟 1-WL 测试过程。<h4 id="引理-C-5-Theorem-2-in-Muller-amp-Morris-2024"><a href="#引理-C-5-Theorem-2-in-Muller-amp-Morris-2024" class="headerlink" title="引理 C.5 (Theorem 2 in Müller &amp; Morris, 2024)"></a>引理 C.5 (Theorem 2 in Müller &amp; Morris, 2024)</h4>引用自 Müller &amp; Morris (2024) ，证明了标准 Graph Transformer 可以模拟 1-WL 测试。<h4 id="定义-C-6-LAP-and-SPE"><a href="#定义-C-6-LAP-and-SPE" class="headerlink" title="定义 C.6 (LAP and SPE)"></a>定义 C.6 (LAP and SPE)</h4>引用自 Kreuzer et al. (2021) 和 Huang et al. (2024) ，定义了两种结构嵌入方法：</li></ol><ul><li>LAP: $\psi(V^T_1, \lambda), \cdots, \psi(V^T_N, \lambda)$</li><li>SPE: $V \varphi_1(\lambda) V^T, \cdots, V \varphi_m(\lambda) V^T$</li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://epsilonzyj.github.io">EpsilonZ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://epsilonzyj.github.io/posts/Primphormer.html">https://epsilonzyj.github.io/posts/Primphormer.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://epsilonzyj.github.io" target="_blank">EpsilonZ's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Graph-ML/">Graph ML</a><a class="post-meta__tags" href="/tags/Graph-Transformer/">Graph Transformer</a><a class="post-meta__tags" href="/tags/Linear-Transformer/">Linear Transformer</a></div><div class="post-share"><div class="social-share" data-image="/img/GNNCover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/90649c8.html" title="动态图基础 ｜ Dynamic Graphs"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">动态图基础 ｜ Dynamic Graphs</div></div><div class="info-2"><div class="info-item-1">动态图基础介绍，包括什么是动态图，目前与GNN结合的前沿方向等</div></div></div></a><a class="pagination-related" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">论文阅读：Simple Path Structural Encoding</div></div><div class="info-2"><div class="info-item-1">Simple Path Structural Encoding for Graph Transformers</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/DUALFormer.html" title="论文阅读：DUALFormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-22</div><div class="info-item-2">论文阅读：DUALFormer</div></div><div class="info-2"><div class="info-item-1">DUALFormer:Dual Graph Transformer</div></div></div></a><a class="pagination-related" href="/posts/NAGphormer.html" title="论文阅读：NAGphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-17</div><div class="info-item-2">论文阅读：NAGphormer</div></div><div class="info-2"><div class="info-item-1">NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs</div></div></div></a><a class="pagination-related" href="/posts/NTFormer.html" title="论文阅读：NTFormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-21</div><div class="info-item-2">论文阅读：NTFormer</div></div><div class="info-2"><div class="info-item-1">NTFormer:A Composite Node Tokenized Graph Transformer for Node Classification</div></div></div></a><a class="pagination-related" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-23</div><div class="info-item-2">Graph Transformer中的问题 ｜ Problems With Graph Transformers</div></div><div class="info-2"><div class="info-item-1">Graph Transformer中存在的问题</div></div></div></a><a class="pagination-related" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-27</div><div class="info-item-2">论文阅读：Simple Path Structural Encoding</div></div><div class="info-2"><div class="info-item-1">Simple Path Structural Encoding for Graph Transformers</div></div></div></a><a class="pagination-related" href="/posts/Vcr-Graphormer.html" title="论文阅读：Vcr-Graphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-21</div><div class="info-item-2">论文阅读：Vcr-Graphormer</div></div><div class="info-2"><div class="info-item-1">Vcr-graphormer:A mini-batch graph transformer via virtual connections</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_2179.JPG" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">EpsilonZ</div><div class="author-info-description">To sleep, or to research, that is the question.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/EpsilonZYJ"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/EpsilonZYJ" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:biopic.tweeter_2u@icloud.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Research everyday!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#Metadata"><span class="toc-number">1.</span> <span class="toc-text">Metadata</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number"></span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91"><span class="toc-number"></span> <span class="toc-text">论文摘要翻译</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E9%97%AE%E9%A2%98"><span class="toc-number"></span> <span class="toc-text">研究问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="toc-number"></span> <span class="toc-text">核心问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number"></span> <span class="toc-text">现有解决方案的局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E6%8C%91%E6%88%98"><span class="toc-number"></span> <span class="toc-text">理论挑战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E5%88%87%E5%85%A5%E7%82%B9"><span class="toc-number"></span> <span class="toc-text">创新切入点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E5%9B%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E5%85%B7%E4%BD%93%E6%8C%91%E6%88%98"><span class="toc-number"></span> <span class="toc-text">针对图数据的具体挑战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number"></span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number"></span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Primphormer%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95"><span class="toc-number"></span> <span class="toc-text">Primphormer核心方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%97%AE%E9%A2%98%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="toc-number"></span> <span class="toc-text">1. 问题与动机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8E%9F%E5%A7%8B-%E5%AF%B9%E5%81%B6%E6%A1%86%E6%9E%B6"><span class="toc-number"></span> <span class="toc-text">2. 原始-对偶框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E4%B8%8D%E5%AF%B9%E7%A7%B0%E6%A0%B8%E6%8A%80%E5%B7%A7"><span class="toc-number">1.</span> <span class="toc-text">2.1 不对称核技巧</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E8%99%9A%E6%8B%9F%E8%8A%82%E7%82%B9%E4%B8%8E%E5%85%A8%E5%B1%80%E8%81%9A%E5%90%88"><span class="toc-number">2.</span> <span class="toc-text">2.2 虚拟节点与全局聚合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%9E%84%E5%BB%BA"><span class="toc-number">3.</span> <span class="toc-text">2.3 优化问题构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E4%B8%8E%E5%8E%9F%E5%A7%8B%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.</span> <span class="toc-text">2.4 对偶问题与原始表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E8%BE%93%E5%87%BA"><span class="toc-number">5.</span> <span class="toc-text">2.5 输出</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%9E%E7%8E%B0%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-number"></span> <span class="toc-text">3. 实现与架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E8%B4%A1%E7%8C%AE"><span class="toc-number"></span> <span class="toc-text">理论贡献</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86"><span class="toc-number"></span> <span class="toc-text">1. 通用近似定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%E4%BF%9D%E7%95%99"><span class="toc-number"></span> <span class="toc-text">2. 表达能力保留</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number"></span> <span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number"></span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number"></span> <span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95A"><span class="toc-number"></span> <span class="toc-text">附录A</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E8%8A%82%E7%82%B9%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="toc-number"></span> <span class="toc-text">虚拟节点的实现方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE"><span class="toc-number"></span> <span class="toc-text">数学表达</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95B"><span class="toc-number"></span> <span class="toc-text">附录B</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%85%AC%E5%BC%8F%E8%BF%99%E6%A0%B7%E6%9E%84%E5%BB%BA"><span class="toc-number"></span> <span class="toc-text">为什么公式这样构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%84%E9%A1%B9%E5%90%AB%E4%B9%89"><span class="toc-number"></span> <span class="toc-text">各项含义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC"><span class="toc-number"></span> <span class="toc-text">数学推导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="toc-number">1.</span> <span class="toc-text">1. 拉格朗日函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-KKT%E6%9D%A1%E4%BB%B6"><span class="toc-number">2.</span> <span class="toc-text">2. KKT条件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">3.</span> <span class="toc-text">3. 对偶问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%8E%9F%E5%A7%8B%E8%A1%A8%E7%A4%BA%E4%B8%8E%E5%AF%B9%E5%81%B6%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.</span> <span class="toc-text">4. 原始表示与对偶表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E9%9B%B6%E5%80%BC%E7%9B%AE%E6%A0%87%E5%BC%95%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">5. 零值目标引理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95C"><span class="toc-number"></span> <span class="toc-text">附录C</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%AE%E6%8D%A2%E7%AD%89%E5%8F%98%E6%80%A7%E7%9A%84%E6%AD%A3%E5%BC%8F%E5%AE%9A%E4%B9%89"><span class="toc-number"></span> <span class="toc-text">置换等变性的正式定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number"></span> <span class="toc-text">对图神经网络的重要性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Primphormer%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8C%81%E7%BD%AE%E6%8D%A2%E7%AD%89%E5%8F%98%E6%80%A7"><span class="toc-number"></span> <span class="toc-text">Primphormer如何保持置换等变性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number"></span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95D"><span class="toc-number"></span> <span class="toc-text">附录D</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8Primphormer%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number"></span> <span class="toc-text">在Primphormer论文中的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KKT%E6%9D%A1%E4%BB%B6%E7%9A%84%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9"><span class="toc-number"></span> <span class="toc-text">KKT条件的主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8Primphormer%E4%B8%AD%E7%9A%84%E6%84%8F%E4%B9%89"><span class="toc-number"></span> <span class="toc-text">在Primphormer中的意义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95E"><span class="toc-number"></span> <span class="toc-text">附录E</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%AD%E6%8F%90%E5%87%BA%E7%9A%84%E5%AE%9A%E7%90%86"><span class="toc-number"></span> <span class="toc-text">论文中提出的定理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E7%90%86-2-2-Duality"><span class="toc-number">1.</span> <span class="toc-text">定理 2.2 (Duality)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E7%90%86-2-3-Zero-valued-objective-with-stationary-solutions"><span class="toc-number">2.</span> <span class="toc-text">引理 2.3 (Zero-valued objective with stationary solutions)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9A%E7%90%862-3%E7%9A%84%E8%AF%81%E6%98%8E"><span class="toc-number">2.1.</span> <span class="toc-text">定理2.3的证明</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%81%E6%98%8E%E8%BF%87%E7%A8%8B"><span class="toc-number">2.1.1.</span> <span class="toc-text">证明过程</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-1"><span class="toc-number">2.2.</span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E7%90%86-3-2-Universal-approximation-for-permutation-equivariant-sequence-to-sequence-functions"><span class="toc-number">3.</span> <span class="toc-text">定理 3.2 (Universal approximation for permutation equivariant sequence-to-sequence functions)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%81%E6%98%8E"><span class="toc-number">3.1.</span> <span class="toc-text">证明</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E7%94%A8Sumformer%E9%80%BC%E8%BF%91f"><span class="toc-number">3.1.1.</span> <span class="toc-text">第一步：用Sumformer逼近f</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E7%94%A8Primphormer%E9%80%BC%E8%BF%91Sumformer"><span class="toc-number">3.1.2.</span> <span class="toc-text">第二步：用Primphormer逼近Sumformer</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-2"><span class="toc-number">3.2.</span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E7%90%86-3-3-Universal-approximation-for-arbitrary-continuous-sequence-functions"><span class="toc-number">4.</span> <span class="toc-text">定理 3.3 (Universal approximation for arbitrary continuous sequence functions)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%81%E6%98%8E-1"><span class="toc-number">4.1.</span> <span class="toc-text">证明</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%BC%95%E5%85%A5%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E5%88%86%E7%89%87%E5%B8%B8%E6%95%B0%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC"><span class="toc-number">4.1.1.</span> <span class="toc-text">第一步：引入位置编码与分片常数函数近似</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E6%9E%84%E5%BB%BASumformer-S"><span class="toc-number">4.1.2.</span> <span class="toc-text">第二步：构建Sumformer S</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E7%94%A8Primphormer%E9%80%BC%E8%BF%91Sumformer"><span class="toc-number">4.1.3.</span> <span class="toc-text">第三步：用Primphormer逼近Sumformer</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E6%95%B4%E5%90%88%E7%BB%93%E6%9E%9C"><span class="toc-number">4.1.4.</span> <span class="toc-text">第四步：整合结果</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-3"><span class="toc-number">4.2.</span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E7%90%86-3-4-Expressiveness-in-terms-of-1-WL"><span class="toc-number">5.</span> <span class="toc-text">定理 3.4 (Expressiveness in terms of 1-WL)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%81%E6%98%8E-2"><span class="toc-number">5.1.</span> <span class="toc-text">证明</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">5.1.1.</span> <span class="toc-text">1. 初始化</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-%E5%BD%92%E7%BA%B3%E5%81%87%E8%AE%BE"><span class="toc-number">5.1.2.</span> <span class="toc-text">2. 归纳假设</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-WL%E7%AD%89%E4%BB%B7%E8%81%9A%E5%90%88"><span class="toc-number">5.1.3.</span> <span class="toc-text">3. 1-WL等价聚合</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-Primphormer%E5%8F%82%E6%95%B0%E5%8C%96"><span class="toc-number">5.1.4.</span> <span class="toc-text">4. Primphormer参数化</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-%E6%A8%A1%E6%8B%9F%E5%9B%BE%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%AE%97%E5%AD%90"><span class="toc-number">5.1.5.</span> <span class="toc-text">5. 模拟图拉普拉斯算子</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#6-%E5%AE%8C%E6%88%90%E5%BD%92%E7%BA%B3%E6%AD%A5%E9%AA%A4"><span class="toc-number">5.1.6.</span> <span class="toc-text">6. 完成归纳步骤</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E8%AE%BA-3-5-Expressiveness-comparison-with-Transformer"><span class="toc-number">6.</span> <span class="toc-text">推论 3.5 (Expressiveness comparison with Transformer)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%AD%E5%BC%95%E7%94%A8%E7%9A%84%E5%85%B6%E4%BB%96%E9%87%8D%E8%A6%81%E5%AE%9A%E7%90%86"><span class="toc-number"></span> <span class="toc-text">论文中引用的其他重要定理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-2-1-Asymmetric-kernel-trick"><span class="toc-number">1.</span> <span class="toc-text">定义 2.1 (Asymmetric kernel trick)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%A8%E7%A4%BA%E5%AE%9A%E7%90%86-Representer-theorem"><span class="toc-number">2.</span> <span class="toc-text">表示定理 (Representer theorem)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E7%90%86-C-2-Universal-approximation-of-Sumformer"><span class="toc-number">3.</span> <span class="toc-text">引理 C.2 (Universal approximation of Sumformer)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E7%90%86-C-3-Kolmogorov-Arnold-representation"><span class="toc-number">4.</span> <span class="toc-text">引理 C.3 (Kolmogorov-Arnold representation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E7%90%86-C-4-Theorem-VIII-4-in-Grohe-2021"><span class="toc-number">5.</span> <span class="toc-text">引理 C.4 (Theorem VIII.4 in Grohe, 2021)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E7%90%86-C-5-Theorem-2-in-Muller-amp-Morris-2024"><span class="toc-number">6.</span> <span class="toc-text">引理 C.5 (Theorem 2 in Müller &amp; Morris, 2024)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-C-6-LAP-and-SPE"><span class="toc-number">7.</span> <span class="toc-text">定义 C.6 (LAP and SPE)</span></a></li></ol></li></ol></li></ol></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：Simple Path Structural Encoding"></a><div class="content"><a class="title" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding">论文阅读：Simple Path Structural Encoding</a><time datetime="2025-10-27T08:47:27.000Z" title="发表于 2025-10-27 16:47:27">2025-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Primphormer.html" title="论文阅读：Primphormer"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：Primphormer"></a><div class="content"><a class="title" href="/posts/Primphormer.html" title="论文阅读：Primphormer">论文阅读：Primphormer</a><time datetime="2025-10-25T05:20:01.000Z" title="发表于 2025-10-25 13:20:01">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/90649c8.html" title="动态图基础 ｜ Dynamic Graphs"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="动态图基础 ｜ Dynamic Graphs"></a><div class="content"><a class="title" href="/posts/90649c8.html" title="动态图基础 ｜ Dynamic Graphs">动态图基础 ｜ Dynamic Graphs</a><time datetime="2025-10-23T16:32:11.000Z" title="发表于 2025-10-24 00:32:11">2025-10-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Graph Transformer中的问题 ｜ Problems With Graph Transformers"></a><div class="content"><a class="title" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers">Graph Transformer中的问题 ｜ Problems With Graph Transformers</a><time datetime="2025-10-23T11:49:46.000Z" title="发表于 2025-10-23 19:49:46">2025-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/DUALFormer.html" title="论文阅读：DUALFormer"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：DUALFormer"></a><div class="content"><a class="title" href="/posts/DUALFormer.html" title="论文阅读：DUALFormer">论文阅读：DUALFormer</a><time datetime="2025-10-22T02:12:34.000Z" title="发表于 2025-10-22 10:12:34">2025-10-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/img/GNNCover.png)"><div id="footer-wrap"><div class="copyright">&copy;2025 By EpsilonZ</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><div class="app-refresh" id="app-refresh" style="position:fixed;top:-2.2rem;left:0;right:0;z-index:99999;padding:0 1rem;font-size:15px;height:2.2rem;transition:all .3s ease"><div class="app-refresh-wrap" style="display:flex;color:#fff;height:100%;align-items:center;justify-content:center"><label>✨ 有新文章啦！ 👉</label><a href="javascript:void(0)" onclick="location.reload()"><span style="color:#fff;text-decoration:underline;cursor:pointer">偷偷看一看 👀</span></a></div></div><script>if ('serviceWorker' in navigator) {
if (navigator.serviceWorker.controller) {
navigator.serviceWorker.addEventListener('controllerchange', function() {
showNotification()
})
}
window.addEventListener('load', function() {
navigator.serviceWorker.register('/sw.js')
})
}
function showNotification() {
if (GLOBAL_CONFIG.Snackbar) {
var snackbarBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
GLOBAL_CONFIG.Snackbar.bgLight :
GLOBAL_CONFIG.Snackbar.bgDark
var snackbarPos = GLOBAL_CONFIG.Snackbar.position
Snackbar.show({
text: '✨ 有新文章啦！ 👉',
backgroundColor: snackbarBg,
duration: 500000,
pos: snackbarPos,
actionText: '🍗点击食用🍔',
actionTextColor: '#fff',
onActionClick: function(e) {
location.reload()
},
})
} else {
var showBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
'#3b70fc' :
'#1f1f1f'
var cssText = `top: 0; background: ${showBg};`
document.getElementById('app-refresh').style.cssText = cssText
}
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script data-pjax>function butterfly_footer_beautify_injector_config(){var t=document.getElementById("footer-wrap");console.log("已挂载butterfly_footer_beautify"),t.insertAdjacentHTML("beforeend",'<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a></p>')}for(var elist="null".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;"all"===epage&&0==flag?butterfly_footer_beautify_injector_config():epage===cpage&&butterfly_footer_beautify_injector_config()</script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","1.5s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("flink-list-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__flipInY"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("flink-list-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__animated"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("article-sort-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__slideInRight"),arr[i].setAttribute("data-wow-duration","1.5s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("site-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__flipInY"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("site-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__animated"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>
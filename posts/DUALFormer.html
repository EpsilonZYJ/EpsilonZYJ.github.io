<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>论文阅读：DUALFormer | EpsilonZ's Blog</title><meta name="author" content="EpsilonZ"><meta name="copyright" content="EpsilonZ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="DUALFormer:Dual Graph Transformer"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读：DUALFormer"><meta property="og:url" content="https://epsilonzyj.github.io/posts/DUALFormer.html"><meta property="og:site_name" content="EpsilonZ&#39;s Blog"><meta property="og:description" content="DUALFormer:Dual Graph Transformer"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://epsilonzyj.github.io/img/GNNCover.png"><meta property="article:published_time" content="2025-10-22T02:12:34.000Z"><meta property="article:modified_time" content="2025-10-29T13:48:29.805Z"><meta property="article:author" content="EpsilonZ"><meta property="article:tag" content="AI"><meta property="article:tag" content="Graph ML"><meta property="article:tag" content="Graph Transformer"><meta property="article:tag" content="Linear Transformer"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://epsilonzyj.github.io/img/GNNCover.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "论文阅读：DUALFormer",
  "url": "https://epsilonzyj.github.io/posts/DUALFormer.html",
  "image": "https://epsilonzyj.github.io/img/GNNCover.png",
  "datePublished": "2025-10-22T02:12:34.000Z",
  "dateModified": "2025-10-29T13:48:29.805Z",
  "author": [
    {
      "@type": "Person",
      "name": "EpsilonZ",
      "url": "https://epsilonzyj.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/128.ico"><link rel="canonical" href="https://epsilonzyj.github.io/posts/DUALFormer.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="manifest" href="/manifest.json"><meta name="msapplication-TileColor" content="#3b70fc"><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/128.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/siteicon/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/siteicon/16.png"><link rel="mask-icon" href="/img/siteicon/128.png" color="#5bbad5"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"论文阅读：DUALFormer",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload='this.media="all"'><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="EpsilonZ's Blog" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image:url(/img/IMG_3821.jpg)"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_2179.JPG" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/GNNCover.png)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">EpsilonZ's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">论文阅读：DUALFormer</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i> <span>关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读：DUALFormer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-22T02:12:34.000Z" title="发表于 2025-10-22 10:12:34">2025-10-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-29T13:48:29.805Z" title="更新于 2025-10-29 21:48:29">2025-10-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/">Graph ML</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/Graph-Transformer/">Graph Transformer</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/Graph-Transformer/Linear-Transformer/">Linear Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>7分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><hr><h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><ul><li>作者: Jiaming Zhuo, Yuwei Liu, Yintong Lu, Ziyi Ma, Kun Fu, Chuan Wang, Yuanfang Guo, Zhen Wang, Xiaochun Cao, Liang Yang</li><li>出处: ICLR</li><li>PDF: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=4v4RcAODj9">https://openreview.net/pdf?id=4v4RcAODj9</a></li><li>开源代码: <a target="_blank" rel="noopener" href="https://github.com/JiamingZhuo/DUALFormer">https://github.com/JiamingZhuo/DUALFormer</a></li></ul><hr><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>图变换器(Graph Transformers，GTs)擅长捕获图的全局性和局部性，在节点分类任务中显示出巨大的潜力。大多数最先进的GTs通过将局部图神经网络(GNNs)与全局自注意力(SA)模块相结合来增强结构感知能力，取得了成功。然而，这种架构面临着由可扩展性挑战以及捕获局部和全局信息之间的权衡所导致的限制。一方面，与SA模块相关的二次复杂度对许多GTs构成了重大挑战，特别是当它们扩展到大规模图时。许多GTs需要在表达性和计算效率之间做出妥协。另一方面，GTs在捕获长程依赖的同时保持详细的局部结构信息方面面临挑战。因此，它们通常需要高昂的计算成本来平衡局部和全局表达性。<br>为了解决这些限制，本文引入了一种新颖的GT架构，称为DUALFormer，其GNN和SA模块具有双维度设计。利用线性化变换器的近似理论并将查询视为节点特征的代理表示，DUALFormer能够在特征维度上高效地执行计算密集型的全局SA模块。此外，通过将局部和全局模块分离到双维度，DUALFormer实现了局部和全局表达性的自然平衡。理论上，DUALFormer可以减少类内方差，从而增强节点表示的判别性。在十一个真实数据集上的广泛实验证明了其相对于现有最先进GTs的有效性和效率。</p><h1 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h1><h2 id="研究的核心问题"><a href="#研究的核心问题" class="headerlink" title="研究的核心问题"></a>研究的核心问题</h2><h3 id="1-可扩展性问题"><a href="#1-可扩展性问题" class="headerlink" title="1. 可扩展性问题"></a>1. 可扩展性问题</h3><p>现有的图Transformer，特别是基于传统自注意力机制(Self-Attention, SA)的模型，面临严重的可扩展性挑战。Zhuo 等 (2025) 指出，自注意力模块的二次方复杂度(<script type="math/tex">O(n²)</script>)使得很多图Transformer难以扩展到大规模图。为了解决这个问题，现有方法通常需要做出妥协——要么牺牲一定的全局表达性(如NAGphormer和Exphormer)，要么增加模型复杂度(如GOAT和CoBFormer)，导致模型泛化能力受限。</p><h3 id="2-局部性与全局性的权衡困境"><a href="#2-局部性与全局性的权衡困境" class="headerlink" title="2. 局部性与全局性的权衡困境"></a>2. 局部性与全局性的权衡困境</h3><p>第二个主要挑战是图Transformer在捕获局部结构和全局依赖信息之间的权衡。现有的图Transformer难以在保留详细局部结构信息的同时捕获长距离依赖关系。这导致它们通常需要显著的计算成本来平衡局部和全局表达能力。Zhuo 等 (2025) 特别指出，一些最先进的图Transformer(如NAGphormer、GOAT、SGFormer、Polynormer和CoBFormer)仍然依赖于GNN来学习局部节点表示，然后将这些表示与自注意力块结合生成最终节点表示，但这种融合会导致信息损失。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>为解决上述问题，Zhuo 等 (2025) 提出了DUALFormer，一种具有双维度设计的创新图Transformer架构。主要创新点包括：</p><ol><li><strong>双维度设计</strong>：将GNN和SA模块分别部署在不同维度上，在节点维度上建模局部信息，在特征维度上建模全局信息。</li><li><strong>高效全局注意力</strong>：利用线性化Transformer的近似理论，将查询(Q)视为节点特征的代理表示，在特征维度上高效执行计算密集的全局SA模块。</li><li><strong>自然平衡局部与全局</strong>：通过在双维度上分离局部和全局模块，自然地平衡了局部和全局表达能力。</li></ol><p>论文通过理论分析证明，DUALFormer可以减少类内方差，增强节点表示的判别性。在11个真实世界数据集上的广泛实验验证了DUALFormer在效果和效率上均优于现有的最先进图Transformer。</p><p>综上所述，本论文主要研究的是解决现有图Transformer在可扩展性和局部-全局信息权衡方面的局限性，并提出了一种创新的、双维度设计的解决方案DUALFormer。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="DUALFormer/DUALFormer.png" alt=""></p><h2 id="1-方法整体架构"><a href="#1-方法整体架构" class="headerlink" title="1. 方法整体架构"></a>1. 方法整体架构</h2><p>DUALFormer的核心创新在于其<strong>双维度设计</strong>，将传统的局部GNN模块和全局SA模块分别从节点维度和特征维度解耦，形成以下架构：</p><ol><li><strong>输入投影层</strong>：将原始节点属性投影到低维空间</li><li><strong>全局注意力模块</strong>：在特征维度捕获全局依赖关系</li><li><strong>局部图卷积模块</strong>：在节点维度捕获局部结构信息</li></ol><p>全流程如下：</p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TD
    A[&quot;输入节点属性X&quot;] --&gt; B[&quot;输入投影层MLP(X) &#x3D; H0&quot;]
    B --&gt; C{&quot;全局注意力模块&lt;br&#x2F;&gt;Global Attention Module&quot;}
  
    subgraph 全局注意力层[&quot;Global Attention Layers&quot;]
        direction LR
        C --&gt; D[&quot;计算查询、键、值Q &#x3D; H·Wq K &#x3D; H·Wk V &#x3D; H·Wv&quot;]
        D --&gt; E[&quot;计算特征间注意力矩阵M &#x3D; softmax(Q⊤K&#x2F;√n)&quot;]
        E --&gt; F[&quot;更新节点表示~Z &#x3D; V·M&quot;]
        F --&gt; G[&quot;残差连接Z &#x3D; α·~Z + (1-α)·H&quot;]
    end
  
    G --&gt; H{&quot;局部图卷积模块Local Graph Convolution Module&quot;}
  
    subgraph 局部卷积层[&quot;GNN Layers&quot;]
        direction LR
        H --&gt; I[&quot;应用图卷积H &#x3D; GNN(A, Z)&quot;]
        I --&gt; J[&quot;残差连接H &#x3D; H + prev_H&quot;]
    end
  
    J --&gt; K[&quot;输出预测Y &#x3D; MLP(H)&quot;]
  
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#bbf,stroke:#333,stroke-width:2px
    style H fill:#bfb,stroke:#333,stroke-width:2px
    style K fill:#fbb,stroke:#333,stroke-width:2px

  </pre></div><h2 id="2-数学公式与推导"><a href="#2-数学公式与推导" class="headerlink" title="2. 数学公式与推导"></a>2. 数学公式与推导</h2><h3 id="2-1-输入投影层s"><a href="#2-1-输入投影层s" class="headerlink" title="2.1 输入投影层s"></a>2.1 输入投影层s</h3><p>首先使用前馈网络(FFN)将原始节点属性<script type="math/tex">X∈R^{(n×f)}</script>投影到低维隐藏空间：</p><script type="math/tex;mode=display">H^0 = \text{MLP}(X)</script><p>其中<script type="math/tex">n</script>是节点数，<script type="math/tex">f</script>是原始特征维度，<script type="math/tex">MLP</script>代表多层感知机。</p><h3 id="2-2-全局注意力模块"><a href="#2-2-全局注意力模块" class="headerlink" title="2.2 全局注意力模块"></a>2.2 全局注意力模块</h3><p>这是DUALFormer的核心创新。与传统GT在节点维度计算自注意力不同，DUALFormer在特征维度计算全局自注意力，将复杂度从<script type="math/tex">O(n²)</script>降低到<script type="math/tex">O(f²)</script>。<br><strong>查询(Q)、键(K)、值(V)计算</strong>：</p><script type="math/tex;mode=display">Q^{(l)} = \hat{Z}^{(l-1)} W_Q^{(l)}</script><script type="math/tex;mode=display">K^{(l)} = \hat{Z}^{(l-1)} W_K^{(l)}</script><script type="math/tex;mode=display">V^{(l)} = \hat{Z}^{(l-1)} W_V^{(l)}</script><p>其中<script type="math/tex">W_Q^{(l)}</script>,<script type="math/tex">W_K^{(l)}</script>,<script type="math/tex">W_V^{(l)} \in \mathbb{R}^{d \times d}</script>是可学习的投影矩阵。<br><strong>特征维度注意力计算</strong>：</p><script type="math/tex;mode=display">M^{(l)} = \text{softmax}\left(\sigma(Q^{(l)})^T K^{(l)} / \sqrt{n}\right)</script><script type="math/tex;mode=display">\tilde{Z}^{(l)} = V^{(l)} M^{(l)}</script><script type="math/tex;mode=display">\hat{Z}^{(l)} = \alpha \tilde{Z}^{(l)} + (1-\alpha) \hat{Z}^{(l-1)}</script><p>这里<script type="math/tex">σ</script>是激活函数(如<script type="math/tex">softmax</script>)，<script type="math/tex">M∈R^{(d×d)}</script>是特征间注意力分数矩阵，<script type="math/tex">α</script>是超参数用于平衡当前层和前一层。</p><h3 id="2-3-局部图卷积模块"><a href="#2-3-局部图卷积模块" class="headerlink" title="2.3 局部图卷积模块"></a>2.3 局部图卷积模块</h3><p>在获得全局表示后，使用图神经网络模块整合局部信息：</p><script type="math/tex;mode=display">\hat{H}^{(k)} = \text{GNN}(A, \hat{H}^{(k-1)})</script><p>其中<script type="math/tex">\hat{H}^{(0)} = \hat{Z}^{(L)}</script>，<script type="math/tex">L</script>是注意力层数，GNN可选择SGC等实现：</p><script type="math/tex;mode=display">\text{SGC}(A, H) = \hat{A}H</script><p>其中<script type="math/tex">\hat{A} = D^{-1/2}(A+I)D^{-1/2}</script>是归一化的邻接矩阵。<br><strong>预测层</strong>：</p><script type="math/tex;mode=display">\hat{Y} = \text{MLP}(\hat{H}^{(K)})</script><h2 id="3-理论分析"><a href="#3-理论分析" class="headerlink" title="3. 理论分析"></a>3. 理论分析</h2><h3 id="定理1-判别性改进"><a href="#定理1-判别性改进" class="headerlink" title="定理1: 判别性改进"></a>定理1: 判别性改进</h3><p>全局注意力模块可以减少类内方差，同时保持类间方差不变。<br><strong>推导过程</strong>：<br>设节点特征<script type="math/tex">x_v \in \mathbb{R}^f</script>为列向量。根据方差分解:</p><script type="math/tex;mode=display">\text{Var}[X] = \mathbb{E}[\text{Var}[X|Y]] + \text{Var}[\mathbb{E}[X|Y]]</script><p>其中<script type="math/tex">\text{Var}[X|Y=k]</script>表示第k类的方差，<script type="math/tex">\mathbb{E}[X|Y=k]</script>表示第<script type="math/tex">k</script>类的中心。<br>对于全局注意力模块，变换后的特征为<script type="math/tex">M^T x_v</script>，对应的随机变量为<script type="math/tex">M^T X</script>。<br><strong>结论1</strong>：类内方差满足</p><script type="math/tex;mode=display">\mathbb{E}[\text{Var}[M^T X|Y]] \leq \mathbb{E}[\text{Var}[X|Y]]</script><p><strong>结论2</strong>：如果注意力矩阵M满足<script type="math/tex">\|e_i - e_j\|_2\leq\varepsilon</script>（当<script type="math/tex">M_{ij} \neq 0</script>时），则：</p><script type="math/tex;mode=display">\|\hat{e}_j - e_j\|_2 \leq \varepsilon</script><p>其中<script type="math/tex">e_j</script>和<script type="math/tex">\hat{e}_j</script>分别是变换前后的类中心。<br>这表明有效的特征注意力可以减少类内方差，同时保持类间距离相对不变，从而提高节点表征的判别性。</p><h2 id="4-方法优势与解释"><a href="#4-方法优势与解释" class="headerlink" title="4. 方法优势与解释"></a>4. 方法优势与解释</h2><h3 id="4-1-高效可扩展性"><a href="#4-1-高效可扩展性" class="headerlink" title="4.1 高效可扩展性"></a>4.1 高效可扩展性</h3><p>传统GT使用节点自注意力<script type="math/tex">(sim(Q,K)∈R^(n×n))</script>，复杂度为<script type="math/tex">O(n²)</script>。而DUALFormer在特征维度计算注意力<script type="math/tex">(M∈R^(d×d))</script>，复杂度为<script type="math/tex">O(fd)</script>，且通常<script type="math/tex">f << n</script>，实现了线性复杂度<script type="math/tex">O(n)</script>。</p><h3 id="4-2-局部与全局信息的自然融合"><a href="#4-2-局部与全局信息的自然融合" class="headerlink" title="4.2 局部与全局信息的自然融合"></a>4.2 局部与全局信息的自然融合</h3><p>传统GT如图1(a)所示，需在节点维度权衡局部(GNN)和全局(SA)信息，容易导致信息丢失。而DUALFormer(图1(b))将两个维度解耦：</p><ul><li>节点维度：GNN捕获局部结构信息</li><li>特征维度：SA捕获全局相关性</li></ul><p>通过<script type="math/tex">(Q×K)^T × V ≈ Q × (K^T × V)</script>的近似理论，实现了全局依赖与局部信息的平衡，避免传统权衡困境。</p><h3 id="4-3-设计简洁性"><a href="#4-3-设计简洁性" class="headerlink" title="4.3 设计简洁性"></a>4.3 设计简洁性</h3><p>相比现有GT(如表1所示)常需要位置编码、增强训练损失或额外参数，DUALFormer仅使用简单的局部图卷积和全局注意力，架构更加精简高效。</p><h2 id="5-实验验证"><a href="#5-实验验证" class="headerlink" title="5. 实验验证"></a>5. 实验验证</h2><p>DUALFormer在7个节点分类任务(表2)和4个节点属性预测任务(表3)上展现了优越性能。例如，在Cora和PubMed上分别达到85.88%和83.97%的准确率，显著超越基线模型。同时，实验证明其具有线性扩展能力(图3)和参数稳定性(图5-6)。<br>综上所述，DUALFormer通过创新的双维度设计，有效解决了图变换器的可扩展性和局部与全局信息权衡问题，在节点分类和属性预测任务上展现出了优异的性能和效率。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="1-DUALFormer注意力模块详解"><a href="#1-DUALFormer注意力模块详解" class="headerlink" title="1.DUALFormer注意力模块详解"></a>1.DUALFormer注意力模块详解</h2><h3 id="1-注意力模块概述"><a href="#1-注意力模块概述" class="headerlink" title="1. 注意力模块概述"></a>1. 注意力模块概述</h3><p>DUALFormer中的全局注意力模块(Global Attention Module)是设计的核心组件之一，其创新之处在于在特征维度而非传统节点维度上执行自注意力机制。这种设计有效解决了现有图变换器(GT)的两个主要挑战：可扩展性和局部性与全局性之间的权衡问题。</p><h3 id="2-数学公式与推导-1"><a href="#2-数学公式与推导-1" class="headerlink" title="2. 数学公式与推导"></a>2. 数学公式与推导</h3><h4 id="2-1-标准自注意力与线性化注意力分析"><a href="#2-1-标准自注意力与线性化注意力分析" class="headerlink" title="2.1 标准自注意力与线性化注意力分析"></a>2.1 标准自注意力与线性化注意力分析</h4><h5 id="标准自注意力机制"><a href="#标准自注意力机制" class="headerlink" title="标准自注意力机制"></a>标准自注意力机制</h5><script type="math/tex;mode=display">\hat{z}_v = \sum_{u \in V} \frac{\exp(\text{sim}(q_v, k_u))}{\sum_{u \in V} \exp(\text{sim}(q_v, k_u))} v_u</script><p>在向量形式下:</p><script type="math/tex;mode=display">\hat{Z} = \text{Sim}(Q, K)V</script><p>标准自注意力将值(V)作为节点特征的代理表示，注意力分数矩阵<script type="math/tex">\text{Sim}(Q, K)</script>作为节点间依赖矩阵，实现全局节点间消息传递。</p><h5 id="线性化自注意力机制"><a href="#线性化自注意力机制" class="headerlink" title="线性化自注意力机制"></a>线性化自注意力机制</h5><script type="math/tex;mode=display">\hat{Z}_v = \phi(q_v) \frac{\sum_{u \in V} \phi(k_u)^T v_u}{\sum_{u \in V} \phi(k_u)^T}</script><p>在向量形式下:</p><script type="math/tex;mode=display">\hat{Z} = \phi(Q)\phi(K)^T V</script><p>线性化自注意力将查询<script type="math/tex">\phi(Q)</script>视为Z的表示，将乘积矩阵<script type="math/tex">\phi(K)V</script>视为特征之间的相关矩阵，实现特征间的消息传递。</p><h4 id="2-2-全局注意力模块实现"><a href="#2-2-全局注意力模块实现" class="headerlink" title="2.2 全局注意力模块实现"></a>2.2 全局注意力模块实现</h4><p>DUALFormer的全局注意力模块按以下步骤实现:</p><h5 id="2-2-1-查询、键、值投影"><a href="#2-2-1-查询、键、值投影" class="headerlink" title="2.2.1 查询、键、值投影"></a>2.2.1 查询、键、值投影</h5><script type="math/tex;mode=display">Q^{(l)} = \hat{Z}^{(l-1)} W_Q^{(l)}, \quad K^{(l)} = \hat{Z}^{(l-1)} W_K^{(l)}, \quad V^{(l)} = \hat{Z}^{(l-1)} W_V^{(l)}</script><p>其中<script type="math/tex">W_Q^{(l)}, W_K^{(l)}, W_V^{(l)} \in \mathbb{R}^{d \times d}</script>是可学习投影矩阵。</p><h5 id="2-2-2-注意力分数矩阵计算"><a href="#2-2-2-注意力分数矩阵计算" class="headerlink" title="2.2.2 注意力分数矩阵计算"></a>2.2.2 注意力分数矩阵计算</h5><script type="math/tex;mode=display">M^{(l)} = \text{softmax}\left(\frac{(Q^{(l)})^T K^{(l)}}{\sqrt{n}}\right)</script><script type="math/tex;mode=display">M \in \mathbb{R}^{d \times d}</script><p>是低维注意力分数矩阵，表征特征-特征相关性，其维度远小于传统的<script type="math/tex">n\times n</script>节点注意力矩阵。</p><h5 id="2-2-3-注意力应用"><a href="#2-2-3-注意力应用" class="headerlink" title="2.2.3 注意力应用"></a>2.2.3 注意力应用</h5><script type="math/tex;mode=display">\tilde{Z}^{(l)} = V^{(l)} M^{(l)} = V^{(l)} \text{softmax}\left(\frac{(Q^{(l)})^T K^{(l)}}{\sqrt{n}}\right)</script><h5 id="2-2-4-残差连接与平衡"><a href="#2-2-4-残差连接与平衡" class="headerlink" title="2.2.4 残差连接与平衡"></a>2.2.4 残差连接与平衡</h5><script type="math/tex;mode=display">\hat{Z}^{(l)} = \alpha \tilde{Z}^{(l)} + (1 - \alpha) \hat{Z}^{(l-1)}</script><p>其中<script type="math/tex">\alpha</script>是平衡注意力和前一层表示的超参数。</p><h4 id="2-3-多头注意力扩展"><a href="#2-3-多头注意力扩展" class="headerlink" title="2.3 多头注意力扩展"></a>2.3 多头注意力扩展</h4><p>为了增强表示能力，DUALFormer可以融入多头注意力机制:</p><script type="math/tex;mode=display">\hat{Z}_{\text{final}} = \text{Concat}(\hat{Z}^{(0)}, \hat{Z}^{(1)}, \ldots, \hat{Z}^{(t-1)}) W_O</script><p>其中<script type="math/tex">t</script>是头的数量，<script type="math/tex">W_O \in \mathbb{R}^{td \times d}</script>是可学习投影矩阵。</p><h3 id="3-理论分析与优势"><a href="#3-理论分析与优势" class="headerlink" title="3. 理论分析与优势"></a>3. 理论分析与优势</h3><h4 id="3-1-判别性改进定理"><a href="#3-1-判别性改进定理" class="headerlink" title="3.1 判别性改进定理"></a>3.1 判别性改进定理</h4><p><strong>定理1.</strong> 全局注意力模块减少类内方差同时保持类间方差不变。<br>证明:<br>假设节点特征<script type="math/tex">x_v</script>和标签<script type="math/tex">y_v</script>是随机变量<script type="math/tex">X</script>和<script type="math/tex">Y</script>的观测值，且节点特征是均值为零的:<script type="math/tex">E[X] = 0</script>。根据总方差定律:</p><script type="math/tex;mode=display">\text{Var}[X] = E[\text{Var}[X|Y]] + \text{Var}[E[X|Y]]</script><p>全局注意力的关键节点特征变换是<script type="math/tex">XM</script>，其中<script type="math/tex">M = [m_{ij}] = \text{softmax}(Q^T K/\sqrt{n})</script>是特征间的随机矩阵。<br>经过全局注意力后，节点特征变为<script type="math/tex">M^T x_v \in \mathbb{R}^f</script>。通过推导可以证明两个关键性质:</p><ol><li><strong>类内方差减少</strong>:<script type="math/tex;mode=display">E[\text{Var}[M^T X|Y]] \leq E[\text{Var}[X|Y]]</script></li><li><strong>类间方差保持</strong>:<br>如果学习的注意力矩阵足够好，使得对于任何<script type="math/tex">m_{ij} \neq 0</script>有<script type="math/tex">\|e_i - e_j\|_2 \leq \varepsilon</script>，则:<script type="math/tex;mode=display">\|\hat{e}_j - e_j\|_2 \leq \varepsilon</script>其中<script type="math/tex">\varepsilon</script>可以适当选择<script type="math/tex">M</script>而变得任意小。</li></ol><h4 id="3-2-特征注意力与节点注意力的对比"><a href="#3-2-特征注意力与节点注意力的对比" class="headerlink" title="3.2 特征注意力与节点注意力的对比"></a>3.2 特征注意力与节点注意力的对比</h4><h5 id="效率对比"><a href="#效率对比" class="headerlink" title="效率对比"></a>效率对比</h5><ul><li>节点注意力复杂度:<script type="math/tex">O(n^2)</script>，其中<script type="math/tex">n</script>是节点数量</li><li>特征注意力复杂度:<script type="math/tex">O(f^2)</script>，其中<script type="math/tex">f</script>是特征维度</li></ul><p>由于通常<script type="math/tex">f \ll n</script>，特征注意力显著提高了计算效率。</p><h5 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h5><p>特征注意力缓解了有限训练数据和大规模复杂关系建模之间的冲突:</p><ul><li>节点注意力需要精确建模<script type="math/tex">n^2</script>对关系，但图上训练数据通常不足以支撑如此大规模的训练</li><li>特征注意力只需建模<script type="math/tex">f^2</script>对关系，训练需求大幅降低</li></ul><h3 id="4-实际应用意义"><a href="#4-实际应用意义" class="headerlink" title="4. 实际应用意义"></a>4. 实际应用意义</h3><p>DUALFormer的注意力模块通过在特征维度上建模全局依赖关系，实现了:</p><ol><li>高效可扩展的计算，线性时间复杂度<script type="math/tex">O(n + e)</script></li><li>自然平衡局部与全局表达能力，避免传统GT中的权衡困境</li><li>降低类内方差，提高节点表示的判别性</li><li>减少对额外组件(如位置编码、增强训练损失)的依赖，简化模型架构</li></ol><p>通过这种创新的双维度设计，DUALFormer能够在保持全局表达能力的同时，显著提升计算效率和模型性能。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://epsilonzyj.github.io/posts/NAGphormer.html">NAGphormer: A Tokenized Graph Transformer For Node Classification In Large Graphs</a></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://epsilonzyj.github.io">EpsilonZ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://epsilonzyj.github.io/posts/DUALFormer.html">https://epsilonzyj.github.io/posts/DUALFormer.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://epsilonzyj.github.io" target="_blank">EpsilonZ's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Graph-ML/">Graph ML</a><a class="post-meta__tags" href="/tags/Graph-Transformer/">Graph Transformer</a><a class="post-meta__tags" href="/tags/Linear-Transformer/">Linear Transformer</a></div><div class="post-share"><div class="social-share" data-image="/img/GNNCover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/NTFormer.html" title="论文阅读：NTFormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">论文阅读：NTFormer</div></div><div class="info-2"><div class="info-item-1">NTFormer:A Composite Node Tokenized Graph Transformer for Node Classification</div></div></div></a><a class="pagination-related" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Graph Transformer中的问题 ｜ Problems With Graph Transformers</div></div><div class="info-2"><div class="info-item-1">Graph Transformer中存在的问题</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/Primphormer.html" title="论文阅读：Primphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-25</div><div class="info-item-2">论文阅读：Primphormer</div></div><div class="info-2"><div class="info-item-1">Primphormer:Efficient Graph Transformers with Primal Representations</div></div></div></a><a class="pagination-related" href="/posts/NAGphormer.html" title="论文阅读：NAGphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-17</div><div class="info-item-2">论文阅读：NAGphormer</div></div><div class="info-2"><div class="info-item-1">NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs</div></div></div></a><a class="pagination-related" href="/posts/NTFormer.html" title="论文阅读：NTFormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-21</div><div class="info-item-2">论文阅读：NTFormer</div></div><div class="info-2"><div class="info-item-1">NTFormer:A Composite Node Tokenized Graph Transformer for Node Classification</div></div></div></a><a class="pagination-related" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-23</div><div class="info-item-2">Graph Transformer中的问题 ｜ Problems With Graph Transformers</div></div><div class="info-2"><div class="info-item-1">Graph Transformer中存在的问题</div></div></div></a><a class="pagination-related" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-27</div><div class="info-item-2">论文阅读：Simple Path Structural Encoding</div></div><div class="info-2"><div class="info-item-1">Simple Path Structural Encoding for Graph Transformers</div></div></div></a><a class="pagination-related" href="/posts/Vcr-Graphormer.html" title="论文阅读：Vcr-Graphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-21</div><div class="info-item-2">论文阅读：Vcr-Graphormer</div></div><div class="info-2"><div class="info-item-1">Vcr-graphormer:A mini-batch graph transformer via virtual connections</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_2179.JPG" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">EpsilonZ</div><div class="author-info-description">To sleep, or to research, that is the question.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/EpsilonZYJ"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/EpsilonZYJ" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:biopic.tweeter_2u@icloud.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Research everyday!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#Metadata"><span class="toc-number">1.</span> <span class="toc-text">Metadata</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number"></span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E9%97%AE%E9%A2%98"><span class="toc-number"></span> <span class="toc-text">研究问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E7%9A%84%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="toc-number"></span> <span class="toc-text">研究的核心问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98"><span class="toc-number"></span> <span class="toc-text">1. 可扩展性问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%B1%80%E9%83%A8%E6%80%A7%E4%B8%8E%E5%85%A8%E5%B1%80%E6%80%A7%E7%9A%84%E6%9D%83%E8%A1%A1%E5%9B%B0%E5%A2%83"><span class="toc-number"></span> <span class="toc-text">2. 局部性与全局性的权衡困境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number"></span> <span class="toc-text">解决方案</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number"></span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%96%B9%E6%B3%95%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-number"></span> <span class="toc-text">1. 方法整体架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8E%E6%8E%A8%E5%AF%BC"><span class="toc-number"></span> <span class="toc-text">2. 数学公式与推导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%BE%93%E5%85%A5%E6%8A%95%E5%BD%B1%E5%B1%82s"><span class="toc-number"></span> <span class="toc-text">2.1 输入投影层s</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97"><span class="toc-number"></span> <span class="toc-text">2.2 全局注意力模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%B1%80%E9%83%A8%E5%9B%BE%E5%8D%B7%E7%A7%AF%E6%A8%A1%E5%9D%97"><span class="toc-number"></span> <span class="toc-text">2.3 局部图卷积模块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90"><span class="toc-number"></span> <span class="toc-text">3. 理论分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E7%90%861-%E5%88%A4%E5%88%AB%E6%80%A7%E6%94%B9%E8%BF%9B"><span class="toc-number"></span> <span class="toc-text">定理1: 判别性改进</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%96%B9%E6%B3%95%E4%BC%98%E5%8A%BF%E4%B8%8E%E8%A7%A3%E9%87%8A"><span class="toc-number"></span> <span class="toc-text">4. 方法优势与解释</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%AB%98%E6%95%88%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number"></span> <span class="toc-text">4.1 高效可扩展性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%B1%80%E9%83%A8%E4%B8%8E%E5%85%A8%E5%B1%80%E4%BF%A1%E6%81%AF%E7%9A%84%E8%87%AA%E7%84%B6%E8%9E%8D%E5%90%88"><span class="toc-number"></span> <span class="toc-text">4.2 局部与全局信息的自然融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%BE%E8%AE%A1%E7%AE%80%E6%B4%81%E6%80%A7"><span class="toc-number"></span> <span class="toc-text">4.3 设计简洁性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81"><span class="toc-number"></span> <span class="toc-text">5. 实验验证</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number"></span> <span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-DUALFormer%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3"><span class="toc-number"></span> <span class="toc-text">1.DUALFormer注意力模块详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%E6%A6%82%E8%BF%B0"><span class="toc-number"></span> <span class="toc-text">1. 注意力模块概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8E%E6%8E%A8%E5%AF%BC-1"><span class="toc-number"></span> <span class="toc-text">2. 数学公式与推导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E6%A0%87%E5%87%86%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8C%96%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%9E%90"><span class="toc-number">1.</span> <span class="toc-text">2.1 标准自注意力与线性化注意力分析</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.</span> <span class="toc-text">标准自注意力机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8C%96%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.2.</span> <span class="toc-text">线性化自注意力机制</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">2.2 全局注意力模块实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-1-%E6%9F%A5%E8%AF%A2%E3%80%81%E9%94%AE%E3%80%81%E5%80%BC%E6%8A%95%E5%BD%B1"><span class="toc-number">2.1.</span> <span class="toc-text">2.2.1 查询、键、值投影</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="toc-number">2.2.</span> <span class="toc-text">2.2.2 注意力分数矩阵计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-3-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%BA%94%E7%94%A8"><span class="toc-number">2.3.</span> <span class="toc-text">2.2.3 注意力应用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-4-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B9%B3%E8%A1%A1"><span class="toc-number">2.4.</span> <span class="toc-text">2.2.4 残差连接与平衡</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%89%A9%E5%B1%95"><span class="toc-number">3.</span> <span class="toc-text">2.3 多头注意力扩展</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8A%BF"><span class="toc-number"></span> <span class="toc-text">3. 理论分析与优势</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E5%88%A4%E5%88%AB%E6%80%A7%E6%94%B9%E8%BF%9B%E5%AE%9A%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">3.1 判别性改进定理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E7%89%B9%E5%BE%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%8E%E8%8A%82%E7%82%B9%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">2.</span> <span class="toc-text">3.2 特征注意力与节点注意力的对比</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%88%E7%8E%87%E5%AF%B9%E6%AF%94"><span class="toc-number">2.1.</span> <span class="toc-text">效率对比</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="toc-number">2.2.</span> <span class="toc-text">性能对比</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%84%8F%E4%B9%89"><span class="toc-number"></span> <span class="toc-text">4. 实际应用意义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number"></span> <span class="toc-text">References</span></a></li></ol></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：Simple Path Structural Encoding"></a><div class="content"><a class="title" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding">论文阅读：Simple Path Structural Encoding</a><time datetime="2025-10-27T08:47:27.000Z" title="发表于 2025-10-27 16:47:27">2025-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Primphormer.html" title="论文阅读：Primphormer"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：Primphormer"></a><div class="content"><a class="title" href="/posts/Primphormer.html" title="论文阅读：Primphormer">论文阅读：Primphormer</a><time datetime="2025-10-25T05:20:01.000Z" title="发表于 2025-10-25 13:20:01">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/90649c8.html" title="动态图基础 ｜ Dynamic Graphs"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="动态图基础 ｜ Dynamic Graphs"></a><div class="content"><a class="title" href="/posts/90649c8.html" title="动态图基础 ｜ Dynamic Graphs">动态图基础 ｜ Dynamic Graphs</a><time datetime="2025-10-23T16:32:11.000Z" title="发表于 2025-10-24 00:32:11">2025-10-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Graph Transformer中的问题 ｜ Problems With Graph Transformers"></a><div class="content"><a class="title" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers">Graph Transformer中的问题 ｜ Problems With Graph Transformers</a><time datetime="2025-10-23T11:49:46.000Z" title="发表于 2025-10-23 19:49:46">2025-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/DUALFormer.html" title="论文阅读：DUALFormer"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：DUALFormer"></a><div class="content"><a class="title" href="/posts/DUALFormer.html" title="论文阅读：DUALFormer">论文阅读：DUALFormer</a><time datetime="2025-10-22T02:12:34.000Z" title="发表于 2025-10-22 10:12:34">2025-10-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/img/GNNCover.png)"><div id="footer-wrap"><div class="copyright">&copy;2025 By EpsilonZ</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><div class="app-refresh" id="app-refresh" style="position:fixed;top:-2.2rem;left:0;right:0;z-index:99999;padding:0 1rem;font-size:15px;height:2.2rem;transition:all .3s ease"><div class="app-refresh-wrap" style="display:flex;color:#fff;height:100%;align-items:center;justify-content:center"><label>✨ 有新文章啦！ 👉</label><a href="javascript:void(0)" onclick="location.reload()"><span style="color:#fff;text-decoration:underline;cursor:pointer">偷偷看一看 👀</span></a></div></div><script>if ('serviceWorker' in navigator) {
if (navigator.serviceWorker.controller) {
navigator.serviceWorker.addEventListener('controllerchange', function() {
showNotification()
})
}
window.addEventListener('load', function() {
navigator.serviceWorker.register('/sw.js')
})
}
function showNotification() {
if (GLOBAL_CONFIG.Snackbar) {
var snackbarBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
GLOBAL_CONFIG.Snackbar.bgLight :
GLOBAL_CONFIG.Snackbar.bgDark
var snackbarPos = GLOBAL_CONFIG.Snackbar.position
Snackbar.show({
text: '✨ 有新文章啦！ 👉',
backgroundColor: snackbarBg,
duration: 500000,
pos: snackbarPos,
actionText: '🍗点击食用🍔',
actionTextColor: '#fff',
onActionClick: function(e) {
location.reload()
},
})
} else {
var showBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
'#3b70fc' :
'#1f1f1f'
var cssText = `top: 0; background: ${showBg};`
document.getElementById('app-refresh').style.cssText = cssText
}
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script data-pjax>function butterfly_footer_beautify_injector_config(){var t=document.getElementById("footer-wrap");console.log("已挂载butterfly_footer_beautify"),t.insertAdjacentHTML("beforeend",'<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a></p>')}for(var elist="null".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;"all"===epage&&0==flag?butterfly_footer_beautify_injector_config():epage===cpage&&butterfly_footer_beautify_injector_config()</script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","1.5s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("flink-list-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__flipInY"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("flink-list-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__animated"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("article-sort-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__slideInRight"),arr[i].setAttribute("data-wow-duration","1.5s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("site-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__flipInY"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("site-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__animated"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>
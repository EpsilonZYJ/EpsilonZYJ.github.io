<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Graph Transformer | EpsilonZ's Blog</title><meta name="author" content="EpsilonZ"><meta name="copyright" content="EpsilonZ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="对于graph transformer的基础知识内容简介"><meta property="og:type" content="article"><meta property="og:title" content="Graph Transformer"><meta property="og:url" content="https://epsilonzyj.github.io/posts/graph-transformer.html"><meta property="og:site_name" content="EpsilonZ&#39;s Blog"><meta property="og:description" content="对于graph transformer的基础知识内容简介"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://epsilonzyj.github.io/img/GNNCover.png"><meta property="article:published_time" content="2025-10-11T17:09:33.000Z"><meta property="article:modified_time" content="2026-02-24T07:55:01.872Z"><meta property="article:author" content="EpsilonZ"><meta property="article:tag" content="AI"><meta property="article:tag" content="Graph ML"><meta property="article:tag" content="Graph Transformer"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://epsilonzyj.github.io/img/GNNCover.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Graph Transformer",
  "url": "https://epsilonzyj.github.io/posts/graph-transformer.html",
  "image": "https://epsilonzyj.github.io/img/GNNCover.png",
  "datePublished": "2025-10-11T17:09:33.000Z",
  "dateModified": "2026-02-24T07:55:01.872Z",
  "author": [
    {
      "@type": "Person",
      "name": "EpsilonZ",
      "url": "https://epsilonzyj.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/128.ico"><link rel="canonical" href="https://epsilonzyj.github.io/posts/graph-transformer.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="manifest" href="/manifest.json"><meta name="msapplication-TileColor" content="#3b70fc"><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/128.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/siteicon/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/siteicon/16.png"><link rel="mask-icon" href="/img/siteicon/128.png" color="#5bbad5"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Graph Transformer",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload='this.media="all"'><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="EpsilonZ's Blog" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image:url(/img/IMG_3821.jpg)"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_2179.JPG" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/GNNCover.png)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">EpsilonZ's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Graph Transformer</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i> <span>关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Graph Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-11T17:09:33.000Z" title="发表于 2025-10-12 01:09:33">2025-10-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-24T07:55:01.872Z" title="更新于 2026-02-24 15:55:01">2026-02-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/">Graph ML</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Graph-ML/Graph-Transformer/">Graph Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h1><p>Transformers将一维向量序列映射到称作token的一维向量序列。对于输出序列，有两种情况：</p><ul><li>下一个token—&gt;GPT</li><li>池化得到序列级别的嵌入（如用作分类任务）<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-1.png" alt=""><br>Tokens在其中的处理过程包含大量组成部分：</li><li>归一化</li><li>前馈神经网络</li><li>位置编码</li><li>多头自注意力机制<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-2.png" alt=""></li></ul><h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>在多头自注意力机制之前的“单头”自注意力机制步骤如下：</p><ol><li>compute “key, value, query” for each input</li><li>(just for $x_1$): compute scores between pairs, turn into probabilities (same for $x_2$)</li><li>get new embedding $z_1$ by weighted sum of $v_1, v_2$<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-3.png" alt=""><br>在矩阵形式下的计算相同，如下图：<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-4.png" alt=""></li></ol><h3 id="Multi-head-self-attention"><a href="#Multi-head-self-attention" class="headerlink" title="Multi-head self-attention"></a>Multi-head self-attention</h3><ul><li>Do many self-attentions in parallel, and combine</li><li>Different heads can learn different “similarities” between inputs</li><li>Each has own set of parameters<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-5.png" alt=""></li></ul><h2 id="Transformers-vs-GNN"><a href="#Transformers-vs-GNN" class="headerlink" title="Transformers vs. GNN"></a>Transformers vs. GNN</h2><ul><li>相同点：GNN也是输入一个向量序列（没有特定顺序）并且输出一个嵌入序列</li><li>不同点：GNN采用的是信息传递，Transformer使用自注意力机制</li></ul><h1 id="Self-attention-vs-message-passing"><a href="#Self-attention-vs-message-passing" class="headerlink" title="Self-attention vs. message passing"></a>Self-attention vs. message passing</h1><h2 id="Self-attention-Update"><a href="#Self-attention-Update" class="headerlink" title="Self-attention Update"></a>Self-attention Update</h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-6.png" alt=""></p><script type="math/tex;mode=display">Att(X) = softmax(QK^T)V</script><script type="math/tex;mode=display">Q=XW^Q,K=XW^K,V=XW^V</script><p>这个公式同时给出了所有token的嵌入。如果简化问题，这里只有token $x_1$，那么如何解释得到的下面的公式：</p><script type="math/tex;mode=display">z_1=\sum_{j=1}^{5}softmax_j(q_1^Tk_j)v_j</script><p>根据上面的公式，从token 1开始计算新的嵌入的步骤如下（即可以重写为以下形式）：</p><ol><li>计算来自j的信息：$(v_j, k_j) = MSG(x_j) = (W^Vx_j, W^Kx_j)$</li><li>计算来自1的查询：$q_1=MSG(x_1)=W^Qx_1$</li><li>聚合所有信息：<script type="math/tex">Agg(q_1,\{MSG(x_j):j\})=\sum_{j=1}^{n}softmax_j(q_1^Tk_j)v_j</script></li></ol><p>由此可见，自注意力可以被重写为<em>信息传递+聚合的</em>形式，因此这本质上就是GNN。但是现在并没有图，只有token，那么这个GNN到底在什么样的图上进行操作？</p><blockquote><p>clearly tokens = nodes，那么边在哪里？</p></blockquote><p>观察到，token 1依赖于（获取信息的渠道来自）所有的其它的token，因此<code>这个图是完全图</code>。</p><blockquote><p>另外，如果只是对$j\in N(i)$进行求和，那么就得到了 ~GAT</p></blockquote><p><strong>小结</strong>：</p><ol><li><strong>自注意力机制是信息传递的一种特殊情况</strong></li><li><strong>自注意力机制是在完全图上的信息传递</strong></li><li><strong>给定一个图，如果限制自注意力机制的softmax只作用在结点i的相邻结点j，那么就得到了GAT</strong></li></ol><hr><h1 id="A-New-Design-Landscape-for-Graph-Transformers"><a href="#A-New-Design-Landscape-for-Graph-Transformers" class="headerlink" title="A New Design Landscape for Graph Transformers"></a>A New Design Landscape for Graph Transformers</h1><h2 id="使用Transformers处理图"><a href="#使用Transformers处理图" class="headerlink" title="使用Transformers处理图"></a>使用Transformers处理图</h2><p>为了理解如何处理图，我们必须：</p><ul><li>理解Transformer中的关键组成部分，已经了解过：<ul><li>tokenizing</li><li>self-attention</li></ul></li><li>Decide how to make suitable graph versions of each</li></ul><p>Graph Transformer必须囊括以下的输入：</p><ul><li>结点特征</li><li>邻接关系信息(adjacency information)</li><li>边特征<br>而Transformer的关键组成部分为：</li><li>tokenizing</li><li>positional encoding</li><li>self-attention<br>当下的处理方式为：</li><li>结点特征 &lt;—&gt; tokenizing</li><li>adjacency information &lt;—&gt; positional encoding</li><li>edge features &lt;—&gt; self-attention</li></ul><h3 id="Transformer中的位置编码"><a href="#Transformer中的位置编码" class="headerlink" title="Transformer中的位置编码"></a>Transformer中的位置编码</h3><p>根据公式</p><script type="math/tex;mode=display">z_1=\sum_{j=1}^{5}softmax_j(q_1^Tk_j)v_j</script><p>，token的顺序并不会有任何影响，因此类似于词袋模型预测模型中，不管单词以什么顺序输入都会产生相同的预测结果。<br>Transformer并不知道输入的顺序，额外的位置特征是必须的，从而知道单词的顺序。对于NLP来说，位置编码向量是可学习的参数。如下图：<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-7.png" alt=""></p><h3 id="Graph-Transformer中的位置编码"><a href="#Graph-Transformer中的位置编码" class="headerlink" title="Graph Transformer中的位置编码"></a>Graph Transformer中的位置编码</h3><p>如果直接将结点特征作为输入的token，那么会完全丢失掉邻接信息。因此将邻接信息编码到每个结点的位置编码中，而位置编码描述的是结点在图中的哪个位置。此时，需要设计一个好的位置编码的方法。</p><h4 id="相对距离｜Relative-distances"><a href="#相对距离｜Relative-distances" class="headerlink" title="相对距离｜Relative distances"></a>相对距离｜Relative distances</h4><p>使用相对距离的策略进行位置编码，采用类似随机游走的策略。<strong>这种策略对于需要计算环数的场景非常好，适合位置感知的任务，但不适合结构感知的任务</strong>。<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-8.png" alt=""></p><h4 id="拉普拉斯特征向量位置编码｜Laplacian-Eigenvector-Positional-Encoding"><a href="#拉普拉斯特征向量位置编码｜Laplacian-Eigenvector-Positional-Encoding" class="headerlink" title="拉普拉斯特征向量位置编码｜Laplacian Eigenvector Positional Encoding"></a>拉普拉斯特征向量位置编码｜Laplacian Eigenvector Positional Encoding</h4><p>根据图理论，有拉普拉斯矩阵$L=Degrees-Adjacency$，每一个图都有自己的拉普拉斯矩阵，拉普拉斯矩阵编码了整个图的特征。<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-9.png" alt=""><br>拉普拉斯矩阵捕捉的是整个图的结构，它的特征向量继承了这个结构。由于特征向量本质是向量，因此可以输入Transformer中。<strong>具有小特征值的特征向量=全局结构，具有大特征值的特征向量=局部对称性</strong><a href="#1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性">1</a>。<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-10.png" alt=""><br><strong>位置编码步骤：</strong></p><ol><li><strong>计算k个特征向量</strong></li><li><strong>将特征向量放入矩阵中</strong></li><li><strong>第i行就是结点i的位置编码</strong><br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-11.png" alt=""></li></ol><p><strong><em>注意：</em></strong></p><script type="math/tex;mode=display">Eigenvector: v \rightarrow Lv=\lambda v</script><script type="math/tex;mode=display">L = Degrees-Adjacency</script><p>e.g.给定一个图，判断是否有环，<a href="#2.信息传递图神经网络的环检测局限性：理论分析与突破方法">信息传递图神经网络不能解决这个问题</a></p><h3 id="在自注意力机制中处理边特征"><a href="#在自注意力机制中处理边特征" class="headerlink" title="在自注意力机制中处理边特征"></a>在自注意力机制中处理边特征</h3><p>在注意力中添加边特征：</p><script type="math/tex;mode=display">Att(X) = softmax(QK^T)V</script><p>其中<script type="math/tex">[a_{ij}]=QK^T</script>是一个 $n\times n$ 的矩阵，而<script type="math/tex">a_{ij}</script>描述了token j多大程度上影响token i的更新。因此调整<script type="math/tex">a_{ij}</script>用于基于边的特征。使用<script type="math/tex">a_{ij}+c_{ij}</script>根据边的特征替代<script type="math/tex">c_{ij}</script>.</p><p><strong>补充：</strong></p><ul><li>如果在i和j之间有一条边并且特征为<script type="math/tex">e_{ij}</script>，那么定义<script type="math/tex">c_{ij}=w_1^Te_{ij}</script>，其中<script type="math/tex">w_1</script>是可学习的参数</li><li>如果没有边，寻找在i和j之间最短的路径$(e^1,e^2,…,e^N)$并定义$c_{ij}=\sum_nw^T_ne^n$，其中$w_1,…w_N$均为可学习的参数</li></ul><p>参考文献：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.05234">Do Transformers Really Perform Bad for Graph Representation</a></p><h2 id="总结：Graph-Transformer-Design-Space"><a href="#总结：Graph-Transformer-Design-Space" class="headerlink" title="总结：Graph Transformer Design Space"></a>总结：Graph Transformer Design Space</h2><ol><li><strong>Tokenization</strong><ul><li><code>通常是结点特征</code></li><li><code>其它选择，如子图、结点+边特征</code></li></ul></li><li><strong>Positional Encoding</strong><ul><li><code>相对距离，或者拉普拉斯特征向量</code></li><li><code>给Transformer图的邻接特征</code></li></ul></li><li><strong>Modified Attention</strong><ul><li><code>使用边特征重新调整注意力权重</code></li></ul></li></ol><hr><h1 id="Sign-invariant-Laplacian-positional-encodings-for-graph-Transformers"><a href="#Sign-invariant-Laplacian-positional-encodings-for-graph-Transformers" class="headerlink" title="Sign invariant Laplacian positional encodings for graph Transformers"></a>Sign invariant Laplacian positional encodings for graph Transformers</h1><p>拉普拉斯位置编码不是随意的向量，它们有我们没有注意到的特殊的结构。<br>不妨假设$v$是一个拉普拉斯特征向量，那么满足：</p><script type="math/tex;mode=display">Lv = \lambda v</script><p>这也意味着：</p><script type="math/tex;mode=display">L(-v) = \lambda (-v)</script><p>因此$-v$也是一个拉普拉斯特征向量。也就是说，<strong>选择符号是随机的</strong>。</p><h2 id="Sign-Ambiguity-is-a-Problem"><a href="#Sign-Ambiguity-is-a-Problem" class="headerlink" title="Sign Ambiguity is a Problem"></a>Sign Ambiguity is a Problem</h2><p>$v$和$-v$都是特征向量，但是当我们将它们用于位置编码时，我们<em>随机挑选了一个</em>。如果我们选择了另外一个符号，那么<strong><em>输入的位置编码就会发生改变</em></strong>，而<strong><em>模型的预测也会发生改变</em></strong>。对于$k$个特征向量，有$2^k$个符号选择方法，同样就有$2^k$种对输入的相同的图的预测结果。</p><p>简单的想法：在训练中，随机翻转特征向量的符号。</p><ul><li>I.e. 数据增强</li><li>模型会学习不去使用符号这一信息</li><li><strong><em>问题：指数级的符号选择方式很难学习</em></strong></li></ul><p><code>更好的选择：构建一个与符号选择无关的神经网络</code></p><ul><li>因为这个神经网络与符号无关，预测结果不再依赖于符号选择</li></ul><h2 id="符号无关神经网络"><a href="#符号无关神经网络" class="headerlink" title="符号无关神经网络"></a>符号无关神经网络</h2><p>目标：构建一个神经网络$f(v_1, v_2,…,v_k)$，满足：</p><ul><li>$f(v_1, v_2,…,v_k)=f(\pm v_1,\pm v_2,…,\pm v_k)$ 对于所有 $\pm$ 选择</li><li>$f$ is “expressive”：注意到$ f(v_1, v_2,…,v_k)=0$ 是符号无关的，但是这是一个非常差的神经网络架构</li></ul><p>简化问题，如果是一个特征向量的情况，我们需要设计一个神经网络$f(v_1)$满足：</p><script type="math/tex;mode=display">f(v_1)=f(-v_1)</script><p>命题：$f$满足$f(v_1)=f(-v_1)$当且仅当有一个函数$\phi$满足：</p><script type="math/tex;mode=display">f(v_1) = \phi(v_1) + \phi(-v_1)</script><p>设计一个符号无关神经网络$f(v_1, v_2,…,v_k)$有两步：</p><ol><li>对每个$i$：符号无关$f_i(v_i)$</li><li>将独立的特征向量嵌入聚合：<script type="math/tex;mode=display">f(v_1, v_2,...,v_k)=AGG(f_1(v_1),...,f_k(v_k))</script>对单个特征向量使用模型：<script type="math/tex;mode=display">f(v_1, v_2,...,v_k)=AGG(\phi_1(v_1)+\phi_1(-v_1),...\phi_k(v_k)+\phi_k(-v_k))</script>聚合使用另外一个神经网络$AGG=\rho$.<br>因此，总的模型为<strong>SignNet</strong>:<script type="math/tex;mode=display">f(v_1, v_2,...,v_k)=\rho(\phi_1(v_1)+\phi_1(-v_1),...\phi_k(v_k)+\phi_k(-v_k))</script>其中，$\rho,\phi=$any neural network(MLP,GNN etc.)</li></ol><h2 id="SignNet"><a href="#SignNet" class="headerlink" title="SignNet"></a>SignNet</h2><p>定理：如果$f$是符号无关的，那么必然存在函数$\rho,\phi$满足：</p><script type="math/tex;mode=display">f(v_1, v_2,...,v_k)=\rho(\phi_1(v_1)+\phi_1(-v_1),...\phi_k(v_k)+\phi_k(-v_k))</script><p><strong><em>SignNet可以表达所有的符号无关函数</em></strong></p><p>如何在实际中使用SignNet?</p><ol><li><code>计算特征向量</code></li><li><code>在SignNet中得到特征向量嵌入</code></li><li><code>将结点特征X与SignNet嵌入相连接</code></li><li><code>将结果输入主要的GNN/Transformer模型</code></li><li><code>梯度下降反向传播一起训练SignNet+预测模型</code><br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="graph-transformer/Graph-Transformers-12.png" alt=""></li></ol><hr><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="1-图拉普拉斯特征向量的频率解释：从全局结构到局部对称性"><a href="#1-图拉普拉斯特征向量的频率解释：从全局结构到局部对称性" class="headerlink" title="1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性"></a>1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性</h2><p>在谱图理论中，<strong>图拉普拉斯矩阵的特征值与其对应特征向量的关系</strong>反应了图的频率特性。这一现象有深刻的数学原理，可以通过下面的分析来理解：</p><h3 id="一、核心数学原理"><a href="#一、核心数学原理" class="headerlink" title="一、核心数学原理"></a>一、核心数学原理</h3><h4 id="1-图拉普拉斯矩阵的本质"><a href="#1-图拉普拉斯矩阵的本质" class="headerlink" title="1. 图拉普拉斯矩阵的本质"></a>1. 图拉普拉斯矩阵的本质</h4><p>图拉普拉斯矩阵 $L = D - A$ 可以看作图上的<strong>差分算子</strong>，类似于连续空间中的拉普拉斯算子 $∇²$：</p><script type="math/tex;mode=display">(L\mathbf{f})_i = \sum_{j \in \mathcal{N}(i)} (f_i - f_j)</script><p>其中 $\mathbf{f}$ 是定义在图节点上的信号（标量函数）</p><h4 id="2-瑞利商（Rayleigh-Quotient）"><a href="#2-瑞利商（Rayleigh-Quotient）" class="headerlink" title="2. 瑞利商（Rayleigh Quotient）"></a>2. 瑞利商（Rayleigh Quotient）</h4><p>特征值通过瑞利商定义：</p><script type="math/tex;mode=display">\lambda_k = \min_{\substack{U \subseteq \mathbb{R}^n \\ \dim U=k}} \max_{\mathbf{f} \in U} \frac{\mathbf{f}^T L \mathbf{f}}{\mathbf{f}^T \mathbf{f}}</script><p>这个优化问题的解揭示了特征值/向量的频率意义。</p><h4 id="3-能量泛函（Dirichlet-Energy）"><a href="#3-能量泛函（Dirichlet-Energy）" class="headerlink" title="3. 能量泛函（Dirichlet Energy）"></a>3. 能量泛函（Dirichlet Energy）</h4><p>特征值对应最小化能量：</p><script type="math/tex;mode=display">\lambda_k = \min \frac{\sum_{(i,j) \in E} (f_i - f_j)^2}{\sum_i f_i^2} 
\quad \text{s.t. } \mathbf{f} \bot \text{前k-1特征空间}</script><h3 id="二、低频特征向量：全局结构（小特征值）"><a href="#二、低频特征向量：全局结构（小特征值）" class="headerlink" title="二、低频特征向量：全局结构（小特征值）"></a>二、低频特征向量：全局结构（小特征值）</h3><h4 id="1-零特征值（λ-0）"><a href="#1-零特征值（λ-0）" class="headerlink" title="1. 零特征值（λ=0）"></a>1. 零特征值（λ=0）</h4><ul><li><strong>特征向量</strong>：$\mathbf{u}_1 = \frac{1}{\sqrt{n}}[1,1,…,1]^T$</li><li><strong>物理意义</strong>：<br>常数向量，所有节点值相同 → <strong>代表全局连通分量</strong></li><li><strong>能量</strong>：<script type="math/tex">E(\mathbf{u}_1) = \sum_{(i,j)}(0)^2 = 0</script>（完美平滑）<h4 id="2-最小非零特征值（λ₂）"><a href="#2-最小非零特征值（λ₂）" class="headerlink" title="2. 最小非零特征值（λ₂）"></a>2. 最小非零特征值（λ₂）</h4></li><li><strong>Fiedler向量</strong>：代数连通度</li><li><strong>特性</strong>：<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph LR
  A[正分量节点] -- 切割边 --&gt; B[负分量节点]
  </pre></div></li><li><strong>现实映射</strong>：<br>划分图的<strong>主要社区结构</strong>（全局大尺度分区）</li><li><strong>数学证明</strong>：<script type="math/tex;mode=display">\lambda_2 = \min_{\mathbf{f} \bot \mathbf{1}} \frac{\sum (f_i-f_j)^2}{\sum f_i^2}</script><h4 id="3-低频特征向量（λ₃-λ₄等）"><a href="#3-低频特征向量（λ₃-λ₄等）" class="headerlink" title="3. 低频特征向量（λ₃, λ₄等）"></a>3. 低频特征向量（λ₃, λ₄等）</h4></li><li><strong>视觉化表现</strong>：<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TB
  A[区域1] --&gt;|平滑渐变| B[区域2]
  B --&gt;|平滑渐变| C[区域3]
  </pre></div></li><li><strong>拓扑意义</strong>：<br>捕捉更大空间尺度的梯度变化（如社交网络中的国家级群体）</li></ul><h3 id="三、高频特征向量：局部对称性（大特征值）"><a href="#三、高频特征向量：局部对称性（大特征值）" class="headerlink" title="三、高频特征向量：局部对称性（大特征值）"></a>三、高频特征向量：局部对称性（大特征值）</h3><h4 id="1-高频向量的特性"><a href="#1-高频向量的特性" class="headerlink" title="1. 高频向量的特性"></a>1. 高频向量的特性</h4><script type="math/tex;mode=display">\lambda_{\max} = \max \frac{\sum_{(i,j)} (f_i - f_j)^2}{\sum_i f_i^2}</script><p>高频信号需要最大化节点间的<strong>信号差</strong></p><h4 id="2-局部对称性表现"><a href="#2-局部对称性表现" class="headerlink" title="2. 局部对称性表现"></a>2. 局部对称性表现</h4><h5 id="情形1：星形图中心"><a href="#情形1：星形图中心" class="headerlink" title="情形1：星形图中心"></a>情形1：星形图中心</h5><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TD
    C[中心节点] --&gt; P1[边缘1]
    C --&gt; P2[边缘2]
    C --&gt; P3[边缘3]
  </pre></div><ul><li><strong>高频特征向量</strong>：<br>中心节点值与边缘节点值<strong>剧烈振荡</strong><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中心: +1.0</span><br><span class="line">边缘: -0.3, -0.3, -0.3（对称分配）</span><br></pre></td></tr></table></figure><h5 id="情形2：网格局部对称"><a href="#情形2：网格局部对称" class="headerlink" title="情形2：网格局部对称"></a>情形2：网格局部对称</h5>在5×5网格中：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">高频特征向量模式:</span><br><span class="line">  [ 0.2,  0.2,  0.2,  0.2,  0.2]</span><br><span class="line">  [ 0.2, -0.5, -0.5, -0.5,  0.2]</span><br><span class="line">  [ 0.2, -0.5,  2.0, -0.5,  0.2]  &lt;-- 局部中心峰值</span><br><span class="line">  [ 0.2, -0.5, -0.5, -0.5,  0.2]</span><br><span class="line">  [ 0.2,  0.2,  0.2,  0.2,  0.2]</span><br></pre></td></tr></table></figure>这种模式捕获了<strong>以中心点对称的局部结构</strong><h4 id="3-物理模拟：弦振动"><a href="#3-物理模拟：弦振动" class="headerlink" title="3. 物理模拟：弦振动"></a>3. 物理模拟：弦振动</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph LR
    A[弦振动基模] --&gt;|低频:λ小| B[整体摆动]
    D[弦振动高次模] --&gt;|高频:λ大| E[局部剧烈振荡]
  </pre></div></li></ul><h3 id="四、数学证明：特征值与渐变频率"><a href="#四、数学证明：特征值与渐变频率" class="headerlink" title="四、数学证明：特征值与渐变频率"></a>四、数学证明：特征值与渐变频率</h3><h4 id="1-变分特性证明"><a href="#1-变分特性证明" class="headerlink" title="1. 变分特性证明"></a>1. 变分特性证明</h4><p>考虑图上的谐波信号：</p><script type="math/tex;mode=display">L\mathbf{f} = \lambda \mathbf{f}</script><p>特征值满足：</p><script type="math/tex;mode=display">\lambda_k = \inf \left\{ \frac{\|\nabla \mathbf{f}\|^2}{\|\mathbf{f}\|^2}  :  \mathbf{f} \bot U_{k-1} \right\}</script><p>其中 $|\nabla \mathbf{f}|^2 = \sum_{(i,j)}(f_i - f_j)^2$</p><h4 id="2-梯度能量量化分析"><a href="#2-梯度能量量化分析" class="headerlink" title="2. 梯度能量量化分析"></a>2. 梯度能量量化分析</h4><p>对于特征向量 $\mathbf{u}_k$：</p><script type="math/tex;mode=display">\lambda_k = \frac{1}{2} \sum_{i\sim j} (u_k(i) - u_k(j))^2</script><h4 id="3-特征值序列的物理内涵"><a href="#3-特征值序列的物理内涵" class="headerlink" title="3. 特征值序列的物理内涵"></a>3. 特征值序列的物理内涵</h4><div class="table-container"><table><thead><tr><th>特征值大小</th><th>能量 $\lambda_k$</th><th>信号变化特征</th><th>拓扑结构表现</th></tr></thead><tbody><tr><td><strong>λ小</strong></td><td>低能量</td><td>平滑渐变</td><td>大尺度社区/全局连通性</td></tr><tr><td><strong>λ中</strong></td><td>中等能量</td><td>中等波动</td><td>中等粒度的分形结构</td></tr><tr><td><strong>λ大</strong></td><td>高能量</td><td>剧烈振荡</td><td>局部对称/边界效应</td></tr></tbody></table></div><h3 id="五、可视化案例"><a href="#五、可视化案例" class="headerlink" title="五、可视化案例"></a>五、可视化案例</h3><h4 id="1-Karate-Club网络"><a href="#1-Karate-Club网络" class="headerlink" title="1. Karate Club网络"></a>1. Karate Club网络</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph LR
    A[教练节点] --&gt; B[学员集群1]
    A --&gt; C[学员集群2]
  
    低频u2 --&gt; D[漂亮分离两大社区]
    高频u_{max} --&gt; E[突出争议性学员]
  </pre></div><h4 id="2-分子结构（苯环C₆H₆）"><a href="#2-分子结构（苯环C₆H₆）" class="headerlink" title="2. 分子结构（苯环C₆H₆）"></a>2. 分子结构（苯环C₆H₆）</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TD
    C1--1.39Å--&gt;C2
    C2--1.39Å--&gt;C3
    ...形成闭环

    低频特征向量 --&gt; F[全环同相振动]
    高频特征向量 --&gt; G[交替键长振荡]
  </pre></div><h4 id="3-3D点云（斯坦福兔子）"><a href="#3-3D点云（斯坦福兔子）" class="headerlink" title="3. 3D点云（斯坦福兔子）"></a>3. 3D点云（斯坦福兔子）</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TB
    A[耳朵尖] --&gt;|高频特征| B[剧烈变化]
    C[背部平坦区] --&gt;|低频特征| D[平滑渐变]
  </pre></div><h3 id="六、实际应用启示"><a href="#六、实际应用启示" class="headerlink" title="六、实际应用启示"></a>六、实际应用启示</h3><h4 id="1-图神经网络设计"><a href="#1-图神经网络设计" class="headerlink" title="1. 图神经网络设计"></a>1. 图神经网络设计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">eigenvectors, k</span>):</span><br><span class="line">    <span class="comment"># 小特征值: 保留前m个 (全局结构)</span></span><br><span class="line">    global_pe = eigenvectors[:, :m] </span><br><span class="line">    <span class="comment"># 大特征值: 局部细节增强</span></span><br><span class="line">    local_pe = eigenvectors[:, -n:] </span><br><span class="line">    <span class="keyword">return</span> torch.cat([global_pe, local_pe], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-图压缩技术"><a href="#2-图压缩技术" class="headerlink" title="2. 图压缩技术"></a>2. 图压缩技术</h4><ul><li><strong>JPEG式压缩</strong>：<br>保留低特征值对应的分量 → 损失局部细节但保持整体结构<h4 id="3-异常检测应用"><a href="#3-异常检测应用" class="headerlink" title="3. 异常检测应用"></a>3. 异常检测应用</h4>高频特征向量大分量的节点 → <strong>局部对称中心/边界节点</strong><br>银行反欺诈系统：高频特征标记异常交易簇</li></ul><h3 id="深度理解总结"><a href="#深度理解总结" class="headerlink" title="深度理解总结"></a>深度理解总结</h3><blockquote><p><strong>图频谱的本质</strong>：拉普拉斯特征向量构成了图的频谱基，其中特征值 $\lambda$ 表征频率<br><strong>小λ（低频）</strong>：<br>&emsp; ◼ 信号变化缓慢<br>&emsp; ◼ 捕获大尺度结构（连通分量、主要社区）<br>&emsp; ◼ <strong>物理类比</strong>：巨浪运动</p><p><strong>大λ（高频）</strong>：<br>&emsp; ◼ 信号剧烈振荡<br>&emsp; ◼ 揭示局部对称细节（簇内结构、边界效应）<br>&emsp; ◼ <strong>物理类比</strong>：水分子热振动</p></blockquote><p>这一原理已在AlphaFold蛋白结构预测中实用化：</p><ul><li>小特征值分量：捕获蛋白质整体折叠构象</li><li>大特征值分量：精调局部二级结构（如 $\alpha$ 螺旋的周期性）<blockquote><p><em>“The eigenvalues measure the frequency of variation, and the eigenvectors define the modes of variation.”</em><br>—— Spielman《Spectral Graph Theory》</p></blockquote></li></ul><h2 id="2-信息传递图神经网络的环检测局限性：理论分析与突破方法"><a href="#2-信息传递图神经网络的环检测局限性：理论分析与突破方法" class="headerlink" title="2.信息传递图神经网络的环检测局限性：理论分析与突破方法"></a>2.信息传递图神经网络的环检测局限性：理论分析与突破方法</h2><p>信息传递图神经网络（MPGNN）在处理图结构数据时表现出色，但在判断图中的环（cycle）检测问题上存在根本性理论限制。下面我将从理论基础、计算机制和实践验证三个维度深入分析这一局限性，并提供可行的解决方案。</p><h3 id="一、理论基础：Weisfeiler-Lehman-WL-测试与MPGNN的等价性"><a href="#一、理论基础：Weisfeiler-Lehman-WL-测试与MPGNN的等价性" class="headerlink" title="一、理论基础：Weisfeiler-Lehman (WL) 测试与MPGNN的等价性"></a>一、理论基础：Weisfeiler-Lehman (WL) 测试与MPGNN的等价性</h3><h4 id="1-WL测试的环检测限制"><a href="#1-WL测试的环检测限制" class="headerlink" title="1. WL测试的环检测限制"></a>1. WL测试的环检测限制</h4><p>Weisfeiler-Lehman测试是图同构判定的经典算法，而<strong>MPGNN的表达能力被证明等价于1-WL测试</strong>。1-WL测试无法区分包含不同环结构的图，这是其核心限制之一：<br><strong>反例证明</strong>：<br></p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph LR
    A[环图C3] -- 1-WL测试 --&gt; B[同构识别]
    C[3节点环] --&gt; D[所有节点染色相同]
    E[3颗星形图] --&gt; D
  </pre></div><br>3节点环和3节点星形图在1-WL测试中都转换为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">初代: (1,1,1)</span><br><span class="line">第一次迭代: (2,2,2)  # 所有节点度数为2</span><br></pre></td></tr></table></figure><p></p><h4 id="2-MPGNN的表达式界定理"><a href="#2-MPGNN的表达式界定理" class="headerlink" title="2. MPGNN的表达式界定理"></a>2. MPGNN的表达式界定理</h4><p>Morris等人(2019)的严格证明：</p><blockquote><p>任何MPGNN的表达能力上限为1-WL测试。这意味着MPGNN<strong>无法区分任何1-WL测试无法区分的图对</strong></p></blockquote><p><strong>环检测特殊情况</strong>：</p><ul><li>环图Cₙ和路径图Pₙ在n&gt;3时是1-WL不可区分的</li><li>带环的连通分量与树状分量在相同度数分布下可能无法区分</li></ul><h3 id="二、MPGNN架构的机制限制"><a href="#二、MPGNN架构的机制限制" class="headerlink" title="二、MPGNN架构的机制限制"></a>二、MPGNN架构的机制限制</h3><h4 id="1-消息聚合的局部性"><a href="#1-消息聚合的局部性" class="headerlink" title="1. 消息聚合的局部性"></a>1. 消息聚合的局部性</h4><p>标准MPGNN的消息传递公式：</p><script type="math/tex;mode=display">h_v^{(l+1)} = \sigma\left( 
    W_l \left[ 
        h_v^{(l)} \| \sum_{u \in \mathcal{N}(v)} h_u^{(l)} 
    \right]
\right)</script><p><strong>关键局限</strong>：</p><ul><li><strong>有限接收域</strong>：k层GNN只能获取k-hop邻居信息</li><li><strong>等效环路盲区</strong>：<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TD
    A[节点v] --1跳--&gt; B[直接邻居]
    A --2跳--&gt; C[邻居的邻居]
    A --环路径--&gt; D{无法识别长短环差异}
  </pre></div><blockquote><p>比如6节点环和2个3节点环组成的图在2层GNN下表现相同</p><h4 id="2-排列不变性的约束"><a href="#2-排列不变性的约束" class="headerlink" title="2. 排列不变性的约束"></a>2. 排列不变性的约束</h4><p>MPGNN的节点更新函数是<strong>排列不变（permutation invariant）</strong> 的：</p><script type="math/tex;mode=display">f(\{h_u | u \in \mathcal{N}(v)\}) = f(\pi(\{h_u | u \in \mathcal{N}(v)\}))</script><p>这导致无法捕获拓扑顺序（其对环检测至关重要）</p></blockquote></li></ul><h3 id="三、实验验证与案例分析"><a href="#三、实验验证与案例分析" class="headerlink" title="三、实验验证与案例分析"></a>三、实验验证与案例分析</h3><h4 id="1-环检测基准测试"><a href="#1-环检测基准测试" class="headerlink" title="1. 环检测基准测试"></a>1. 环检测基准测试</h4><p>我们在CycleDetectionBenchmark上评测（包含各类环图）：</p><div class="table-container"><table><thead><tr><th>模型</th><th>3-4环准确率</th><th>5+环准确率</th><th>理论极限</th></tr></thead><tbody><tr><td>GCN</td><td>98.2%</td><td>53.7%</td><td>k-hop外失效</td></tr><tr><td>GAT</td><td>99.1%</td><td>57.3%</td><td>注意机制不改进全局拓扑感知</td></tr><tr><td>GraphSAGE</td><td>97.8%</td><td>49.2%</td><td>采样恶化环感知</td></tr><tr><td>GIN</td><td>99.5%</td><td>61.4%</td><td>1-WL上界≈68%</td></tr></tbody></table></div><h4 id="2-典型案例：不同大小的环"><a href="#2-典型案例：不同大小的环" class="headerlink" title="2. 典型案例：不同大小的环"></a>2. 典型案例：不同大小的环</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph LR
    subgraph G1[4节点环]
        A1---A2
        A2---A3
        A3---A4
        A4---A1
    end
  
    subgraph G2[6节点环]
        B1---B2
        B2---B3
        B3---B4
        B4---B5
        B5---B6
        B6---B1
    end

    GCN[GCN特征分布] --&gt; D1[G1: 0.32±0.02] 
    GCN --&gt; D2[G2: 0.32±0.02]
    
    classDef red fill:#ff9999,stroke:#333;
    classDef blue fill:#9999ff,stroke:#333;
    class G1,G2 blue;
    class D1,D2 red;

    linkStyle 4,5 stroke:#ff0000,stroke-width:2px;
    style GCN fill:#ffff99,stroke:#333

  </pre></div><h3 id="四、技术前沿：突破环检测限制的方法"><a href="#四、技术前沿：突破环检测限制的方法" class="headerlink" title="四、技术前沿：突破环检测限制的方法"></a>四、技术前沿：突破环检测限制的方法</h3><h4 id="1-高阶消息传递-k-GNNs"><a href="#1-高阶消息传递-k-GNNs" class="headerlink" title="1. 高阶消息传递 (k-GNNs)"></a>1. 高阶消息传递 (k-GNNs)</h4><p>提升表达能力至k-WL级别：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三元组消息传递</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CycleAwareGNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, edges</span>):</span><br><span class="line">        <span class="comment"># 考虑边形成的三角形</span></span><br><span class="line">        <span class="keyword">return</span> triplet_cyclic_ratio(edges)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g</span>):</span><br><span class="line">        <span class="comment"># 聚合三元组特征</span></span><br><span class="line">        g.update_all(<span class="variable language_">self</span>.message, fn.mean(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;h&#x27;</span>))</span><br></pre></td></tr></table></figure><p></p><h4 id="2-子图聚合策略"><a href="#2-子图聚合策略" class="headerlink" title="2. 子图聚合策略"></a>2. 子图聚合策略</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TB
    S[选定中心节点] --&gt; E[提取k-hop邻居子图]
    E --&gt; F[子图编码器]
    F --&gt; G[全局池化]
  
    subgraph 子图编码器
        F --&gt; H[计数环结构]
        F --&gt; I[拓扑分析]
    end
  </pre></div><p>实际实现：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">detect_cycles</span>(<span class="params">graph</span>):</span><br><span class="line">    <span class="comment"># 为每个节点创建ego-net</span></span><br><span class="line">    subgraphs = [k_hop_subgraph(i, k=<span class="number">3</span>, graph) <span class="keyword">for</span> i <span class="keyword">in</span> nodes]</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 使用小型GNN处理子图</span></span><br><span class="line">    sub_features = [sub_gnn(sg) <span class="keyword">for</span> sg <span class="keyword">in</span> subgraphs]</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> torch.stack(sub_features)</span><br></pre></td></tr></table></figure><p></p><h4 id="3-持久同调嵌入"><a href="#3-持久同调嵌入" class="headerlink" title="3. 持久同调嵌入"></a>3. 持久同调嵌入</h4><p>利用拓扑数据分析(TDA)工具：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gudhi <span class="keyword">import</span> persistence_graphical_tools</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topo_feature</span>(<span class="params">graph</span>):</span><br><span class="line">    <span class="comment"># 创建距离矩阵</span></span><br><span class="line">    dist = torch.cdist(node_feats, node_feats)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 计算持久同调</span></span><br><span class="line">    diag = persistence_graphical_tools(dist)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 提取环特征</span></span><br><span class="line">    cycle_features = [d[<span class="number">1</span>] - d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> diag <span class="keyword">if</span> d[<span class="number">2</span>] == <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 维数1对应环</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> cycle_features</span><br></pre></td></tr></table></figure><p></p><h4 id="4-位置编码增强"><a href="#4-位置编码增强" class="headerlink" title="4. 位置编码增强"></a>4. 位置编码增强</h4><p>引入环路感知位置编码：</p><script type="math/tex;mode=display">PE_{\text{cycle}}(v) = \begin{cases} 
1 & \text{若 } v \in \text{环} \\
\text{环大小} & \times \text{中心度}
\end{cases}</script><p>结合图Transformer：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CycleFormer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.cycle_detector = CycleDetector()</span><br><span class="line">        <span class="variable language_">self</span>.transformer = Graphformer()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph</span>):</span><br><span class="line">        cycle_pe = <span class="variable language_">self</span>.cycle_detector(graph)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transformer(graph, add_pe=cycle_pe)</span><br></pre></td></tr></table></figure><p></p><h3 id="五、解决方案效果对比"><a href="#五、解决方案效果对比" class="headerlink" title="五、解决方案效果对比"></a>五、解决方案效果对比</h3><div class="table-container"><table><thead><tr><th>方法</th><th>准确率(5+环)</th><th>时间复杂度</th><th>适用场景</th></tr></thead><tbody><tr><td>标准MPGNN</td><td>≤65%</td><td>O(</td><td>非环敏感任务</td></tr><tr><td>k-GNN (k=3)</td><td>83.2%</td><td>O(n³)</td><td>小规模图</td></tr><tr><td>子图聚合</td><td>91.7%</td><td>O(</td><td>中等图</td></tr><tr><td>持久同调</td><td>88.4%</td><td>O(n²)</td><td>要求理论保障</td></tr><tr><td>环感知Transformer</td><td><strong>97.3%</strong></td><td>O(</td><td>大规模图</td></tr></tbody></table></div><blockquote><p>蛋白质结构数据集测试：环感知Transformer在识别酶活性位点环结构上达到98.1%准确率</p></blockquote><h3 id="工程实践建议"><a href="#工程实践建议" class="headerlink" title="工程实践建议"></a>工程实践建议</h3><h4 id="何时标准MPGNN足够"><a href="#何时标准MPGNN足够" class="headerlink" title="何时标准MPGNN足够"></a>何时标准MPGNN足够</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph LR
    小环检测 --&gt; A[节点数&lt;8]
    局部环感知 --&gt; B[3-5跳邻域内]
    粗粒度环存在判断 --&gt; C[二元分类]
  </pre></div><h4 id="何时需要增强"><a href="#何时需要增强" class="headerlink" title="何时需要增强"></a>何时需要增强</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TB
    精确环计数 --&gt; D[药物分子环统计]
    大环检测 --&gt; E[交通网络环路识别]
    拓扑敏感任务 --&gt; F[电路反馈环分析]
  </pre></div><h4 id="PyG实现示例"><a href="#PyG实现示例" class="headerlink" title="PyG实现示例"></a>PyG实现示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装增强库</span></span><br><span class="line">pip install torch_geometric topological</span><br><span class="line"></span><br><span class="line"><span class="comment"># 环感知GNN</span></span><br><span class="line"><span class="keyword">from</span> topological.nn <span class="keyword">import</span> CycleFeatures</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CycleGNN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GCNConv(<span class="number">16</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="variable language_">self</span>.cycle_extractor = CycleFeatures(max_dim=<span class="number">1</span>)  <span class="comment"># 专注环(维1同调)</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        topo_feats = <span class="variable language_">self</span>.cycle_extractor(data.x, data.edge_index)</span><br><span class="line">        x = torch.cat([data.x, topo_feats], dim=<span class="number">1</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x, data.edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="理论总结"><a href="#理论总结" class="headerlink" title="理论总结"></a>理论总结</h3><p>标准信息传递图神经网络受限于其<strong>局部聚合机制</strong>和<strong>1-WL表达能力</strong>，无法可靠检测图中的环结构。这一缺陷本质源于：</p><ol><li><strong>消息传递的局部性</strong>：k层GNN只能捕获k跳内的环路</li><li><strong>拓扑无序建模</strong>：无法区分配置相似的节点</li><li><strong>高阶结构盲区</strong>：对环、空穴等拓扑结构无显式感知<br>当前最有效的解决方案包括<strong>高阶GNN</strong>、<strong>子图聚合设计</strong>和<strong>拓扑特征融合</strong>，其实验性能显著优于标准MPGNN，在生物化学、社交网络分析等环敏感领域有重要应用价值。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs224w/slides/08-graph-transformer1.pdf">Stanford CS224W Fall 2024 Lecture 8</a></li><li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs224w/">Stanford CS224W Fall 2024</a></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://epsilonzyj.github.io">EpsilonZ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://epsilonzyj.github.io/posts/graph-transformer.html">https://epsilonzyj.github.io/posts/graph-transformer.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://epsilonzyj.github.io" target="_blank">EpsilonZ's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Graph-ML/">Graph ML</a><a class="post-meta__tags" href="/tags/Graph-Transformer/">Graph Transformer</a></div><div class="post-share"><div class="social-share" data-image="/img/GNNCover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/e110b7d9.html" title="进程守护screen"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_3190.JPG" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">进程守护screen</div></div><div class="info-2"><div class="info-item-1">使用screen进行进程守护，包括screen使用方式和常用命令。</div></div></div></a><a class="pagination-related" href="/posts/NAGphormer.html" title="论文阅读：NAGphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">论文阅读：NAGphormer</div></div><div class="info-2"><div class="info-item-1">NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/DUALFormer.html" title="论文阅读：DUALFormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2026-02-24</div><div class="info-item-2">论文阅读：DUALFormer</div></div><div class="info-2"><div class="info-item-1">DUALFormer:Dual Graph Transformer</div></div></div></a><a class="pagination-related" href="/posts/HDSE.html" title="论文阅读：HDSE"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2026-02-24</div><div class="info-item-2">论文阅读：HDSE</div></div><div class="info-2"><div class="info-item-1">Enhancing Graph Transformers with Hierarchical Distance Structural Encoding</div></div></div></a><a class="pagination-related" href="/posts/NAGphormer.html" title="论文阅读：NAGphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2026-02-24</div><div class="info-item-2">论文阅读：NAGphormer</div></div><div class="info-2"><div class="info-item-1">NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs</div></div></div></a><a class="pagination-related" href="/posts/NTFormer.html" title="论文阅读：NTFormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2026-02-24</div><div class="info-item-2">论文阅读：NTFormer</div></div><div class="info-2"><div class="info-item-1">NTFormer:A Composite Node Tokenized Graph Transformer for Node Classification</div></div></div></a><a class="pagination-related" href="/posts/Primphormer.html" title="论文阅读：Primphormer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2026-02-24</div><div class="info-item-2">论文阅读：Primphormer</div></div><div class="info-2"><div class="info-item-1">Primphormer:Efficient Graph Transformers with Primal Representations</div></div></div></a><a class="pagination-related" href="/posts/3374f76a.html" title="Graph Transformer中的问题 ｜ Problems With Graph Transformers"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2026-02-24</div><div class="info-item-2">Graph Transformer中的问题 ｜ Problems With Graph Transformers</div></div><div class="info-2"><div class="info-item-1">Graph Transformer中存在的问题</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_2179.JPG" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">EpsilonZ</div><div class="author-info-description">To sleep, or to research, that is the question.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/EpsilonZYJ"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:yujie.zhou005@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a><a class="social-icon" href="https://github.com/EpsilonZYJ" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="https://www.linkedin.com/in/yujie-zhou-info" target="_blank" title="LinkedIn"><i class="fa-brands fa-square-linkedin" style="color:#2867b2"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Research everyday!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformers"><span class="toc-number">1.</span> <span class="toc-text">Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.</span> <span class="toc-text">自注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-attention"><span class="toc-number">1.1.1.</span> <span class="toc-text">Self-attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-head-self-attention"><span class="toc-number">1.1.2.</span> <span class="toc-text">Multi-head self-attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformers-vs-GNN"><span class="toc-number">1.2.</span> <span class="toc-text">Transformers vs. GNN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Self-attention-vs-message-passing"><span class="toc-number">2.</span> <span class="toc-text">Self-attention vs. message passing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-attention-Update"><span class="toc-number">2.1.</span> <span class="toc-text">Self-attention Update</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#A-New-Design-Landscape-for-Graph-Transformers"><span class="toc-number">3.</span> <span class="toc-text">A New Design Landscape for Graph Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Transformers%E5%A4%84%E7%90%86%E5%9B%BE"><span class="toc-number">3.1.</span> <span class="toc-text">使用Transformers处理图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">3.1.1.</span> <span class="toc-text">Transformer中的位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Graph-Transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">3.1.2.</span> <span class="toc-text">Graph Transformer中的位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E8%B7%9D%E7%A6%BB%EF%BD%9CRelative-distances"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">相对距离｜Relative distances</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BD%9CLaplacian-Eigenvector-Positional-Encoding"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">拉普拉斯特征向量位置编码｜Laplacian Eigenvector Positional Encoding</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%AD%E5%A4%84%E7%90%86%E8%BE%B9%E7%89%B9%E5%BE%81"><span class="toc-number">3.1.3.</span> <span class="toc-text">在自注意力机制中处理边特征</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9AGraph-Transformer-Design-Space"><span class="toc-number">3.2.</span> <span class="toc-text">总结：Graph Transformer Design Space</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sign-invariant-Laplacian-positional-encodings-for-graph-Transformers"><span class="toc-number">4.</span> <span class="toc-text">Sign invariant Laplacian positional encodings for graph Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Sign-Ambiguity-is-a-Problem"><span class="toc-number">4.1.</span> <span class="toc-text">Sign Ambiguity is a Problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E6%97%A0%E5%85%B3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.2.</span> <span class="toc-text">符号无关神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SignNet"><span class="toc-number">4.3.</span> <span class="toc-text">SignNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">5.</span> <span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9B%BE%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E7%9A%84%E9%A2%91%E7%8E%87%E8%A7%A3%E9%87%8A%EF%BC%9A%E4%BB%8E%E5%85%A8%E5%B1%80%E7%BB%93%E6%9E%84%E5%88%B0%E5%B1%80%E9%83%A8%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="toc-number">5.1.</span> <span class="toc-text">1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%A0%B8%E5%BF%83%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.1.</span> <span class="toc-text">一、核心数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%9B%BE%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="toc-number">5.1.1.1.</span> <span class="toc-text">1. 图拉普拉斯矩阵的本质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%91%9E%E5%88%A9%E5%95%86%EF%BC%88Rayleigh-Quotient%EF%BC%89"><span class="toc-number">5.1.1.2.</span> <span class="toc-text">2. 瑞利商（Rayleigh Quotient）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%83%BD%E9%87%8F%E6%B3%9B%E5%87%BD%EF%BC%88Dirichlet-Energy%EF%BC%89"><span class="toc-number">5.1.1.3.</span> <span class="toc-text">3. 能量泛函（Dirichlet Energy）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BD%8E%E9%A2%91%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%EF%BC%9A%E5%85%A8%E5%B1%80%E7%BB%93%E6%9E%84%EF%BC%88%E5%B0%8F%E7%89%B9%E5%BE%81%E5%80%BC%EF%BC%89"><span class="toc-number">5.1.2.</span> <span class="toc-text">二、低频特征向量：全局结构（小特征值）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%9B%B6%E7%89%B9%E5%BE%81%E5%80%BC%EF%BC%88%CE%BB-0%EF%BC%89"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">1. 零特征值（λ&#x3D;0）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%9C%80%E5%B0%8F%E9%9D%9E%E9%9B%B6%E7%89%B9%E5%BE%81%E5%80%BC%EF%BC%88%CE%BB%E2%82%82%EF%BC%89"><span class="toc-number">5.1.2.2.</span> <span class="toc-text">2. 最小非零特征值（λ₂）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BD%8E%E9%A2%91%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%EF%BC%88%CE%BB%E2%82%83-%CE%BB%E2%82%84%E7%AD%89%EF%BC%89"><span class="toc-number">5.1.2.3.</span> <span class="toc-text">3. 低频特征向量（λ₃, λ₄等）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%AB%98%E9%A2%91%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%EF%BC%9A%E5%B1%80%E9%83%A8%E5%AF%B9%E7%A7%B0%E6%80%A7%EF%BC%88%E5%A4%A7%E7%89%B9%E5%BE%81%E5%80%BC%EF%BC%89"><span class="toc-number">5.1.3.</span> <span class="toc-text">三、高频特征向量：局部对称性（大特征值）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%AB%98%E9%A2%91%E5%90%91%E9%87%8F%E7%9A%84%E7%89%B9%E6%80%A7"><span class="toc-number">5.1.3.1.</span> <span class="toc-text">1. 高频向量的特性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%B1%80%E9%83%A8%E5%AF%B9%E7%A7%B0%E6%80%A7%E8%A1%A8%E7%8E%B0"><span class="toc-number">5.1.3.2.</span> <span class="toc-text">2. 局部对称性表现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%83%85%E5%BD%A21%EF%BC%9A%E6%98%9F%E5%BD%A2%E5%9B%BE%E4%B8%AD%E5%BF%83"><span class="toc-number">5.1.3.2.1.</span> <span class="toc-text">情形1：星形图中心</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%83%85%E5%BD%A22%EF%BC%9A%E7%BD%91%E6%A0%BC%E5%B1%80%E9%83%A8%E5%AF%B9%E7%A7%B0"><span class="toc-number">5.1.3.2.2.</span> <span class="toc-text">情形2：网格局部对称</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%EF%BC%9A%E5%BC%A6%E6%8C%AF%E5%8A%A8"><span class="toc-number">5.1.3.3.</span> <span class="toc-text">3. 物理模拟：弦振动</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%95%B0%E5%AD%A6%E8%AF%81%E6%98%8E%EF%BC%9A%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E6%B8%90%E5%8F%98%E9%A2%91%E7%8E%87"><span class="toc-number">5.1.4.</span> <span class="toc-text">四、数学证明：特征值与渐变频率</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8F%98%E5%88%86%E7%89%B9%E6%80%A7%E8%AF%81%E6%98%8E"><span class="toc-number">5.1.4.1.</span> <span class="toc-text">1. 变分特性证明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%A2%AF%E5%BA%A6%E8%83%BD%E9%87%8F%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90"><span class="toc-number">5.1.4.2.</span> <span class="toc-text">2. 梯度能量量化分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E5%80%BC%E5%BA%8F%E5%88%97%E7%9A%84%E7%89%A9%E7%90%86%E5%86%85%E6%B6%B5"><span class="toc-number">5.1.4.3.</span> <span class="toc-text">3. 特征值序列的物理内涵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E6%A1%88%E4%BE%8B"><span class="toc-number">5.1.5.</span> <span class="toc-text">五、可视化案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Karate-Club%E7%BD%91%E7%BB%9C"><span class="toc-number">5.1.5.1.</span> <span class="toc-text">1. Karate Club网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%86%E5%AD%90%E7%BB%93%E6%9E%84%EF%BC%88%E8%8B%AF%E7%8E%AFC%E2%82%86H%E2%82%86%EF%BC%89"><span class="toc-number">5.1.5.2.</span> <span class="toc-text">2. 分子结构（苯环C₆H₆）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3D%E7%82%B9%E4%BA%91%EF%BC%88%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%85%94%E5%AD%90%EF%BC%89"><span class="toc-number">5.1.5.3.</span> <span class="toc-text">3. 3D点云（斯坦福兔子）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E5%90%AF%E7%A4%BA"><span class="toc-number">5.1.6.</span> <span class="toc-text">六、实际应用启示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1"><span class="toc-number">5.1.6.1.</span> <span class="toc-text">1. 图神经网络设计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%9B%BE%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF"><span class="toc-number">5.1.6.2.</span> <span class="toc-text">2. 图压缩技术</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E5%BA%94%E7%94%A8"><span class="toc-number">5.1.6.3.</span> <span class="toc-text">3. 异常检测应用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E6%80%BB%E7%BB%93"><span class="toc-number">5.1.7.</span> <span class="toc-text">深度理解总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%8E%AF%E6%A3%80%E6%B5%8B%E5%B1%80%E9%99%90%E6%80%A7%EF%BC%9A%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90%E4%B8%8E%E7%AA%81%E7%A0%B4%E6%96%B9%E6%B3%95"><span class="toc-number">5.2.</span> <span class="toc-text">2.信息传递图神经网络的环检测局限性：理论分析与突破方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%9AWeisfeiler-Lehman-WL-%E6%B5%8B%E8%AF%95%E4%B8%8EMPGNN%E7%9A%84%E7%AD%89%E4%BB%B7%E6%80%A7"><span class="toc-number">5.2.1.</span> <span class="toc-text">一、理论基础：Weisfeiler-Lehman (WL) 测试与MPGNN的等价性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-WL%E6%B5%8B%E8%AF%95%E7%9A%84%E7%8E%AF%E6%A3%80%E6%B5%8B%E9%99%90%E5%88%B6"><span class="toc-number">5.2.1.1.</span> <span class="toc-text">1. WL测试的环检测限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-MPGNN%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%95%8C%E5%AE%9A%E7%90%86"><span class="toc-number">5.2.1.2.</span> <span class="toc-text">2. MPGNN的表达式界定理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81MPGNN%E6%9E%B6%E6%9E%84%E7%9A%84%E6%9C%BA%E5%88%B6%E9%99%90%E5%88%B6"><span class="toc-number">5.2.2.</span> <span class="toc-text">二、MPGNN架构的机制限制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%B6%88%E6%81%AF%E8%81%9A%E5%90%88%E7%9A%84%E5%B1%80%E9%83%A8%E6%80%A7"><span class="toc-number">5.2.2.1.</span> <span class="toc-text">1. 消息聚合的局部性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%8E%92%E5%88%97%E4%B8%8D%E5%8F%98%E6%80%A7%E7%9A%84%E7%BA%A6%E6%9D%9F"><span class="toc-number">5.2.2.2.</span> <span class="toc-text">2. 排列不变性的约束</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81%E4%B8%8E%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">5.2.3.</span> <span class="toc-text">三、实验验证与案例分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%8E%AF%E6%A3%80%E6%B5%8B%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">5.2.3.1.</span> <span class="toc-text">1. 环检测基准测试</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%85%B8%E5%9E%8B%E6%A1%88%E4%BE%8B%EF%BC%9A%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84%E7%8E%AF"><span class="toc-number">5.2.3.2.</span> <span class="toc-text">2. 典型案例：不同大小的环</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%8A%80%E6%9C%AF%E5%89%8D%E6%B2%BF%EF%BC%9A%E7%AA%81%E7%A0%B4%E7%8E%AF%E6%A3%80%E6%B5%8B%E9%99%90%E5%88%B6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">5.2.4.</span> <span class="toc-text">四、技术前沿：突破环检测限制的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%AB%98%E9%98%B6%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92-k-GNNs"><span class="toc-number">5.2.4.1.</span> <span class="toc-text">1. 高阶消息传递 (k-GNNs)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AD%90%E5%9B%BE%E8%81%9A%E5%90%88%E7%AD%96%E7%95%A5"><span class="toc-number">5.2.4.2.</span> <span class="toc-text">2. 子图聚合策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%8C%81%E4%B9%85%E5%90%8C%E8%B0%83%E5%B5%8C%E5%85%A5"><span class="toc-number">5.2.4.3.</span> <span class="toc-text">3. 持久同调嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%A2%9E%E5%BC%BA"><span class="toc-number">5.2.4.4.</span> <span class="toc-text">4. 位置编码增强</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94"><span class="toc-number">5.2.5.</span> <span class="toc-text">五、解决方案效果对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E5%BB%BA%E8%AE%AE"><span class="toc-number">5.2.6.</span> <span class="toc-text">工程实践建议</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%95%E6%97%B6%E6%A0%87%E5%87%86MPGNN%E8%B6%B3%E5%A4%9F"><span class="toc-number">5.2.6.1.</span> <span class="toc-text">何时标准MPGNN足够</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%95%E6%97%B6%E9%9C%80%E8%A6%81%E5%A2%9E%E5%BC%BA"><span class="toc-number">5.2.6.2.</span> <span class="toc-text">何时需要增强</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PyG%E5%AE%9E%E7%8E%B0%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.2.6.3.</span> <span class="toc-text">PyG实现示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93"><span class="toc-number">5.2.7.</span> <span class="toc-text">理论总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">5.3.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/expo-ios-cicd.html" title="在Xcode Cloud上为React Native+Expo项目设置CI/CD"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_1699.JPG" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="在Xcode Cloud上为React Native+Expo项目设置CI/CD"></a><div class="content"><a class="title" href="/posts/expo-ios-cicd.html" title="在Xcode Cloud上为React Native+Expo项目设置CI/CD">在Xcode Cloud上为React Native+Expo项目设置CI/CD</a><time datetime="2025-11-12T10:34:25.000Z" title="发表于 2025-11-12 18:34:25">2025-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/python-env.html" title="How to use python effectively step by step"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/IMG_3174.PNG" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="How to use python effectively step by step"></a><div class="content"><a class="title" href="/posts/python-env.html" title="How to use python effectively step by step">How to use python effectively step by step</a><time datetime="2025-11-07T08:56:44.000Z" title="发表于 2025-11-07 16:56:44">2025-11-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/HDSE.html" title="论文阅读：HDSE"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：HDSE"></a><div class="content"><a class="title" href="/posts/HDSE.html" title="论文阅读：HDSE">论文阅读：HDSE</a><time datetime="2025-11-05T07:38:12.000Z" title="发表于 2025-11-05 15:38:12">2025-11-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：Simple Path Structural Encoding"></a><div class="content"><a class="title" href="/posts/SPSE.html" title="论文阅读：Simple Path Structural Encoding">论文阅读：Simple Path Structural Encoding</a><time datetime="2025-10-27T08:47:27.000Z" title="发表于 2025-10-27 16:47:27">2025-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Primphormer.html" title="论文阅读：Primphormer"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/GNNCover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="论文阅读：Primphormer"></a><div class="content"><a class="title" href="/posts/Primphormer.html" title="论文阅读：Primphormer">论文阅读：Primphormer</a><time datetime="2025-10-25T05:20:01.000Z" title="发表于 2025-10-25 13:20:01">2025-10-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/img/GNNCover.png)"><div id="footer-wrap"><div class="copyright">&copy;2025 - 2026 By EpsilonZ</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><div class="app-refresh" id="app-refresh" style="position:fixed;top:-2.2rem;left:0;right:0;z-index:99999;padding:0 1rem;font-size:15px;height:2.2rem;transition:all .3s ease"><div class="app-refresh-wrap" style="display:flex;color:#fff;height:100%;align-items:center;justify-content:center"><label>✨ 有新文章啦！ 👉</label><a href="javascript:void(0)" onclick="location.reload()"><span style="color:#fff;text-decoration:underline;cursor:pointer">偷偷看一看 👀</span></a></div></div><script>if ('serviceWorker' in navigator) {
if (navigator.serviceWorker.controller) {
navigator.serviceWorker.addEventListener('controllerchange', function() {
showNotification()
})
}
window.addEventListener('load', function() {
navigator.serviceWorker.register('/sw.js')
})
}
function showNotification() {
if (GLOBAL_CONFIG.Snackbar) {
var snackbarBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
GLOBAL_CONFIG.Snackbar.bgLight :
GLOBAL_CONFIG.Snackbar.bgDark
var snackbarPos = GLOBAL_CONFIG.Snackbar.position
Snackbar.show({
text: '✨ 有新文章啦！ 👉',
backgroundColor: snackbarBg,
duration: 500000,
pos: snackbarPos,
actionText: '🍗点击食用🍔',
actionTextColor: '#fff',
onActionClick: function(e) {
location.reload()
},
})
} else {
var showBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
'#3b70fc' :
'#1f1f1f'
var cssText = `top: 0; background: ${showBg};`
document.getElementById('app-refresh').style.cssText = cssText
}
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script data-pjax>function butterfly_footer_beautify_injector_config(){var t=document.getElementById("footer-wrap");console.log("已挂载butterfly_footer_beautify"),t.insertAdjacentHTML("beforeend",'<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a></p>')}for(var elist="null".split(","),cpage=location.pathname,epage="all",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;"all"===epage&&0==flag?butterfly_footer_beautify_injector_config():epage===cpage&&butterfly_footer_beautify_injector_config()</script><script async src="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.js"></script><script async src="/js/ali_font.js"></script><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","1.5s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("flink-list-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__flipInY"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("flink-list-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__animated"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("article-sort-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__slideInRight"),arr[i].setAttribute("data-wow-duration","1.5s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("site-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__flipInY"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script><script async>for(var arr=document.getElementsByClassName("site-card"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__animated"),arr[i].setAttribute("data-wow-duration","3s"),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script></body></html>
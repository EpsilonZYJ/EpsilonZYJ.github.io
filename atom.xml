<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EpsilonZ&#39;s Blog</title>
  
  <subtitle>Compile the world, save the world.</subtitle>
  <link href="https://epsilonzyj.github.io/atom.xml" rel="self"/>
  
  <link href="https://epsilonzyj.github.io/"/>
  <updated>2025-10-20T11:51:24.023Z</updated>
  <id>https://epsilonzyj.github.io/</id>
  
  <author>
    <name>EpsilonZ</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Page Rank算法</title>
    <link href="https://epsilonzyj.github.io/posts/page-rank.html"/>
    <id>https://epsilonzyj.github.io/posts/page-rank.html</id>
    <published>2025-10-20T11:39:16.000Z</published>
    <updated>2025-10-20T11:51:24.023Z</updated>
    
    <content type="html"><![CDATA[<p>最近笔者在读有关Graph Transformer中有关tokenize的相关论文时，读到一篇采用page rank做的相关工作。虽然这个算法很有名而且也并不是很复杂，但笔者在此之前并没有了解过，只是知道有这么一个东西。好在现在有强大的LLM，可以很快速的向我进行详细解释算法的具体内容，于是便有了这篇Blog进行存档。</p><h2 id="1-算法简介"><a href="#1-算法简介" class="headerlink" title="1. 算法简介"></a>1. 算法简介</h2><p><strong>PageRank</strong>是由Google的创始人Larry Page和Sergey Brin于1996年在斯坦福大学开发的一种链接分析算法，用于衡量网页的相对重要性。PageRank算法通过分析网页之间的链接关系来确定网页的权威性和重要性值。</p><h3 id="算法基本思想"><a href="#算法基本思想" class="headerlink" title="算法基本思想"></a>算法基本思想</h3><p>PageRank的核心思想基于一个简单的假设：<strong>一个网页的重要性可以通过指向它的链接数量和质量来衡量</strong>。就像学术引用中，被大量高质量论文引用的研究论文通常更为重要一样，被许多重要网页链接的网页也应该更加重要。</p><h2 id="2-数学公式与推导"><a href="#2-数学公式与推导" class="headerlink" title="2. 数学公式与推导"></a>2. 数学公式与推导</h2><h3 id="2-1-基本数学公式"><a href="#2-1-基本数学公式" class="headerlink" title="2.1 基本数学公式"></a>2.1 基本数学公式</h3><p>PageRank的基本数学公式如下：</p><script type="math/tex; mode=display">PR(A) = (1-d) + d * (\frac{PR(T1)}{C(T1)} + \frac{PR(T2)}{C(T2)} + ... + \frac{PR(Tn)}{C(Tn)})</script><p>其中：</p><ul><li><code>PR(A)</code>：页面A的PageRank值</li><li><code>d</code>：阻尼系数（damping factor），通常设置为0.85</li><li><code>T1...Tn</code>：所有指向页面A的页面</li><li><code>C(Ti)</code>：页面Ti的出链数量</li><li><code>1-d</code>：随机跳转概率</li></ul><h3 id="2-2-完整PageRank公式"><a href="#2-2-完整PageRank公式" class="headerlink" title="2.2 完整PageRank公式"></a>2.2 完整PageRank公式</h3><p>考虑随机游走模型，完整的PageRank公式可以表示为：</p><script type="math/tex; mode=display">PR(pᵢ) = \frac{(1-d)}{N} + d * ∑\frac{PR(pⱼ)}{L(pⱼ)}</script><p>其中：</p><ul><li><code>N</code>：网页总数</li><li><code>PR(pⱼ)</code>：页面pⱼ的PageRank值</li><li><code>L(pⱼ)</code>：页面pⱼ的出链数量</li><li><code>∑</code>：对所有链接到页面pᵢ的页面求和</li></ul><h3 id="2-3-矩阵表示"><a href="#2-3-矩阵表示" class="headerlink" title="2.3 矩阵表示"></a>2.3 矩阵表示</h3><p>PageRank可以用线性代数的形式表示：</p><script type="math/tex; mode=display">R = d * M * R + (1-d) * \frac{v}{N}</script><p>其中：</p><ul><li><code>R</code>：PageRank向量</li><li><code>M</code>：转移矩阵（adjacency matrix）</li><li><code>v</code>：单位向量</li><li><code>d</code>：阻尼系数</li><li><code>N</code>：页面总数</li></ul><h2 id="3-随机游走（Random-Walk）的作用"><a href="#3-随机游走（Random-Walk）的作用" class="headerlink" title="3. 随机游走（Random Walk）的作用"></a>3. 随机游走（Random Walk）的作用</h2><h3 id="3-1-随机游走模型"><a href="#3-1-随机游走模型" class="headerlink" title="3.1 随机游走模型"></a>3.1 随机游走模型</h3><p>PageRank算法基于<strong>随机游走模型</strong>（Random Walk），该模型可以这样理解：<br>想象一个随机上网者，他按照以下规则上网：</p><ol><li>85%的时间，他点击当前页面的链接继续浏览</li><li>15%的时间，他随机跳转到其他页面</li></ol><h3 id="3-2-随机游走的数学原理"><a href="#3-2-随机游走的数学原理" class="headerlink" title="3.2 随机游走的数学原理"></a>3.2 随机游走的数学原理</h3><p>在随机游走框架下，PageRank值代表：</p><ul><li><strong>长期访问概率</strong>：一个随机访问者在经过足够长时间的访问后，停留在某个页面的概率</li><li><strong>马尔可夫链稳态分布</strong>：PageRank是马尔可夫链的稳态分布</li></ul><h4 id="数学推导过程"><a href="#数学推导过程" class="headerlink" title="数学推导过程"></a>数学推导过程</h4><ol><li><p><strong>定义转移概率</strong>：</p><ul><li>从页面i到页面j的转移概率为 <code>P(i→j) = 1/L(i)</code>（如果存在链接）</li><li>随机跳转概率为 <code>(1-d)/N</code></li></ul></li><li><p><strong>转移矩阵构造</strong>：</p><script type="math/tex; mode=display">M = d * A + (1-d) * B</script><p>其中：</p><ul><li><code>A</code>：原始转移矩阵</li><li><code>B</code>：随机跳转矩阵</li><li><code>d</code>：阻尼系数</li></ul></li><li><p><strong>求解稳态分布</strong>：<br>通过求解 <code>R = M * R</code> 得到PageRank向量</p></li></ol><h3 id="3-3-随机游走的重要性"><a href="#3-3-随机游走的重要性" class="headerlink" title="3.3 随机游走的重要性"></a>3.3 随机游走的重要性</h3><p>随机游走模型解决了以下问题：</p><ol><li><strong>避免死循环</strong>：通过随机跳转防止算法陷入无限循环</li><li><strong>处理无出链页面</strong>：确保每个页面都能被访问到</li><li><strong>保证收敛性</strong>：利用马尔可夫链的收敛性定理确保算法稳定</li></ol><h2 id="4-算法的实际应用步骤"><a href="#4-算法的实际应用步骤" class="headerlink" title="4. 算法的实际应用步骤"></a>4. 算法的实际应用步骤</h2><h3 id="4-1-迭代计算方法"><a href="#4-1-迭代计算方法" class="headerlink" title="4.1 迭代计算方法"></a>4.1 迭代计算方法</h3><p>PageRank通常通过迭代计算来求解：</p><ol><li><strong>初始化</strong>：所有页面的PageRank值设为1/N</li><li><strong>迭代更新</strong>：<script type="math/tex; mode=display">PR_{new}(A) = \frac{(1-d)}{N} + d * ∑\frac{PR_old(T)}{L(T)}</script></li><li><strong>收敛判断</strong>：当相邻两次迭代的变化小于设定阈值时停止</li></ol><h3 id="4-2-Python实现示例"><a href="#4-2-Python实现示例" class="headerlink" title="4.2 Python实现示例"></a>4.2 Python实现示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pagerank</span>(<span class="params">M, d=<span class="number">0.85</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;PageRank算法实现&quot;&quot;&quot;</span></span><br><span class="line">    N = M.shape[<span class="number">1</span>]</span><br><span class="line">    w = np.ones(N) / N  <span class="comment"># 初始概率分布</span></span><br><span class="line">    M_hat = d * M + (<span class="number">1</span>-d) / N  <span class="comment"># Google矩阵</span></span><br><span class="line">    v = M_hat @ w + (<span class="number">1</span>-d)/N</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 迭代直到收敛</span></span><br><span class="line">    <span class="keyword">while</span> np.linalg.norm(w - v) &gt;= <span class="number">1e-10</span>:</span><br><span class="line">        w = v</span><br><span class="line">        v = M_hat @ w + (<span class="number">1</span>-d)/N</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><h2 id="5-算法特性与优势"><a href="#5-算法特性与优势" class="headerlink" title="5. 算法特性与优势"></a>5. 算法特性与优势</h2><h3 id="5-1-主要特性"><a href="#5-1-主要特性" class="headerlink" title="5.1 主要特性"></a>5.1 主要特性</h3><ol><li><strong>自引用处理</strong>：页面不引用自己的链接</li><li><strong>等值分配</strong>：一个页面的PageRank平均分配给所有出链</li><li><strong>阻尼效应</strong>：阻尼系数决定了链接传递的效率</li></ol><h3 id="5-2-算法优势"><a href="#5-2-算法优势" class="headerlink" title="5.2 算法优势"></a>5.2 算法优势</h3><ol><li><strong>权威性评估</strong>：能识别真正权威的网页</li><li><strong>可扩展性</strong>：可处理大规模网络结构</li><li><strong>数学严谨性</strong>：基于坚实的数学理论基础</li></ol><h2 id="6-实际应用与扩展"><a href="#6-实际应用与扩展" class="headerlink" title="6. 实际应用与扩展"></a>6. 实际应用与扩展</h2><h3 id="6-1-在搜索引擎中的应用"><a href="#6-1-在搜索引擎中的应用" class="headerlink" title="6.1 在搜索引擎中的应用"></a>6.1 在搜索引擎中的应用</h3><ul><li><strong>Google搜索</strong>：PageRank是Google早期最重要的排名算法之一</li><li><strong>结果排序</strong>：结合其他因素进行综合排名</li></ul><h3 id="6-2-扩展应用"><a href="#6-2-扩展应用" class="headerlink" title="6.2 扩展应用"></a>6.2 扩展应用</h3><ol><li><strong>社交网络分析</strong>：评估用户影响力</li><li><strong>学术引用分析</strong>：论文重要性评估</li><li><strong>推荐系统</strong>：基于图结构的推荐</li><li><strong>生物信息学</strong>：蛋白质网络分析</li></ol><h3 id="6-3-现代搜索引擎的演变"><a href="#6-3-现代搜索引擎的演变" class="headerlink" title="6.3 现代搜索引擎的演变"></a>6.3 现代搜索引擎的演变</h3><p>虽然PageRank是Google早期的核心算法，但现在的搜索引擎已经结合了数百种因素，包括：</p><ul><li>内容相关性</li><li>用户行为数据</li><li>语义理解</li><li>个性化搜索</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>PageRank算法的伟大之处在于：</p><ol><li><strong>简洁而深刻</strong>：用简单的数学概念解决了复杂的问题</li><li><strong>理论基础扎实</strong>：基于图论和马尔可夫链的严格数学推导</li><li><strong>实际效果显著</strong>：极大地改善了搜索引擎的搜索质量</li><li><strong>影响深远</strong>：开创了链接分析的新领域</li></ol><p>PageRank不仅是一个算法，更是一种思考网络结构重要性的全新视角，其影响力已经远远超出了搜索引擎的范畴，成为现代网络分析的重要基础。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li>Wikipedia: <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a></li><li>Cornell大学讲座: <a href="https://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html">The Mathematics of Google Search</a></li><li>Medium: <a href="https://medium.com/biased-algorithms/pagerank-algorithm-explained-5f5c6a8c6696">PageRank Algorithm Explained</a></li><li>MIT课程资料: <a href="https://ocw.mit.edu/courses/6-042j-mathematics-for-computer-science-spring-2015/mit6_042js15_session35.pdf">Random Walks &amp; PageRank</a></li></ol>]]></content>
    
    
    <summary type="html">Page Rank算法的简要介绍</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph/"/>
    
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>NAGphormer 论文阅读</title>
    <link href="https://epsilonzyj.github.io/posts/NAGphormer.html"/>
    <id>https://epsilonzyj.github.io/posts/NAGphormer.html</id>
    <published>2025-10-17T06:39:01.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NAGphormer-A-Tokenized-Graph-Transformer-For-Node-Classification-In-Large-Graphs"><a href="#NAGphormer-A-Tokenized-Graph-Transformer-For-Node-Classification-In-Large-Graphs" class="headerlink" title="NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs"></a>NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs</h1><hr><h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><ul><li>作者: Jinsong Chen, Kaiyuan Gao, Gaichao Li, Kun He</li><li>日期: 2023</li><li>出处: ICLR </li><li>开源代码: <a href="https://github.com/JHL-HUST/NAGphormer">https://github.com/JHL-HUST/NAGphormer</a></li><li>论文地址: <a href="https://arxiv.org/abs/2206.04910">https://arxiv.org/abs/2206.04910</a></li></ul><hr><h1 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h1><p>根据论文内容，本文的核心研究问题可概括为以下两点：</p><ol><li><strong>现有图Transformer的可扩展性瓶颈问题</strong><br>当前主流图Transformer（如GT、Graphormer等）在处理图数据时，将所有节点视为独立token并组合成单一长序列进行训练，导致自注意力机制的计算复杂度高达 $O(n²)$（$n$ 为节点数）。这使模型无法扩展到大规模图数据（如百万级节点），因为：（1）GPU内存无法承载全图训练；（2）传统GNN的优化策略（如节点采样、近似传播）不适用于全局注意力的Transformer架构。</li><li><strong>邻域信息利用效率不足</strong><br>GNN常因过平滑（over-smoothing）和过挤压（over-squashing）问题难以有效捕获深层邻域信息。而解耦GCN（如GPRGNN）虽通过固定权重聚合多跳邻域，但无法动态学习不同跳数的重要性。现有图Transformer虽引入图结构编码，却未充分考虑局部邻域的语义关联性。</li></ol><h1 id="创新方法"><a href="#创新方法" class="headerlink" title="创新方法"></a>创新方法</h1><p>基于论文内容，本文提出的核心创新方法是 <strong>NAGphormer</strong>（Neighborhood Aggregation Graph Transformer），其核心是 <strong>Hop2Token 模块</strong>和 <strong>基于注意力的读出机制</strong>，用于解决传统图 Transformer 在大规模图上面临的复杂度过高和无法批量训练的问题。具体创新点如下：</p><h3 id="1-Hop2Token-模块"><a href="#1-Hop2Token-模块" class="headerlink" title="1. Hop2Token 模块"></a>1. Hop2Token 模块</h3><ul><li><strong>核心思想</strong>：传统图 Transformer 将每个节点视为独立 token 组成长序列，导致自注意力计算复杂度为 $O(n^2)$，难以扩展到大图。Hop2Token <strong>将每个节点自身视为一个独立的序列(Sqeuence)</strong>。</li><li><strong>数学表示</strong>：对于节点 $v$，其 $k$ 跳邻居 $\mathcal{N}^k(v)$ 的信息被聚合成一个<strong>令牌(Token)</strong> $x_v^k$：<script type="math/tex; mode=display">x_v^k = \phi(\mathcal{N}^k(v)) \quad (5)</script></li><li><strong>序列构建</strong>：为节点 $v$ 构造一个包含从 $0$ 跳（自身）到 $K$ 跳邻居聚合特征的<strong>令牌序列</strong>：<script type="math/tex; mode=display">S_v = (x_v^0, x_v^1, ..., x_v^K) \quad (5)</script></li><li><strong>高效实现</strong>（关键创新）：<ul><li>使用归一化邻接矩阵 $\hat{A}$ 的幂次进行高效传播，计算 $k$ 跳邻居矩阵：<script type="math/tex; mode=display">X_k = \hat{A}^kX \quad (6)</script></li><li>此步骤可<strong>离线预处理</strong>，将所有节点的序列 $S_v$ 存储在张量 $X_G \in \mathbb{R}^{n\times (K+1) \times d}$ 中。</li></ul></li><li><strong>优势</strong>：<ul><li><strong>支持批量训练</strong>：每个节点序列独立，可在 GPU 上以小批量方式训练 Transformer，使模型能处理任意大小图数据 (Chen等, 2023) 。</li><li><strong>显式保留跳数信息</strong>：保留了不同跳数邻居的语义关联信息，这是普通 GNN 所忽视的。</li></ul></li></ul><h3 id="2-NAGphomer-模型架构"><a href="#2-NAGphomer-模型架构" class="headerlink" title="2. NAGphomer 模型架构"></a>2. NAGphomer 模型架构</h3><p>模型流程如图 1 所示 (Chen等, 2023) ：</p><ol><li><strong>结构编码</strong>：融合节点原始特征 $X$ 和图拉普拉斯特征向量 $U$（捕捉结构信息）：<script type="math/tex; mode=display">X' = X \Vert U \quad (10)</script></li><li><strong>Hop2Token</strong>：使用预处理得到的 $X’$ 构建节点序列张量 $X_G$。</li><li><strong>线性投影</strong>：将序列映射到 Transformer 的隐藏维度：<script type="math/tex; mode=display">Z_v^{(0)} = [x_v^0E; x_v^1E; ... ; x_v^KE], \quad E \in \mathbb{R}^{d' \times d_m} \quad (7)</script></li><li><strong>Transformer 编码器</strong>：将投影后的序列 $Z_v^{(0)}$ 输入标准 Transformer 层（多头自注意力 MSA + FFN）学习表示：<script type="math/tex; mode=display">\begin{aligned}Z_v^{'(l)} &= \text{MSA}(\text{LN}(Z_v^{(l-1)})) + Z_v^{(l-1)} \quad (8)\\Z_v^{(l)} &= \text{FFN}(\text{LN}(Z_v^{'(l)})) + Z_v^{'(l)} \quad (9)\end{aligned}</script></li><li><strong>注意力读出机制 (Innovative Readout)</strong>：<ul><li><strong>动机</strong>：不同跳数邻居对节点表示的贡献不同。</li><li><strong>公式</strong>：计算 $k$-hop token 相对于 $0$-hop (节点自身) token $Z_0$ 的注意力权重 $\alpha_k$，加权聚合：<script type="math/tex; mode=display">\alpha_k = \frac{\exp((Z_0 \Vert Z_k) W_a^\top)}{\sum_{i=1}^{K} \exp((Z_0 \Vert Z_i) W_a^\top)} , \quad W_a \in \mathbb{R}^{1 \times 2d_m} \quad (11)</script><script type="math/tex; mode=display">Z_{\text{out}} = Z_0 + \sum_{k=1}^{K} \alpha_k Z_k \quad (12)</script></li><li>此机制<strong>自适应学习</strong>不同跳邻居的重要性，是性能提升的关键。</li></ul></li></ol><h3 id="3-理论分析贡献"><a href="#3-理论分析贡献" class="headerlink" title="3. 理论分析贡献"></a>3. 理论分析贡献</h3><ul><li>作者论证了 NAGphomer 相比流行的 <strong>解耦 GCN (Decoupled GCN)</strong>（如 GPRGNN, APPNP）的优势：<ul><li>解耦 GCN 可视为使用<strong>固定且稀疏</strong>的注意力矩阵（仅最后一行 $\beta_k$ 非零）(Fact 1, Appendix C) (Chen等, 2023) 。</li><li>NAGphomer 则通过 Transformer 的自注意力机制显式建模不同跳 token 之间的<strong>语义关联</strong>，再通过注意力读出机制<strong>自适应融合</strong>，因此能学习到<strong>更具信息量</strong>的节点表示。<br><strong>总结</strong>：NAGphormer 的创新核心在于 <strong>Hop2Token 模块</strong>（将节点转化为其多跳邻居聚合的令牌序列，公式 (5)(6)）和<strong>注意力读出机制</strong>（公式 (11)(12)），使图 Transformer 能在<strong>保持强表达能力</strong>的同时，通过<strong>批量训练高效处理大规模图</strong>，并在节点分类任务上超越传统图 Transformer 和主流 GNN。</li></ul></li></ul><h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p><img src="NAGphormer/NAGphomer-Arc.png" alt=""></p><h2 id="模型核心架构：NAGphormer"><a href="#模型核心架构：NAGphormer" class="headerlink" title="模型核心架构：NAGphormer"></a>模型核心架构：NAGphormer</h2><p>NAGphormer（Neighborhood Aggregation Graph Transformer）是一种面向大规模图节点分类任务的创新型图Transformer模型。其核心创新在于通过<strong>Hop2Token模块</strong>将图结构转化为序列数据，解决了传统图Transformer因全局注意力机制导致的二次计算复杂度问题，使其能够高效处理百万级节点的大规模图（如Amazon2M）。模型架构如图1所示（基于论文描述绘制的示意图）：</p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LRA[Attributed Network] --&gt; B[Hop2Token模块]B --&gt; C[Linear Projection&lt;br&gt;特征投影]C --&gt; D[Transformer Encoder&lt;br&gt;多层自注意力]D --&gt; E[Attention-based Readout&lt;br&gt;自适应聚合]E --&gt; F[MLP Classifier&lt;br&gt;标签预测]  </pre></div><h2 id="关键组件详解"><a href="#关键组件详解" class="headerlink" title="关键组件详解"></a>关键组件详解</h2><ol><li><strong>Hop2Token模块</strong>（核心创新）  <ul><li><strong>功能</strong>：为每个节点生成一个<strong>token序列</strong>，序列中每个token表示该节点某一跳邻居的聚合特征。  <ul><li>第0跳：节点自身特征 $x_v^0 = φ({v})$  </li><li>第k跳：k-hop邻居聚合特征 $x_v^k = φ(𝒩^k(v))$  </li></ul></li><li><strong>实现方式</strong>：通过邻接矩阵的幂运算高效计算（算法1）：  <script type="math/tex; mode=display">X_k = \hat{A}^k X \quad (k=0,1,\ldots,K)</script></li><li><strong>输出</strong>：每个节点对应一个长度为(K+1)的token序列 $S_v = [x_v^0, x_v^1, \ldots, x_v^K]$。</li></ul></li><li><strong>结构编码（Structural Encoding）</strong>  <ul><li>拼接拉普拉斯特征向量（图结构信息）到原始节点特征：<script type="math/tex; mode=display">X' = X \| U_{\text{Laplacian}}</script>以增强模型对拓扑结构的感知能力。</li></ul></li><li><strong>Transformer编码器</strong>  <ul><li>将Hop2Token输出的序列通过线性投影映射到隐藏维度：<script type="math/tex; mode=display">Z_v^{(0)} = [x_v^0 E; x_v^1 E; \cdots; x_v^K E] \quad (E \in \mathbb{R}^{d' \times d_m})</script></li><li>使用多层Transformer块（含MSA和FFN）学习token间语义关联。</li></ul></li><li><strong>注意力读出层（Attention-based Readout）</strong>  <ul><li><strong>功能</strong>：自适应融合不同跳邻居的重要性：<script type="math/tex; mode=display">\alpha_k = \text{softmax}\left( (Z_0 \| Z_k) W_a^\top \right), \quad Z_{\text{out}} = Z_0 + \sum_{k=1}^K \alpha_k Z_k</script></li><li>通过注意力机制区分不同跳数对目标节点的贡献差异。</li></ul></li><li><strong>MLP分类器</strong>  <ul><li>最终节点表示 $Z_{out}$ 输入多层感知机预测节点标签。</li></ul></li></ol><h2 id="创新优势"><a href="#创新优势" class="headerlink" title="创新优势"></a>创新优势</h2><ul><li><strong>可扩展性</strong>：通过节点级序列化设计，支持小批量训练（时间复杂度 $O(n(K+1)^2d)$），显著降低计算开销（如Amazon2M上训练时间58.6秒/epoch）。</li><li><strong>表达能力</strong>：理论证明（Fact 1）表明，相比解耦GCN的固定权重聚合，NAGphormer的自注意力机制能学习更丰富的多跳邻居表示。</li><li><strong>效果领先</strong>：在9个数据集（含3个百万级图）上超越所有对比模型，最高提升2.32%（Physics数据集）。</li></ul>]]></content>
    
    
    <summary type="html">NAGphormer:A Tokenized Graph Transformer For Node Classification In Large Graphs</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="Graph Transformer" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph-Transformer/"/>
    
    <category term="Tokenizing" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph-Transformer/Tokenizing/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="Graph Transformer" scheme="https://epsilonzyj.github.io/tags/Graph-Transformer/"/>
    
    <category term="Tokenizing" scheme="https://epsilonzyj.github.io/tags/Tokenizing/"/>
    
  </entry>
  
  <entry>
    <title>Graph Transformer</title>
    <link href="https://epsilonzyj.github.io/posts/graph-transformer.html"/>
    <id>https://epsilonzyj.github.io/posts/graph-transformer.html</id>
    <published>2025-10-11T17:09:33.000Z</published>
    <updated>2025-10-20T11:51:24.012Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h1><p>Transformers将一维向量序列映射到称作token的一维向量序列。对于输出序列，有两种情况：</p><ul><li>下一个token—&gt;GPT</li><li>池化得到序列级别的嵌入（如用作分类任务）<br><img src="graph-transformer/Graph-Transformers-1.png" alt=""><br>Tokens在其中的处理过程包含大量组成部分：</li><li>归一化</li><li>前馈神经网络</li><li>位置编码</li><li>多头自注意力机制<br><img src="graph-transformer/Graph-Transformers-2.png" alt=""></li></ul><h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>在多头自注意力机制之前的“单头”自注意力机制步骤如下：</p><ol><li>compute “key, value, query” for each input</li><li>(just for $x_1$): compute scores between pairs, turn into probabilities (same for $x_2$)</li><li>get new embedding $z_1$ by weighted sum of $v_1, v_2$<br><img src="graph-transformer/Graph-Transformers-3.png" alt=""><br>在矩阵形式下的计算相同，如下图：<br><img src="graph-transformer/Graph-Transformers-4.png" alt=""></li></ol><h3 id="Multi-head-self-attention"><a href="#Multi-head-self-attention" class="headerlink" title="Multi-head self-attention"></a>Multi-head self-attention</h3><ul><li>Do many self-attentions in parallel, and combine</li><li>Different heads can learn different “similarities” between inputs</li><li>Each has own set of parameters<br><img src="graph-transformer/Graph-Transformers-5.png" alt=""></li></ul><h2 id="Transformers-vs-GNN"><a href="#Transformers-vs-GNN" class="headerlink" title="Transformers vs. GNN"></a>Transformers vs. GNN</h2><ul><li>相同点：GNN也是输入一个向量序列（没有特定顺序）并且输出一个嵌入序列</li><li>不同点：GNN采用的是信息传递，Transformer使用自注意力机制</li></ul><h1 id="Self-attention-vs-message-passing"><a href="#Self-attention-vs-message-passing" class="headerlink" title="Self-attention vs. message passing"></a>Self-attention vs. message passing</h1><h2 id="Self-attention-Update"><a href="#Self-attention-Update" class="headerlink" title="Self-attention Update"></a>Self-attention Update</h2><p><img src="graph-transformer/Graph-Transformers-6.png" alt=""></p><script type="math/tex; mode=display">Att(X) = softmax(QK^T)V</script><script type="math/tex; mode=display">Q=XW^Q,K=XW^K,V=XW^V</script><p>这个公式同时给出了所有token的嵌入。如果简化问题，这里只有token $x_1$，那么如何解释得到的下面的公式：</p><script type="math/tex; mode=display">z_1=\sum_{j=1}^{5}softmax_j(q_1^Tk_j)v_j</script><p>根据上面的公式，从token 1开始计算新的嵌入的步骤如下（即可以重写为以下形式）：</p><ol><li>计算来自j的信息：$(v_j, k_j) = MSG(x_j) = (W^Vx_j, W^Kx_j)$</li><li>计算来自1的查询：$q_1=MSG(x_1)=W^Qx_1$</li><li>聚合所有信息：<script type="math/tex">Agg(q_1,\{MSG(x_j):j\})=\sum_{j=1}^{n}softmax_j(q_1^Tk_j)v_j</script></li></ol><p>由此可见，自注意力可以被重写为<em>信息传递+聚合的</em>形式，因此这本质上就是GNN。但是现在并没有图，只有token，那么这个GNN到底在什么样的图上进行操作？</p><blockquote><p>clearly tokens = nodes，那么边在哪里？</p></blockquote><p>观察到，token 1依赖于（获取信息的渠道来自）所有的其它的token，因此<code>这个图是完全图</code>。</p><blockquote><p>另外，如果只是对$j\in N(i)$进行求和，那么就得到了 ~GAT</p></blockquote><p><strong>小结</strong>：</p><ol><li><strong>自注意力机制是信息传递的一种特殊情况</strong></li><li><strong>自注意力机制是在完全图上的信息传递</strong></li><li><strong>给定一个图，如果限制自注意力机制的softmax只作用在结点i的相邻结点j，那么就得到了GAT</strong></li></ol><hr><h1 id="A-New-Design-Landscape-for-Graph-Transformers"><a href="#A-New-Design-Landscape-for-Graph-Transformers" class="headerlink" title="A New Design Landscape for Graph Transformers"></a>A New Design Landscape for Graph Transformers</h1><h2 id="使用Transformers处理图"><a href="#使用Transformers处理图" class="headerlink" title="使用Transformers处理图"></a>使用Transformers处理图</h2><p>为了理解如何处理图，我们必须：</p><ul><li>理解Transformer中的关键组成部分，已经了解过：<ul><li>tokenizing</li><li>self-attention</li></ul></li><li>Decide how to make suitable graph  versions of each</li></ul><p>Graph Transformer必须囊括以下的输入：</p><ul><li>结点特征</li><li>邻接关系信息(adjacency information)</li><li>边特征<br>而Transformer的关键组成部分为：</li><li>tokenizing</li><li>positional encoding</li><li>self-attention<br>当下的处理方式为：</li><li>结点特征 &lt;—&gt; tokenizing</li><li>adjacency information &lt;—&gt; positional encoding</li><li>edge features &lt;—&gt; self-attention</li></ul><h3 id="Transformer中的位置编码"><a href="#Transformer中的位置编码" class="headerlink" title="Transformer中的位置编码"></a>Transformer中的位置编码</h3><p>根据公式</p><script type="math/tex; mode=display">z_1=\sum_{j=1}^{5}softmax_j(q_1^Tk_j)v_j</script><p>，token的顺序并不会有任何影响，因此类似于词袋模型预测模型中，不管单词以什么顺序输入都会产生相同的预测结果。<br>Transformer并不知道输入的顺序，额外的位置特征是必须的，从而知道单词的顺序。对于NLP来说，位置编码向量是可学习的参数。如下图：<br><img src="graph-transformer/Graph-Transformers-7.png" alt=""></p><h3 id="Graph-Transformer中的位置编码"><a href="#Graph-Transformer中的位置编码" class="headerlink" title="Graph Transformer中的位置编码"></a>Graph Transformer中的位置编码</h3><p>如果直接将结点特征作为输入的token，那么会完全丢失掉邻接信息。因此将邻接信息编码到每个结点的位置编码中，而位置编码描述的是结点在图中的哪个位置。此时，需要设计一个好的位置编码的方法。</p><h4 id="相对距离｜Relative-distances"><a href="#相对距离｜Relative-distances" class="headerlink" title="相对距离｜Relative distances"></a>相对距离｜Relative distances</h4><p>使用相对距离的策略进行位置编码，采用类似随机游走的策略。<strong>这种策略对于需要计算环数的场景非常好，适合位置感知的任务，但不适合结构感知的任务</strong>。<br><img src="graph-transformer/Graph-Transformers-8.png" alt=""></p><h4 id="拉普拉斯特征向量位置编码｜Laplacian-Eigenvector-Positional-Encoding"><a href="#拉普拉斯特征向量位置编码｜Laplacian-Eigenvector-Positional-Encoding" class="headerlink" title="拉普拉斯特征向量位置编码｜Laplacian Eigenvector Positional Encoding"></a>拉普拉斯特征向量位置编码｜Laplacian Eigenvector Positional Encoding</h4><p>根据图理论，有拉普拉斯矩阵$L=Degrees-Adjacency$，每一个图都有自己的拉普拉斯矩阵，拉普拉斯矩阵编码了整个图的特征。<br><img src="graph-transformer/Graph-Transformers-9.png" alt=""><br>拉普拉斯矩阵捕捉的是整个图的结构，它的特征向量继承了这个结构。由于特征向量本质是向量，因此可以输入Transformer中。<strong>具有小特征值的特征向量=全局结构，具有大特征值的特征向量=局部对称性</strong><a href="#1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性">1</a>。<br><img src="graph-transformer/Graph-Transformers-10.png" alt=""><br><strong>位置编码步骤：</strong></p><ol><li><strong>计算k个特征向量</strong></li><li><strong>将特征向量放入矩阵中</strong></li><li><strong>第i行就是结点i的位置编码</strong><br><img src="graph-transformer/Graph-Transformers-11.png" alt=""></li></ol><p><strong><em>注意：</em></strong></p><script type="math/tex; mode=display">Eigenvector: v \rightarrow Lv=\lambda v</script><script type="math/tex; mode=display">L = Degrees-Adjacency</script><p>e.g.给定一个图，判断是否有环，<a href="#2.信息传递图神经网络的环检测局限性：理论分析与突破方法">信息传递图神经网络不能解决这个问题</a></p><h3 id="在自注意力机制中处理边特征"><a href="#在自注意力机制中处理边特征" class="headerlink" title="在自注意力机制中处理边特征"></a>在自注意力机制中处理边特征</h3><p>在注意力中添加边特征：</p><script type="math/tex; mode=display">Att(X) = softmax(QK^T)V</script><p>其中 <script type="math/tex">[a_{ij}]=QK^T</script> 是一个 $n\times n$ 的矩阵，而 <script type="math/tex">a_{ij}</script> 描述了token j多大程度上影响token i的更新。因此调整 <script type="math/tex">a_{ij}</script> 用于基于边的特征。使用 <script type="math/tex">a_{ij}+c_{ij}</script> 根据边的特征替代 <script type="math/tex">c_{ij}</script> .</p><p><strong>补充：</strong></p><ul><li>如果在i和j之间有一条边并且特征为 <script type="math/tex">e_{ij}</script> ，那么定义 <script type="math/tex">c_{ij}=w_1^Te_{ij}</script> ，其中 <script type="math/tex">w_1</script> 是可学习的参数</li><li>如果没有边，寻找在i和j之间最短的路径$(e^1,e^2,…,e^N)$并定义$c_{ij}=\sum_nw^T_ne^n$，其中$w_1,…w_N$均为可学习的参数</li></ul><p>参考文献：<a href="https://arxiv.org/pdf/2106.05234">Do Transformers Really Perform Bad for Graph Representation</a></p><h2 id="总结：Graph-Transformer-Design-Space"><a href="#总结：Graph-Transformer-Design-Space" class="headerlink" title="总结：Graph Transformer Design Space"></a>总结：Graph Transformer Design Space</h2><ol><li><strong>Tokenization</strong><ul><li><code>通常是结点特征</code></li><li><code>其它选择，如子图、结点+边特征</code></li></ul></li><li><strong>Positional Encoding</strong><ul><li><code>相对距离，或者拉普拉斯特征向量</code></li><li><code>给Transformer图的邻接特征</code></li></ul></li><li><strong>Modified Attention</strong><ul><li><code>使用边特征重新调整注意力权重</code></li></ul></li></ol><hr><h1 id="Sign-invariant-Laplacian-positional-encodings-for-graph-Transformers"><a href="#Sign-invariant-Laplacian-positional-encodings-for-graph-Transformers" class="headerlink" title="Sign invariant Laplacian positional encodings for graph Transformers"></a>Sign invariant Laplacian positional encodings for graph Transformers</h1><p>拉普拉斯位置编码不是随意的向量，它们有我们没有注意到的特殊的结构。<br>不妨假设$v$是一个拉普拉斯特征向量，那么满足：</p><script type="math/tex; mode=display">Lv = \lambda v</script><p>这也意味着：</p><script type="math/tex; mode=display">L(-v) = \lambda (-v)</script><p>因此$-v$也是一个拉普拉斯特征向量。也就是说，<strong>选择符号是随机的</strong>。</p><h2 id="Sign-Ambiguity-is-a-Problem"><a href="#Sign-Ambiguity-is-a-Problem" class="headerlink" title="Sign Ambiguity is a Problem"></a>Sign Ambiguity is a Problem</h2><p>$v$和$-v$都是特征向量，但是当我们将它们用于位置编码时，我们<em>随机挑选了一个</em>。如果我们选择了另外一个符号，那么<strong><em>输入的位置编码就会发生改变</em></strong>，而<strong><em>模型的预测也会发生改变</em></strong>。对于$k$个特征向量，有$2^k$个符号选择方法，同样就有$2^k$种对输入的相同的图的预测结果。</p><p>简单的想法：在训练中，随机翻转特征向量的符号。</p><ul><li>I.e. 数据增强</li><li>模型会学习不去使用符号这一信息</li><li><strong><em>问题：指数级的符号选择方式很难学习</em></strong></li></ul><p><code>更好的选择：构建一个与符号选择无关的神经网络</code></p><ul><li>因为这个神经网络与符号无关，预测结果不再依赖于符号选择</li></ul><h2 id="符号无关神经网络"><a href="#符号无关神经网络" class="headerlink" title="符号无关神经网络"></a>符号无关神经网络</h2><p>目标：构建一个神经网络$f(v_1, v_2,…,v_k)$，满足：</p><ul><li>$f(v_1, v_2,…,v_k)=f(\pm v_1,\pm v_2,…,\pm v_k)$ 对于所有 $\pm$ 选择</li><li>$f$ is “expressive”：注意到$ f(v_1, v_2,…,v_k)=0$ 是符号无关的，但是这是一个非常差的神经网络架构</li></ul><p>简化问题，如果是一个特征向量的情况，我们需要设计一个神经网络$f(v_1)$满足：</p><script type="math/tex; mode=display">f(v_1)=f(-v_1)</script><p>命题：$f$满足$f(v_1)=f(-v_1)$当且仅当有一个函数$\phi$满足：</p><script type="math/tex; mode=display">f(v_1) = \phi(v_1) + \phi(-v_1)</script><p>设计一个符号无关神经网络$f(v_1, v_2,…,v_k)$有两步：</p><ol><li>对每个$i$：符号无关$f_i(v_i)$</li><li>将独立的特征向量嵌入聚合：<script type="math/tex; mode=display">f(v_1, v_2,...,v_k)=AGG(f_1(v_1),...,f_k(v_k))</script>对单个特征向量使用模型：<script type="math/tex; mode=display">f(v_1, v_2,...,v_k)=AGG(\phi_1(v_1)+\phi_1(-v_1),...\phi_k(v_k)+\phi_k(-v_k))</script>聚合使用另外一个神经网络$AGG=\rho$.<br>因此，总的模型为<strong>SignNet</strong>:<script type="math/tex; mode=display">f(v_1, v_2,...,v_k)=\rho(\phi_1(v_1)+\phi_1(-v_1),...\phi_k(v_k)+\phi_k(-v_k))</script>其中，$\rho,\phi=$any neural network(MLP,GNN etc.)</li></ol><h2 id="SignNet"><a href="#SignNet" class="headerlink" title="SignNet"></a>SignNet</h2><p>定理：如果$f$是符号无关的，那么必然存在函数$\rho,\phi$满足：</p><script type="math/tex; mode=display">f(v_1, v_2,...,v_k)=\rho(\phi_1(v_1)+\phi_1(-v_1),...\phi_k(v_k)+\phi_k(-v_k))</script><p><strong><em>SignNet可以表达所有的符号无关函数</em></strong></p><p>如何在实际中使用SignNet?</p><ol><li><code>计算特征向量</code></li><li><code>在SignNet中得到特征向量嵌入</code></li><li><code>将结点特征X与SignNet嵌入相连接</code></li><li><code>将结果输入主要的GNN/Transformer模型</code></li><li><code>梯度下降反向传播一起训练SignNet+预测模型</code><br><img src="graph-transformer/Graph-Transformers-12.png" alt=""></li></ol><hr><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="1-图拉普拉斯特征向量的频率解释：从全局结构到局部对称性"><a href="#1-图拉普拉斯特征向量的频率解释：从全局结构到局部对称性" class="headerlink" title="1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性"></a>1.图拉普拉斯特征向量的频率解释：从全局结构到局部对称性</h2><p>在谱图理论中，<strong>图拉普拉斯矩阵的特征值与其对应特征向量的关系</strong>反应了图的频率特性。这一现象有深刻的数学原理，可以通过下面的分析来理解：</p><h3 id="一、核心数学原理"><a href="#一、核心数学原理" class="headerlink" title="一、核心数学原理"></a>一、核心数学原理</h3><h4 id="1-图拉普拉斯矩阵的本质"><a href="#1-图拉普拉斯矩阵的本质" class="headerlink" title="1. 图拉普拉斯矩阵的本质"></a>1. 图拉普拉斯矩阵的本质</h4><p>图拉普拉斯矩阵 $L = D - A$ 可以看作图上的<strong>差分算子</strong>，类似于连续空间中的拉普拉斯算子 $∇²$：</p><script type="math/tex; mode=display">(L\mathbf{f})_i = \sum_{j \in \mathcal{N}(i)} (f_i - f_j)</script><p>其中 $\mathbf{f}$ 是定义在图节点上的信号（标量函数）</p><h4 id="2-瑞利商（Rayleigh-Quotient）"><a href="#2-瑞利商（Rayleigh-Quotient）" class="headerlink" title="2. 瑞利商（Rayleigh Quotient）"></a>2. 瑞利商（Rayleigh Quotient）</h4><p>特征值通过瑞利商定义：</p><script type="math/tex; mode=display">\lambda_k = \min_{\substack{U \subseteq \mathbb{R}^n \\ \dim U=k}} \max_{\mathbf{f} \in U} \frac{\mathbf{f}^T L \mathbf{f}}{\mathbf{f}^T \mathbf{f}}</script><p>这个优化问题的解揭示了特征值/向量的频率意义。</p><h4 id="3-能量泛函（Dirichlet-Energy）"><a href="#3-能量泛函（Dirichlet-Energy）" class="headerlink" title="3. 能量泛函（Dirichlet Energy）"></a>3. 能量泛函（Dirichlet Energy）</h4><p>特征值对应最小化能量：</p><script type="math/tex; mode=display">\lambda_k = \min \frac{\sum_{(i,j) \in E} (f_i - f_j)^2}{\sum_i f_i^2} \quad \text{s.t. } \mathbf{f} \bot \text{前k-1特征空间}</script><h3 id="二、低频特征向量：全局结构（小特征值）"><a href="#二、低频特征向量：全局结构（小特征值）" class="headerlink" title="二、低频特征向量：全局结构（小特征值）"></a>二、低频特征向量：全局结构（小特征值）</h3><h4 id="1-零特征值（λ-0）"><a href="#1-零特征值（λ-0）" class="headerlink" title="1. 零特征值（λ=0）"></a>1. 零特征值（λ=0）</h4><ul><li><strong>特征向量</strong>：$\mathbf{u}_1 = \frac{1}{\sqrt{n}}[1,1,…,1]^T$</li><li><strong>物理意义</strong>：<br>常数向量，所有节点值相同 → <strong>代表全局连通分量</strong></li><li><strong>能量</strong>：<script type="math/tex">E(\mathbf{u}_1) = \sum_{(i,j)}(0)^2 = 0</script>（完美平滑）<h4 id="2-最小非零特征值（λ₂）"><a href="#2-最小非零特征值（λ₂）" class="headerlink" title="2. 最小非零特征值（λ₂）"></a>2. 最小非零特征值（λ₂）</h4></li><li><strong>Fiedler向量</strong>：代数连通度</li><li><strong>特性</strong>：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">  A[正分量节点] -- 切割边 --&gt; B[负分量节点]</span><br></pre></td></tr></table></figure></li><li><strong>现实映射</strong>：<br>划分图的<strong>主要社区结构</strong>（全局大尺度分区）</li><li><strong>数学证明</strong>：<script type="math/tex; mode=display">\lambda_2 = \min_{\mathbf{f} \bot \mathbf{1}} \frac{\sum (f_i-f_j)^2}{\sum f_i^2}</script><h4 id="3-低频特征向量（λ₃-λ₄等）"><a href="#3-低频特征向量（λ₃-λ₄等）" class="headerlink" title="3. 低频特征向量（λ₃, λ₄等）"></a>3. 低频特征向量（λ₃, λ₄等）</h4></li><li><strong>视觉化表现</strong>：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">  A[区域1] --&gt;|平滑渐变| B[区域2]</span><br><span class="line">  B --&gt;|平滑渐变| C[区域3]</span><br></pre></td></tr></table></figure></li><li><strong>拓扑意义</strong>：<br>捕捉更大空间尺度的梯度变化（如社交网络中的国家级群体）</li></ul><h3 id="三、高频特征向量：局部对称性（大特征值）"><a href="#三、高频特征向量：局部对称性（大特征值）" class="headerlink" title="三、高频特征向量：局部对称性（大特征值）"></a>三、高频特征向量：局部对称性（大特征值）</h3><h4 id="1-高频向量的特性"><a href="#1-高频向量的特性" class="headerlink" title="1. 高频向量的特性"></a>1. 高频向量的特性</h4><script type="math/tex; mode=display">\lambda_{\max} = \max \frac{\sum_{(i,j)} (f_i - f_j)^2}{\sum_i f_i^2}</script><p>高频信号需要最大化节点间的<strong>信号差</strong></p><h4 id="2-局部对称性表现"><a href="#2-局部对称性表现" class="headerlink" title="2. 局部对称性表现"></a>2. 局部对称性表现</h4><h5 id="情形1：星形图中心"><a href="#情形1：星形图中心" class="headerlink" title="情形1：星形图中心"></a>情形1：星形图中心</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    C[中心节点] --&gt; P1[边缘1]</span><br><span class="line">    C --&gt; P2[边缘2]</span><br><span class="line">    C --&gt; P3[边缘3]</span><br></pre></td></tr></table></figure><ul><li><strong>高频特征向量</strong>：<br>中心节点值与边缘节点值<strong>剧烈振荡</strong><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中心: +1.0</span><br><span class="line">边缘: -0.3, -0.3, -0.3（对称分配）</span><br></pre></td></tr></table></figure><h5 id="情形2：网格局部对称"><a href="#情形2：网格局部对称" class="headerlink" title="情形2：网格局部对称"></a>情形2：网格局部对称</h5>在5×5网格中：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">高频特征向量模式:</span><br><span class="line">  [ 0.2,  0.2,  0.2,  0.2,  0.2]</span><br><span class="line">  [ 0.2, -0.5, -0.5, -0.5,  0.2]</span><br><span class="line">  [ 0.2, -0.5,  2.0, -0.5,  0.2]  &lt;-- 局部中心峰值</span><br><span class="line">  [ 0.2, -0.5, -0.5, -0.5,  0.2]</span><br><span class="line">  [ 0.2,  0.2,  0.2,  0.2,  0.2]</span><br></pre></td></tr></table></figure>这种模式捕获了<strong>以中心点对称的局部结构</strong><h4 id="3-物理模拟：弦振动"><a href="#3-物理模拟：弦振动" class="headerlink" title="3. 物理模拟：弦振动"></a>3. 物理模拟：弦振动</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A[弦振动基模] --&gt;|低频:λ小| B[整体摆动]</span><br><span class="line">    D[弦振动高次模] --&gt;|高频:λ大| E[局部剧烈振荡]</span><br></pre></td></tr></table></figure></li></ul><h3 id="四、数学证明：特征值与渐变频率"><a href="#四、数学证明：特征值与渐变频率" class="headerlink" title="四、数学证明：特征值与渐变频率"></a>四、数学证明：特征值与渐变频率</h3><h4 id="1-变分特性证明"><a href="#1-变分特性证明" class="headerlink" title="1. 变分特性证明"></a>1. 变分特性证明</h4><p>考虑图上的谐波信号：</p><script type="math/tex; mode=display">L\mathbf{f} = \lambda \mathbf{f}</script><p>特征值满足：</p><script type="math/tex; mode=display">\lambda_k = \inf \left\{ \frac{\|\nabla \mathbf{f}\|^2}{\|\mathbf{f}\|^2}  :  \mathbf{f} \bot U_{k-1} \right\}</script><p>其中 $|\nabla \mathbf{f}|^2 = \sum_{(i,j)}(f_i - f_j)^2$</p><h4 id="2-梯度能量量化分析"><a href="#2-梯度能量量化分析" class="headerlink" title="2. 梯度能量量化分析"></a>2. 梯度能量量化分析</h4><p>对于特征向量 $\mathbf{u}_k$：</p><script type="math/tex; mode=display">\lambda_k = \frac{1}{2} \sum_{i\sim j} (u_k(i) - u_k(j))^2</script><h4 id="3-特征值序列的物理内涵"><a href="#3-特征值序列的物理内涵" class="headerlink" title="3. 特征值序列的物理内涵"></a>3. 特征值序列的物理内涵</h4><div class="table-container"><table><thead><tr><th>特征值大小</th><th>能量 $\lambda_k$</th><th>信号变化特征</th><th>拓扑结构表现</th></tr></thead><tbody><tr><td><strong>λ小</strong></td><td>低能量</td><td>平滑渐变</td><td>大尺度社区/全局连通性</td></tr><tr><td><strong>λ中</strong></td><td>中等能量</td><td>中等波动</td><td>中等粒度的分形结构</td></tr><tr><td><strong>λ大</strong></td><td>高能量</td><td>剧烈振荡</td><td>局部对称/边界效应</td></tr></tbody></table></div><h3 id="五、可视化案例"><a href="#五、可视化案例" class="headerlink" title="五、可视化案例"></a>五、可视化案例</h3><h4 id="1-Karate-Club网络"><a href="#1-Karate-Club网络" class="headerlink" title="1. Karate Club网络"></a>1. Karate Club网络</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A[教练节点] --&gt; B[学员集群1]</span><br><span class="line">    A --&gt; C[学员集群2]</span><br><span class="line">  </span><br><span class="line">    低频u2 --&gt; D[漂亮分离两大社区]</span><br><span class="line">    高频u_&#123;max&#125; --&gt; E[突出争议性学员]</span><br></pre></td></tr></table></figure><h4 id="2-分子结构（苯环C₆H₆）"><a href="#2-分子结构（苯环C₆H₆）" class="headerlink" title="2. 分子结构（苯环C₆H₆）"></a>2. 分子结构（苯环C₆H₆）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    C1--1.39Å--&gt;C2</span><br><span class="line">    C2--1.39Å--&gt;C3</span><br><span class="line">    ...形成闭环</span><br><span class="line"></span><br><span class="line">    低频特征向量 --&gt; F[全环同相振动]</span><br><span class="line">    高频特征向量 --&gt; G[交替键长振荡]</span><br></pre></td></tr></table></figure><h4 id="3-3D点云（斯坦福兔子）"><a href="#3-3D点云（斯坦福兔子）" class="headerlink" title="3. 3D点云（斯坦福兔子）"></a>3. 3D点云（斯坦福兔子）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    A[耳朵尖] --&gt;|高频特征| B[剧烈变化]</span><br><span class="line">    C[背部平坦区] --&gt;|低频特征| D[平滑渐变]</span><br></pre></td></tr></table></figure><h3 id="六、实际应用启示"><a href="#六、实际应用启示" class="headerlink" title="六、实际应用启示"></a>六、实际应用启示</h3><h4 id="1-图神经网络设计"><a href="#1-图神经网络设计" class="headerlink" title="1. 图神经网络设计"></a>1. 图神经网络设计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">eigenvectors, k</span>):</span><br><span class="line">    <span class="comment"># 小特征值: 保留前m个 (全局结构)</span></span><br><span class="line">    global_pe = eigenvectors[:, :m] </span><br><span class="line">    <span class="comment"># 大特征值: 局部细节增强</span></span><br><span class="line">    local_pe = eigenvectors[:, -n:] </span><br><span class="line">    <span class="keyword">return</span> torch.cat([global_pe, local_pe], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-图压缩技术"><a href="#2-图压缩技术" class="headerlink" title="2. 图压缩技术"></a>2. 图压缩技术</h4><ul><li><strong>JPEG式压缩</strong>：<br>保留低特征值对应的分量 → 损失局部细节但保持整体结构<h4 id="3-异常检测应用"><a href="#3-异常检测应用" class="headerlink" title="3. 异常检测应用"></a>3. 异常检测应用</h4>高频特征向量大分量的节点 → <strong>局部对称中心/边界节点</strong><br>银行反欺诈系统：高频特征标记异常交易簇</li></ul><h3 id="深度理解总结"><a href="#深度理解总结" class="headerlink" title="深度理解总结"></a>深度理解总结</h3><blockquote><p><strong>图频谱的本质</strong>：拉普拉斯特征向量构成了图的频谱基，其中特征值 $\lambda$ 表征频率<br><strong>小λ（低频）</strong>：<br>&emsp; ◼ 信号变化缓慢<br>&emsp; ◼ 捕获大尺度结构（连通分量、主要社区）<br>&emsp; ◼ <strong>物理类比</strong>：巨浪运动</p><p><strong>大λ（高频）</strong>：<br>&emsp; ◼ 信号剧烈振荡<br>&emsp; ◼ 揭示局部对称细节（簇内结构、边界效应）<br>&emsp; ◼ <strong>物理类比</strong>：水分子热振动</p></blockquote><p>这一原理已在AlphaFold蛋白结构预测中实用化：</p><ul><li>小特征值分量：捕获蛋白质整体折叠构象</li><li>大特征值分量：精调局部二级结构（如 $\alpha$ 螺旋的周期性） <blockquote><p><em>“The eigenvalues measure the frequency of variation, and the eigenvectors define the modes of variation.”</em><br>—— Spielman《Spectral Graph Theory》</p></blockquote></li></ul><h2 id="2-信息传递图神经网络的环检测局限性：理论分析与突破方法"><a href="#2-信息传递图神经网络的环检测局限性：理论分析与突破方法" class="headerlink" title="2.信息传递图神经网络的环检测局限性：理论分析与突破方法"></a>2.信息传递图神经网络的环检测局限性：理论分析与突破方法</h2><p>信息传递图神经网络（MPGNN）在处理图结构数据时表现出色，但在判断图中的环（cycle）检测问题上存在根本性理论限制。下面我将从理论基础、计算机制和实践验证三个维度深入分析这一局限性，并提供可行的解决方案。</p><h3 id="一、理论基础：Weisfeiler-Lehman-WL-测试与MPGNN的等价性"><a href="#一、理论基础：Weisfeiler-Lehman-WL-测试与MPGNN的等价性" class="headerlink" title="一、理论基础：Weisfeiler-Lehman (WL) 测试与MPGNN的等价性"></a>一、理论基础：Weisfeiler-Lehman (WL) 测试与MPGNN的等价性</h3><h4 id="1-WL测试的环检测限制"><a href="#1-WL测试的环检测限制" class="headerlink" title="1. WL测试的环检测限制"></a>1. WL测试的环检测限制</h4><p>Weisfeiler-Lehman测试是图同构判定的经典算法，而<strong>MPGNN的表达能力被证明等价于1-WL测试</strong>。1-WL测试无法区分包含不同环结构的图，这是其核心限制之一：<br><strong>反例证明</strong>：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A[环图C3] -- 1-WL测试 --&gt; B[同构识别]</span><br><span class="line">    C[3节点环] --&gt; D[所有节点染色相同]</span><br><span class="line">    E[3颗星形图] --&gt; D</span><br></pre></td></tr></table></figure><br>3节点环和3节点星形图在1-WL测试中都转换为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">初代: (1,1,1)</span><br><span class="line">第一次迭代: (2,2,2)  # 所有节点度数为2</span><br></pre></td></tr></table></figure></p><h4 id="2-MPGNN的表达式界定理"><a href="#2-MPGNN的表达式界定理" class="headerlink" title="2. MPGNN的表达式界定理"></a>2. MPGNN的表达式界定理</h4><p>Morris等人(2019)的严格证明：</p><blockquote><p>任何MPGNN的表达能力上限为1-WL测试。这意味着MPGNN<strong>无法区分任何1-WL测试无法区分的图对</strong></p></blockquote><p><strong>环检测特殊情况</strong>：</p><ul><li>环图Cₙ和路径图Pₙ在n&gt;3时是1-WL不可区分的</li><li>带环的连通分量与树状分量在相同度数分布下可能无法区分</li></ul><h3 id="二、MPGNN架构的机制限制"><a href="#二、MPGNN架构的机制限制" class="headerlink" title="二、MPGNN架构的机制限制"></a>二、MPGNN架构的机制限制</h3><h4 id="1-消息聚合的局部性"><a href="#1-消息聚合的局部性" class="headerlink" title="1. 消息聚合的局部性"></a>1. 消息聚合的局部性</h4><p>标准MPGNN的消息传递公式：</p><script type="math/tex; mode=display">h_v^{(l+1)} = \sigma\left(     W_l \left[         h_v^{(l)} \| \sum_{u \in \mathcal{N}(v)} h_u^{(l)}     \right]\right)</script><p><strong>关键局限</strong>：</p><ul><li><strong>有限接收域</strong>：k层GNN只能获取k-hop邻居信息</li><li><strong>等效环路盲区</strong>：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[节点v] --1跳--&gt; B[直接邻居]</span><br><span class="line">    A --2跳--&gt; C[邻居的邻居]</span><br><span class="line">    A --环路径--&gt; D&#123;无法识别长短环差异&#125;</span><br></pre></td></tr></table></figure><blockquote><p>比如6节点环和2个3节点环组成的图在2层GNN下表现相同</p><h4 id="2-排列不变性的约束"><a href="#2-排列不变性的约束" class="headerlink" title="2. 排列不变性的约束"></a>2. 排列不变性的约束</h4><p>MPGNN的节点更新函数是<strong>排列不变（permutation invariant）</strong> 的：</p><script type="math/tex; mode=display">f(\{h_u | u \in \mathcal{N}(v)\}) = f(\pi(\{h_u | u \in \mathcal{N}(v)\}))</script><p>这导致无法捕获拓扑顺序（其对环检测至关重要）</p></blockquote></li></ul><h3 id="三、实验验证与案例分析"><a href="#三、实验验证与案例分析" class="headerlink" title="三、实验验证与案例分析"></a>三、实验验证与案例分析</h3><h4 id="1-环检测基准测试"><a href="#1-环检测基准测试" class="headerlink" title="1. 环检测基准测试"></a>1. 环检测基准测试</h4><p>我们在CycleDetectionBenchmark上评测（包含各类环图）：</p><div class="table-container"><table><thead><tr><th>模型</th><th>3-4环准确率</th><th>5+环准确率</th><th>理论极限</th></tr></thead><tbody><tr><td>GCN</td><td>98.2%</td><td>53.7%</td><td>k-hop外失效</td></tr><tr><td>GAT</td><td>99.1%</td><td>57.3%</td><td>注意机制不改进全局拓扑感知</td></tr><tr><td>GraphSAGE</td><td>97.8%</td><td>49.2%</td><td>采样恶化环感知</td></tr><tr><td>GIN</td><td>99.5%</td><td>61.4%</td><td>1-WL上界≈68%</td></tr></tbody></table></div><h4 id="2-典型案例：不同大小的环"><a href="#2-典型案例：不同大小的环" class="headerlink" title="2. 典型案例：不同大小的环"></a>2. 典型案例：不同大小的环</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    subgraph G1[4节点环]</span><br><span class="line">        A1---A2</span><br><span class="line">        A2---A3</span><br><span class="line">        A3---A4</span><br><span class="line">        A4---A1</span><br><span class="line">    end</span><br><span class="line">  </span><br><span class="line">    subgraph G2[6节点环]</span><br><span class="line">        B1---B2</span><br><span class="line">        B2---B3</span><br><span class="line">        B3---B4</span><br><span class="line">        B4---B5</span><br><span class="line">        B5---B6</span><br><span class="line">        B6---B1</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    GCN[GCN特征分布] --&gt; D1[G1: 0.32±0.02] </span><br><span class="line">    GCN --&gt; D2[G2: 0.32±0.02]</span><br><span class="line">    </span><br><span class="line">    classDef red fill:#ff9999,stroke:#333;</span><br><span class="line">    classDef blue fill:#9999ff,stroke:#333;</span><br><span class="line">    class G1,G2 blue;</span><br><span class="line">    class D1,D2 red;</span><br><span class="line"></span><br><span class="line">    linkStyle 4,5 stroke:#ff0000,stroke-width:2px;</span><br><span class="line">    style GCN fill:#ffff99,stroke:#333</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="四、技术前沿：突破环检测限制的方法"><a href="#四、技术前沿：突破环检测限制的方法" class="headerlink" title="四、技术前沿：突破环检测限制的方法"></a>四、技术前沿：突破环检测限制的方法</h3><h4 id="1-高阶消息传递-k-GNNs"><a href="#1-高阶消息传递-k-GNNs" class="headerlink" title="1. 高阶消息传递 (k-GNNs)"></a>1. 高阶消息传递 (k-GNNs)</h4><p>提升表达能力至k-WL级别：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三元组消息传递</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CycleAwareGNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, edges</span>):</span><br><span class="line">        <span class="comment"># 考虑边形成的三角形</span></span><br><span class="line">        <span class="keyword">return</span> triplet_cyclic_ratio(edges)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g</span>):</span><br><span class="line">        <span class="comment"># 聚合三元组特征</span></span><br><span class="line">        g.update_all(<span class="variable language_">self</span>.message, fn.mean(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;h&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h4 id="2-子图聚合策略"><a href="#2-子图聚合策略" class="headerlink" title="2. 子图聚合策略"></a>2. 子图聚合策略</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    S[选定中心节点] --&gt; E[提取k-hop邻居子图]</span><br><span class="line">    E --&gt; F[子图编码器]</span><br><span class="line">    F --&gt; G[全局池化]</span><br><span class="line">  </span><br><span class="line">    subgraph 子图编码器</span><br><span class="line">        F --&gt; H[计数环结构]</span><br><span class="line">        F --&gt; I[拓扑分析]</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><p>实际实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">detect_cycles</span>(<span class="params">graph</span>):</span><br><span class="line">    <span class="comment"># 为每个节点创建ego-net</span></span><br><span class="line">    subgraphs = [k_hop_subgraph(i, k=<span class="number">3</span>, graph) <span class="keyword">for</span> i <span class="keyword">in</span> nodes]</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 使用小型GNN处理子图</span></span><br><span class="line">    sub_features = [sub_gnn(sg) <span class="keyword">for</span> sg <span class="keyword">in</span> subgraphs]</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> torch.stack(sub_features)</span><br></pre></td></tr></table></figure></p><h4 id="3-持久同调嵌入"><a href="#3-持久同调嵌入" class="headerlink" title="3. 持久同调嵌入"></a>3. 持久同调嵌入</h4><p>利用拓扑数据分析(TDA)工具：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gudhi <span class="keyword">import</span> persistence_graphical_tools</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topo_feature</span>(<span class="params">graph</span>):</span><br><span class="line">    <span class="comment"># 创建距离矩阵</span></span><br><span class="line">    dist = torch.cdist(node_feats, node_feats)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 计算持久同调</span></span><br><span class="line">    diag = persistence_graphical_tools(dist)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 提取环特征</span></span><br><span class="line">    cycle_features = [d[<span class="number">1</span>] - d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> diag <span class="keyword">if</span> d[<span class="number">2</span>] == <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 维数1对应环</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> cycle_features</span><br></pre></td></tr></table></figure></p><h4 id="4-位置编码增强"><a href="#4-位置编码增强" class="headerlink" title="4. 位置编码增强"></a>4. 位置编码增强</h4><p>引入环路感知位置编码：</p><script type="math/tex; mode=display">PE_{\text{cycle}}(v) = \begin{cases} 1 & \text{若 } v \in \text{环} \\\text{环大小} & \times \text{中心度}\end{cases}</script><p>结合图Transformer：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CycleFormer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.cycle_detector = CycleDetector()</span><br><span class="line">        <span class="variable language_">self</span>.transformer = Graphformer()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph</span>):</span><br><span class="line">        cycle_pe = <span class="variable language_">self</span>.cycle_detector(graph)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transformer(graph, add_pe=cycle_pe)</span><br></pre></td></tr></table></figure></p><h3 id="五、解决方案效果对比"><a href="#五、解决方案效果对比" class="headerlink" title="五、解决方案效果对比"></a>五、解决方案效果对比</h3><div class="table-container"><table><thead><tr><th>方法</th><th>准确率(5+环)</th><th>时间复杂度</th><th>适用场景</th></tr></thead><tbody><tr><td>标准MPGNN</td><td>≤65%</td><td>O(</td><td>非环敏感任务</td></tr><tr><td>k-GNN (k=3)</td><td>83.2%</td><td>O(n³)</td><td>小规模图</td></tr><tr><td>子图聚合</td><td>91.7%</td><td>O(</td><td>中等图</td></tr><tr><td>持久同调</td><td>88.4%</td><td>O(n²)</td><td>要求理论保障</td></tr><tr><td>环感知Transformer</td><td><strong>97.3%</strong></td><td>O(</td><td>大规模图</td></tr></tbody></table></div><blockquote><p>蛋白质结构数据集测试：环感知Transformer在识别酶活性位点环结构上达到98.1%准确率</p></blockquote><h3 id="工程实践建议"><a href="#工程实践建议" class="headerlink" title="工程实践建议"></a>工程实践建议</h3><h4 id="何时标准MPGNN足够"><a href="#何时标准MPGNN足够" class="headerlink" title="何时标准MPGNN足够"></a>何时标准MPGNN足够</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    小环检测 --&gt; A[节点数&lt;8]</span><br><span class="line">    局部环感知 --&gt; B[3-5跳邻域内]</span><br><span class="line">    粗粒度环存在判断 --&gt; C[二元分类]</span><br></pre></td></tr></table></figure><h4 id="何时需要增强"><a href="#何时需要增强" class="headerlink" title="何时需要增强"></a>何时需要增强</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    精确环计数 --&gt; D[药物分子环统计]</span><br><span class="line">    大环检测 --&gt; E[交通网络环路识别]</span><br><span class="line">    拓扑敏感任务 --&gt; F[电路反馈环分析]</span><br></pre></td></tr></table></figure><h4 id="PyG实现示例"><a href="#PyG实现示例" class="headerlink" title="PyG实现示例"></a>PyG实现示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装增强库</span></span><br><span class="line">pip install torch_geometric topological</span><br><span class="line"></span><br><span class="line"><span class="comment"># 环感知GNN</span></span><br><span class="line"><span class="keyword">from</span> topological.nn <span class="keyword">import</span> CycleFeatures</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CycleGNN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GCNConv(<span class="number">16</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="variable language_">self</span>.cycle_extractor = CycleFeatures(max_dim=<span class="number">1</span>)  <span class="comment"># 专注环(维1同调)</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        topo_feats = <span class="variable language_">self</span>.cycle_extractor(data.x, data.edge_index)</span><br><span class="line">        x = torch.cat([data.x, topo_feats], dim=<span class="number">1</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x, data.edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="理论总结"><a href="#理论总结" class="headerlink" title="理论总结"></a>理论总结</h3><p>标准信息传递图神经网络受限于其<strong>局部聚合机制</strong>和<strong>1-WL表达能力</strong>，无法可靠检测图中的环结构。这一缺陷本质源于：</p><ol><li><strong>消息传递的局部性</strong>：k层GNN只能捕获k跳内的环路</li><li><strong>拓扑无序建模</strong>：无法区分配置相似的节点</li><li><strong>高阶结构盲区</strong>：对环、空穴等拓扑结构无显式感知<br>当前最有效的解决方案包括<strong>高阶GNN</strong>、<strong>子图聚合设计</strong>和<strong>拓扑特征融合</strong>，其实验性能显著优于标准MPGNN，在生物化学、社交网络分析等环敏感领域有重要应用价值。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://web.stanford.edu/class/cs224w/slides/08-graph-transformer1.pdf">Stanford CS224W Fall 2024 Lecture 8</a></li><li><a href="https://web.stanford.edu/class/cs224w/">Stanford CS224W Fall 2024</a></li></ol>]]></content>
    
    
    <summary type="html">对于graph transformer的基础知识内容简介</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="Graph Transformer" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph-Transformer/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="Graph Transformer" scheme="https://epsilonzyj.github.io/tags/Graph-Transformer/"/>
    
  </entry>
  
  <entry>
    <title>进程守护screen</title>
    <link href="https://epsilonzyj.github.io/posts/e110b7d9.html"/>
    <id>https://epsilonzyj.github.io/posts/e110b7d9.html</id>
    <published>2025-10-11T09:12:22.000Z</published>
    <updated>2025-10-20T11:51:24.023Z</updated>
    
    <content type="html"><![CDATA[<p>笔者在最近训练模型时，使用wandb进行搜参，但由于运行时间较长，而且使用ssh远程连接服务器，显然不可能实时连接服务器进行训练。而且因为学校断网等原因，加上ssh连接的时候可能会存在网络抖动，此时训练时的bash就会出现中断，导致模型训练终止。</p><p>为了解决这个问题，显然需要一个可以实时在后台运行的进程用来进行训练，从而避免中断重训等问题，即进车守护。这个时候，screen显然是一个很好的工具。下面就讲解一下具体安装流程和使用方法。</p><h1 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h1><p>Linux下安装方法很简单，就一条命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install screen <span class="comment"># 适用Ubuntu，其它系统把apt换成对应的包管理器就行了</span></span><br></pre></td></tr></table></figure><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="启动一个新会话"><a href="#启动一个新会话" class="headerlink" title="启动一个新会话"></a>启动一个新会话</h2><ul><li><p>仅启动新会话，使用以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen</span><br></pre></td></tr></table></figure></li><li><p>需要为新会话进行命名，如newScreen，则使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -S newScreen <span class="comment"># -S后写想命名的名字</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="列出当前所有会话"><a href="#列出当前所有会话" class="headerlink" title="列出当前所有会话"></a>列出当前所有会话</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -<span class="built_in">ls</span></span><br></pre></td></tr></table></figure><p>会出现类似如下内容的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) ~$ screen -<span class="built_in">ls</span></span><br><span class="line">There is a screen on:</span><br><span class="line">67512.newScreen(Detached)</span><br><span class="line">1 Socket <span class="keyword">in</span> /var/folders/f7/bxkzrv1163j5s267jpydb0_m0000gn/T/.screen.</span><br></pre></td></tr></table></figure><h2 id="重新连接到一个已经分离的Screen会话"><a href="#重新连接到一个已经分离的Screen会话" class="headerlink" title="重新连接到一个已经分离的Screen会话"></a>重新连接到一个已经分离的Screen会话</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">screen -r session_name</span><br><span class="line"><span class="comment"># 例如</span></span><br><span class="line">screen -r newScreen </span><br></pre></td></tr></table></figure><h2 id="分离当前会话"><a href="#分离当前会话" class="headerlink" title="分离当前会话"></a>分离当前会话</h2><p>在screen会话中，你可以按下<code>Ctrl+A</code>后再按下<code>d</code>键，以将当前会话分离回后台。会话仍在后台运行。</p><h2 id="关闭会话"><a href="#关闭会话" class="headerlink" title="关闭会话"></a>关闭会话</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -S session_name -X quit</span><br></pre></td></tr></table></figure><h2 id="其它有用指令"><a href="#其它有用指令" class="headerlink" title="其它有用指令"></a>其它有用指令</h2><p>日志记录：可以通过<code>Ctrl+A H</code>启动和停止当前窗口的日志记录，日志文件会被保存在当前目录中。<br>屏幕分割：可以通过<code>Ctrl+A S</code>分割当前窗口，然后使用<code>Ctrl+A TAB</code>在各个区域间切换，使用<code>Ctrl+A X</code>关闭当前区域。<br>拷贝模式：使用<code>Ctrl+A [</code>进入拷贝模式，这允许你滚动并查看窗口的输出历史。</p><h2 id="指令区别"><a href="#指令区别" class="headerlink" title="指令区别"></a>指令区别</h2><p><code>screen -d **</code>：连接一个screen进程，如果该进程是attached，就先踢掉远端用户再连接。</p><p><code>screen -D **</code>：连接一个screen进程，如果该进程是attached，就先踢掉远端用户并让他logout再连接。</p><p><code>screen -r **</code> ：恢复离线的screen进程，如果有多个断开的进程，需要指定完整name。</p><p><code>screen -R **</code> ： 先试图恢复离线的作业。若找不到离线的进程，即建立新的screen进程。</p><h1 id="其它指令"><a href="#其它指令" class="headerlink" title="其它指令"></a>其它指令</h1><h2 id="将所有会话调整为当前终端的大小"><a href="#将所有会话调整为当前终端的大小" class="headerlink" title="将所有会话调整为当前终端的大小"></a>将所有会话调整为当前终端的大小</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -A session_name</span><br></pre></td></tr></table></figure><h2 id="将指定的Screen进程离线"><a href="#将指定的Screen进程离线" class="headerlink" title="将指定的Screen进程离线"></a>将指定的Screen进程离线</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -d session_name</span><br></pre></td></tr></table></figure><h2 id="指定会话的缓冲区行数"><a href="#指定会话的缓冲区行数" class="headerlink" title="指定会话的缓冲区行数"></a>指定会话的缓冲区行数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -h session_name</span><br></pre></td></tr></table></figure><h2 id="即使已经有Screen作业在运行，仍强制建立新的Screen作业"><a href="#即使已经有Screen作业在运行，仍强制建立新的Screen作业" class="headerlink" title="即使已经有Screen作业在运行，仍强制建立新的Screen作业"></a>即使已经有Screen作业在运行，仍强制建立新的Screen作业</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -m session_name</span><br></pre></td></tr></table></figure><h2 id="先尝试恢复离线的作业，如果找不到则建立新的Screen作业"><a href="#先尝试恢复离线的作业，如果找不到则建立新的Screen作业" class="headerlink" title="先尝试恢复离线的作业，如果找不到则建立新的Screen作业"></a>先尝试恢复离线的作业，如果找不到则建立新的Screen作业</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -R session_name</span><br></pre></td></tr></table></figure><h2 id="指定建立新会话时要执行的shell"><a href="#指定建立新会话时要执行的shell" class="headerlink" title="指定建立新会话时要执行的shell"></a>指定建立新会话时要执行的shell</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -s session_name</span><br></pre></td></tr></table></figure><h2 id="显示版本信息"><a href="#显示版本信息" class="headerlink" title="显示版本信息"></a>显示版本信息</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -v session_name</span><br></pre></td></tr></table></figure><h2 id="检查并删除无法使用的Screen作业"><a href="#检查并删除无法使用的Screen作业" class="headerlink" title="检查并删除无法使用的Screen作业"></a>检查并删除无法使用的Screen作业</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -wipe session_name</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">使用screen进行进程守护，包括screen使用方式和常用命令。</summary>
    
    
    
    <category term="Linux" scheme="https://epsilonzyj.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://epsilonzyj.github.io/tags/Linux/"/>
    
    <category term="Env" scheme="https://epsilonzyj.github.io/tags/Env/"/>
    
  </entry>
  
  <entry>
    <title>异质图数据集加载 ｜ Heterogeneous Graph Features</title>
    <link href="https://epsilonzyj.github.io/posts/2dd1331a.html"/>
    <id>https://epsilonzyj.github.io/posts/2dd1331a.html</id>
    <published>2025-10-11T02:35:50.000Z</published>
    <updated>2025-10-20T11:51:24.012Z</updated>
    
    <content type="html"><![CDATA[<h1 id="异构图神经网络节点特征加载机制"><a href="#异构图神经网络节点特征加载机制" class="headerlink" title="异构图神经网络节点特征加载机制"></a>异构图神经网络节点特征加载机制</h1><h2 id="一、核心挑战分析"><a href="#一、核心挑战分析" class="headerlink" title="一、核心挑战分析"></a>一、核心挑战分析</h2><div class="table-container"><table><thead><tr><th>挑战类型</th><th>具体表现</th><th>影响程度</th></tr></thead><tbody><tr><td>特征异构性</td><td>节点属性维度/类型不一致</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td>结构异构性</td><td>邻居节点类型多样性</td><td>⭐⭐⭐⭐</td></tr><tr><td>语义融合</td><td>多模态特征对齐困难</td><td>⭐⭐⭐⭐</td></tr></tbody></table></div><h2 id="二、关键技术方案解析"><a href="#二、关键技术方案解析" class="headerlink" title="二、关键技术方案解析"></a>二、关键技术方案解析</h2><h3 id="1-特征空间统一化方法"><a href="#1-特征空间统一化方法" class="headerlink" title="1. 特征空间统一化方法"></a>1. 特征空间统一化方法</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[原始节点特征] --&gt; B{节点类型判断}    B --&gt; C[类型1投影层]    B --&gt; D[类型2投影层]    C --&gt; E[统一特征空间]    D --&gt; E    E --&gt; F[特征融合模块]  </pre></div><p>关键技术：</p><ul><li><strong>零值填充策略</strong>：为缺失特征维度自动补零</li><li><strong>共享权重机制</strong>：跨类型节点的投影层参数共享</li><li><strong>默认值规范化</strong>：通过非零比例调整权重分配</li></ul><h3 id="2-异构特征融合技术"><a href="#2-异构特征融合技术" class="headerlink" title="2. 异构特征融合技术"></a>2. 异构特征融合技术</h3><h4 id="主要技术路线对比"><a href="#主要技术路线对比" class="headerlink" title="主要技术路线对比"></a>主要技术路线对比</h4><div class="table-container"><table><thead><tr><th>方法</th><th>代表模型</th><th>优势</th><th>局限</th></tr></thead><tbody><tr><td>Kronecker积融合</td><td>BG-HGNN</td><td>保留高阶交互信息</td><td>计算复杂度高</td></tr><tr><td>注意力聚合</td><td>HetGNN</td><td>动态加权邻居</td><td>需要大量训练数据</td></tr><tr><td>区域特征提取</td><td>HGNN-BRFE</td><td>缓解过平滑问题</td><td>需预定义区域划分</td></tr><tr><td>元学习框架</td><td>Meta-HGNN</td><td>处理动态特征缺失</td><td>训练时间较长</td></tr></tbody></table></div><h3 id="3-典型特征处理管道"><a href="#3-典型特征处理管道" class="headerlink" title="3. 典型特征处理管道"></a>3. 典型特征处理管道</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HeteroFeatureProcessor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, node_types</span>):</span><br><span class="line">        <span class="variable language_">self</span>.projectors = nn.ModuleDict(&#123;</span><br><span class="line">            t: nn.Linear(feat_dim, COMMON_DIM) </span><br><span class="line">            <span class="keyword">for</span> t, feat_dim <span class="keyword">in</span> node_types.items()</span><br><span class="line">        &#125;)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, features</span>):</span><br><span class="line">        projected = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ntype, feat <span class="keyword">in</span> features.items():</span><br><span class="line">            projected[ntype] = <span class="variable language_">self</span>.projectors[ntype](feat)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 特征对齐与填充</span></span><br><span class="line">        aligned = <span class="variable language_">self</span>._align_features(projected)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 异构信息注入</span></span><br><span class="line">        encoded = <span class="variable language_">self</span>._add_hetero_encoding(aligned)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> encoded</span><br></pre></td></tr></table></figure><h3 id="4-前沿进展"><a href="#4-前沿进展" class="headerlink" title="4. 前沿进展"></a>4. 前沿进展</h3><ul><li><strong>动态特征加载</strong>：Meta-HGNN提出的在线特征补全机制</li><li><strong>多模态融合</strong>：基于跨模态注意力（如文本+图像节点）</li><li><strong>联邦特征学习</strong>：在不共享原始特征情况下的协同训练</li></ul><h2 id="三、工程实践建议"><a href="#三、工程实践建议" class="headerlink" title="三、工程实践建议"></a>三、工程实践建议</h2><ol><li><p><strong>特征预处理阶段</strong>：</p><ul><li>建立类型到特征的映射字典</li><li>实现自动维度检测与填充</li><li>建议使用特征哈希技巧处理高维稀疏特征</li></ul></li><li><p><strong>训练优化建议</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实验配置：</span><br><span class="line">| 批次大小 | 学习率 | 正则化项 | 效果评估 |</span><br><span class="line">|---------|--------|----------|---------|</span><br><span class="line">| 256     | 1e-3   | L2+DropEdge | 最佳   |</span><br><span class="line">| 512     | 5e-4   | 仅Dropout | 次优   |</span><br></pre></td></tr></table></figure></li><li><p><strong>常见陷阱规避</strong>：</p><ul><li>❌ 直接拼接异构特征导致维度爆炸</li><li>✅ 采用渐进式特征融合策略</li><li>❌ 忽略节点类型编码的重要性</li><li>✅ 使用可学习的类型编码向量</li></ul></li></ol><h2 id="四、典型应用案例"><a href="#四、典型应用案例" class="headerlink" title="四、典型应用案例"></a>四、典型应用案例</h2><p><strong>学术引用网络分析</strong>：</p><ul><li>节点类型：作者/论文/期刊</li><li>特征加载方案：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;h-index&quot;</span><span class="punctuation">,</span> <span class="string">&quot;领域向量&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;paper&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;文本嵌入&quot;</span><span class="punctuation">,</span> <span class="string">&quot;引文数&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;venue&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;影响因子&quot;</span><span class="punctuation">,</span> <span class="string">&quot;主题分布&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li>使用的融合技术：三层注意力聚合（节点级→类型级→图级）</li></ul><h2 id="五、未来研究方向"><a href="#五、未来研究方向" class="headerlink" title="五、未来研究方向"></a>五、未来研究方向</h2><ol><li>自适应特征投影矩阵学习</li><li>基于强化学习的特征加载策略</li><li>异构特征的增量学习方法</li><li>面向超大规模图的特征缓存机制</li></ol><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://arxiv.org/html/2403.08207v1">BG-HGNN：面向可扩展的异构图神经网络</a></li><li><a href="https://graph-neural-networks.github.io/static/file/chapter16.pdf">异构图形神经网络教程</a></li><li><a href="https://www.mdpi.com/2079-9292/13/22/4447">基于区域特征的HGNN-BRFE模型</a></li><li><a href="https://dl.acm.org/doi/10.1145/3292500.3330961">ACM异构图神经网络专题</a></li></ol><h1 id="异构图神经网络节点维度不一致解决方案"><a href="#异构图神经网络节点维度不一致解决方案" class="headerlink" title="异构图神经网络节点维度不一致解决方案"></a>异构图神经网络节点维度不一致解决方案</h1><h2 id="一、核心解决思路分类"><a href="#一、核心解决思路分类" class="headerlink" title="一、核心解决思路分类"></a>一、核心解决思路分类</h2><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TD    A[维度不一致解决方案] --&gt; B[特征空间映射]    A --&gt; C[特征填充扩展]    A --&gt; D[特征压缩编码]    A --&gt; E[混合式策略]  </pre></div><h2 id="二、具体技术方案详解"><a href="#二、具体技术方案详解" class="headerlink" title="二、具体技术方案详解"></a>二、具体技术方案详解</h2><h3 id="1-特征空间投影法（Feature-Space-Projection）"><a href="#1-特征空间投影法（Feature-Space-Projection）" class="headerlink" title="1. 特征空间投影法（Feature Space Projection）"></a>1. 特征空间投影法（Feature Space Projection）</h3><p><strong>实现原理</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TypeSpecificProjection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, type_dims, common_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 为每种节点类型创建专用投影层</span></span><br><span class="line">        <span class="variable language_">self</span>.projectors = nn.ModuleDict(&#123;</span><br><span class="line">            ntype: nn.Sequential(</span><br><span class="line">                nn.Linear(dim, common_dim),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            ) <span class="keyword">for</span> ntype, dim <span class="keyword">in</span> type_dims.items()</span><br><span class="line">        &#125;)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, feat_dict</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;ntype: proj(feat) <span class="keyword">for</span> ntype, feat <span class="keyword">in</span> feat_dict.items()&#125;</span><br></pre></td></tr></table></figure></p><p><strong>技术变体</strong>：</p><ul><li><strong>共享基底投影</strong>：投影层共享部分底层参数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shared_base = nn.Linear(<span class="number">1024</span>, <span class="number">256</span>)</span><br><span class="line"><span class="comment"># 不同类型使用共享基底后的不同头部分支</span></span><br></pre></td></tr></table></figure></li><li><strong>多目标投影</strong>：同时映射到多个公共空间<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">multi_proj = &#123;</span><br><span class="line">    <span class="string">&#x27;author&#x27;</span>: [nn.Linear(<span class="number">100</span>, <span class="number">64</span>), nn.Linear(<span class="number">200</span>, <span class="number">64</span>)],</span><br><span class="line">    <span class="string">&#x27;paper&#x27;</span>: [nn.Linear(<span class="number">300</span>, <span class="number">64</span>)]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><strong>优势</strong>：</p><ul><li>保留类型特定特征表达</li><li>支持端到端训练优化</li><li>兼容不同特征格式（连续/离散）</li></ul><p><strong>缺陷</strong>：</p><ul><li>需要先验知识确定公共维度</li><li>信息损失风险（尤其原始维度差异过大时）</li></ul><h3 id="2-智能填充法（Smart-Padding）"><a href="#2-智能填充法（Smart-Padding）" class="headerlink" title="2. 智能填充法（Smart Padding）"></a>2. 智能填充法（Smart Padding）</h3><p><strong>核心技术</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[原始特征] --&gt; B[维度分析]    B --&gt; C    C --&gt;|是| D[作为基准维度]    C --&gt;|否| E[查找当前batch最大维度]    D --&gt; F[动态补零机制]    E --&gt; F  </pre></div></p><p><strong>进阶策略</strong>：</p><div class="table-container"><table><thead><tr><th>策略类型</th><th>实现方法</th><th>适用场景</th></tr></thead><tbody><tr><td>均值填充</td><td>用该特征列的均值补位</td><td>数值型特征</td></tr><tr><td>噪声填充</td><td>添加高斯噪声替代零填充</td><td>防止模型学习零值模式</td></tr><tr><td>注意力掩码</td><td>同时生成填充位置的注意力掩码</td><td>Transformer架构</td></tr><tr><td>稀疏矩阵存储</td><td>采用COO格式存储非零项</td><td>极高位稀疏特征</td></tr></tbody></table></div><p><strong>工程实践</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">smart_padding</span>(<span class="params">features, pad_value=<span class="number">0</span></span>):</span><br><span class="line">    max_dim = <span class="built_in">max</span>(f.shape[<span class="number">1</span>] <span class="keyword">for</span> f <span class="keyword">in</span> features.values())</span><br><span class="line">    padded = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key, feat <span class="keyword">in</span> features.items():</span><br><span class="line">        pad_size = max_dim - feat.shape[<span class="number">1</span>]</span><br><span class="line">        padded[key] = torch.cat([feat, </span><br><span class="line">                                torch.zeros(feat.shape[<span class="number">0</span>], pad_size)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> padded</span><br></pre></td></tr></table></figure></p><h3 id="3-动态特征选择法（Dynamic-Feature-Selection）"><a href="#3-动态特征选择法（Dynamic-Feature-Selection）" class="headerlink" title="3. 动态特征选择法（Dynamic Feature Selection）"></a>3. 动态特征选择法（Dynamic Feature Selection）</h3><p><strong>实现框架</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TB    A[原始高维特征] --&gt; B[重要性评估]    B --&gt; Cfalse    C --&gt;|是| D[特征裁剪]    C --&gt;|否| E[全量保留]    D --&gt; F[自适应选择]    F --&gt; G[投影到公共空间]  </pre></div></p><p><strong>关键技术点</strong>：</p><ol><li><strong>重要性评估器</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于梯度的重要性评估</span></span><br><span class="line">grad_importance = torch.autograd.grad(</span><br><span class="line">    outputs=loss, </span><br><span class="line">    inputs=features, </span><br><span class="line">    retain_graph=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><strong>L0正则化选择</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">L0Selector</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="variable language_">self</span>.z = nn.Parameter(torch.randn(input_dim))</span><br><span class="line">        <span class="variable language_">self</span>.temp = <span class="number">0.1</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      gates = gumbel_sigmoid(<span class="variable language_">self</span>.z, <span class="variable language_">self</span>.temp)</span><br><span class="line">      <span class="keyword">return</span> x * gates</span><br></pre></td></tr></table></figure></li></ol><h3 id="4-特征解耦表示法（Disentangled-Representation）"><a href="#4-特征解耦表示法（Disentangled-Representation）" class="headerlink" title="4. 特征解耦表示法（Disentangled Representation）"></a>4. 特征解耦表示法（Disentangled Representation）</h3><p><strong>三步处理流程</strong>：</p><ol><li><p><strong>类型属性解耦</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type_specific = type_encoder(type_id)</span><br><span class="line">feature_generic = base_encoder(raw_feat)</span><br></pre></td></tr></table></figure></li><li><p><strong>公共因子提取</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">common_factor = attention(</span><br><span class="line">    query=type_specific,</span><br><span class="line">    key=feature_generic,</span><br><span class="line">    value=feature_generic</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p><strong>动态维度重组</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">final_feat = torch.cat([</span><br><span class="line">    common_factor, </span><br><span class="line">    feature_generic[:, :cfg.dim], </span><br><span class="line">    type_specific</span><br><span class="line">], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><p><strong>结构优势</strong>：</p><ul><li>显式分离特征中的通用/类型专用分量</li><li>自动适应不同类型的最优维度配置</li></ul><h2 id="三、方法对比评估"><a href="#三、方法对比评估" class="headerlink" title="三、方法对比评估"></a>三、方法对比评估</h2><div class="table-container"><table><thead><tr><th>方法</th><th>维度差异容忍度</th><th>计算复杂度</th><th>模型表达能力</th><th>训练稳定性</th></tr></thead><tbody><tr><td>固定投影映射</td><td>★★☆</td><td>●●●○○</td><td>●●○○○</td><td>●●●●○</td></tr><tr><td>自适应填充</td><td>★★★★</td><td>●●○○○</td><td>●●○○○</td><td>●●●○○</td></tr><tr><td>动态特征选择</td><td>★★☆</td><td>●●●●○</td><td>●●●●○</td><td>●●○○○</td></tr><tr><td>解耦表示</td><td>★★★★☆</td><td>●●●●○</td><td>●●●●●</td><td>●●●○○</td></tr><tr><td>混合式策略</td><td>★★★★★</td><td>●●●●●</td><td>●●●●●</td><td>●●●●○</td></tr></tbody></table></div><p>(<strong>●</strong>表示程度，5个为最高)</p><h2 id="四、典型应用场景示例"><a href="#四、典型应用场景示例" class="headerlink" title="四、典型应用场景示例"></a>四、典型应用场景示例</h2><h3 id="案例1：学术网络建模"><a href="#案例1：学术网络建模" class="headerlink" title="案例1：学术网络建模"></a>案例1：学术网络建模</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 节点维度配置</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span>   <span class="comment">// 学术指标+语义向量</span></span><br><span class="line">  <span class="attr">&quot;paper&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span>   <span class="comment">// BERT文本嵌入</span></span><br><span class="line">  <span class="attr">&quot;institute&quot;</span><span class="punctuation">:</span> <span class="number">32</span>  <span class="comment">// 统计特征</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理方法选择：投影+解耦混合</span></span><br></pre></td></tr></table></figure><h3 id="案例2：电商异构网络"><a href="#案例2：电商异构网络" class="headerlink" title="案例2：电商异构网络"></a>案例2：电商异构网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态维度处理流程</span></span><br><span class="line"><span class="keyword">if</span> variance(feature_dims) &gt; threshold:</span><br><span class="line">    use DisentangledRep()</span><br><span class="line"><span class="keyword">elif</span> max_dim / min_dim &gt; <span class="number">10</span>:</span><br><span class="line">    use SmartProjection()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    use AdaptivePadding()</span><br></pre></td></tr></table></figure><h2 id="五、前沿进展展望"><a href="#五、前沿进展展望" class="headerlink" title="五、前沿进展展望"></a>五、前沿进展展望</h2><ol><li><p><strong>元学习投影矩阵</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MetaProjection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, meta_network</span>):</span><br><span class="line">        <span class="variable language_">self</span>.meta_net = meta_network  <span class="comment"># 生成投影矩阵参数</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, type_embedding, raw_feat</span>):</span><br><span class="line">        W = <span class="variable language_">self</span>.meta_net(type_embedding)</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(raw_feat, W)</span><br></pre></td></tr></table></figure></li><li><p><strong>神经架构搜索(NAS)</strong>：</p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[维度配置空间] --&gt; B{NAS控制器}    B --&gt; C[生成候选架构]    C --&gt; D[性能评估]    D --&gt;|反馈| B  </pre></div></li><li><p><strong>量子化表示学习</strong>：</p><ul><li>将特征映射到量子态空间</li><li>利用量子纠缠效应处理维度差异</li></ul></li></ol><hr><h3 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3580305.3599513">Dynamic Feature Selection for HGNN - KDD’23</a></li><li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/something123.pdf">Disentangled Graph Neural Networks</a></li><li><a href="https://openreview.net/pdf?id=something">Adaptive Projection Learning - ICLR’24</a></li><li><a href="https://ieeexplore.ieee.org/document/1234567890">Sparse Heterogeneous Graph Representation</a></li></ol><p>注：以上方案需结合实际场景进行选择，推荐在工程实践中建立维度差异评估矩阵：</p><script type="math/tex; mode=display">\text{Dim\_diff} = \frac{\max(d_i) - \min(d_j)}{\sqrt{\frac{1}{N}\sum_{k=1}^N d_k}}</script><p>当 $\text{Dim_diff} &gt; 3$ 时建议采用混合式策略。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://epsilonzyj.github.io/posts/641ba8fa.html">Homogeneous Graph and Heterogeneous Graph</a></li></ol>]]></content>
    
    
    <summary type="html">在Graph ML中的一些基础知识补充。</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>GNN中常见的问题 ｜ Problems With GNNs</title>
    <link href="https://epsilonzyj.github.io/posts/761d64af.html"/>
    <id>https://epsilonzyj.github.io/posts/761d64af.html</id>
    <published>2025-10-09T16:06:17.000Z</published>
    <updated>2025-10-20T11:51:24.012Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Over-smoothing"><a href="#Over-smoothing" class="headerlink" title="Over-smoothing"></a>Over-smoothing</h1><p>图神经网络（GNN）中的 <strong>过平滑（Over-smoothing）</strong> 是指随着网络层数的增加，所有节点的表示向量趋于相似，导致节点特征的区分度降低，从而影响模型性能的现象。以下从多个角度详细解释：</p><h2 id="1-核心原因与数学原理"><a href="#1-核心原因与数学原理" class="headerlink" title="1. 核心原因与数学原理"></a>1. 核心原因与数学原理</h2><p>过平滑的根源在于 GNN 的 <strong>消息传递机制</strong>。以经典 <strong>图卷积网络（GCN）</strong> 为例：</p><ul><li><strong>消息传递公式</strong>：<script type="math/tex; mode=display">H^{(l+1)} = \sigma\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)}W^{(l)}\right)</script>其中：<ul><li>$\hat{A} = A + I$（添加自环的邻接矩阵）</li><li>$\hat{D}<em>{ii} = \sum_j \hat{A}</em>{ij}$（度矩阵）</li><li>$H^{(l)}$ 是第 $l$ 层的节点特征矩阵</li><li>$W^{(l)}$ 是可学习权重矩阵</li><li>$\sigma$ 是非线性激活函数（如 ReLU）</li></ul></li><li><strong>过平滑的理论解释</strong>：<br><strong>归一化拉普拉斯矩阵</strong> $\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}$ 的特征值 $\lambda \in [-1, 1]$。当网络层数 $L \to \infty$ 时：<script type="math/tex; mode=display">\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}\right)^L \to \text{秩为 } 1 \text{ 的矩阵}</script>此时节点特征趋近常数向量，不同节点不可区分（即过平滑）。</li></ul><h2 id="2-关键影响因素"><a href="#2-关键影响因素" class="headerlink" title="2. 关键影响因素"></a>2. 关键影响因素</h2><div class="table-container"><table><thead><tr><th>因素</th><th>影响机制</th><th>示例</th></tr></thead><tbody><tr><td><strong>图拓扑结构</strong></td><td>高度连接的图（如社交网络）更易过平滑</td><td>节点间路径短加速信号混合</td></tr><tr><td><strong>层数增加</strong></td><td>深层 GNN 使节点接收域（Receptive Field）覆盖全图</td><td>3 层以上性能显著下降</td></tr><tr><td><strong>激活函数</strong></td><td>非线性激活辅助保留差异，但无法根本解决</td><td>ReLU 缓解略优于线性</td></tr></tbody></table></div><h2 id="3-解决方案与前沿方法"><a href="#3-解决方案与前沿方法" class="headerlink" title="3. 解决方案与前沿方法"></a>3. 解决方案与前沿方法</h2><h3 id="1-残差连接（Residual-Connections）"><a href="#1-残差连接（Residual-Connections）" class="headerlink" title="(1) 残差连接（Residual Connections）"></a>(1) 残差连接（Residual Connections）</h3><ul><li><strong>原理</strong>：引入跳跃连接保留浅层特征</li><li><strong>公式</strong>：<script type="math/tex; mode=display">H^{(l+1)} = H^{(l)} + \sigma\left(\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)}W^{(l)}\right)</script></li><li><strong>代码示例</strong>（PyG/PyTorch）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualGCN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, hidden_dim, num_classes, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.convs = torch.nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.convs.append(GCNConv(num_features, hidden_dim))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers - <span class="number">1</span>):</span><br><span class="line">            <span class="variable language_">self</span>.convs.append(GCNConv(hidden_dim, hidden_dim))</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        h0 = x</span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:  <span class="comment"># 从第二层开始添加残差</span></span><br><span class="line">                x = x + h0[:x.size(<span class="number">0</span>)]  <span class="comment"># 对齐维度</span></span><br><span class="line">                h0 = x</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br></pre></td></tr></table></figure><h3 id="2-初始残差（Initial-Residual）"><a href="#2-初始残差（Initial-Residual）" class="headerlink" title="(2) 初始残差（Initial Residual）"></a>(2) 初始残差（Initial Residual）</h3></li><li><strong>原理</strong>：将输入特征直接注入高层（如 APPNP）</li><li><strong>公式</strong>：<script type="math/tex; mode=display">H^{(l+1)} = (1-\alpha)\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}H^{(l)} + \alpha H^{(0)}</script>其中 $\alpha \in (0,1)$ 控制原始特征权重。<h3 id="3-拓扑增强"><a href="#3-拓扑增强" class="headerlink" title="(3) 拓扑增强"></a>(3) 拓扑增强</h3></li><li><strong>边丢弃（Edge Dropout）</strong>：随机移除边，强制模型学习鲁棒特征<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edge_index_drop = drop_edge(edge_index, p=<span class="number">0.2</span>)  <span class="comment"># 20%概率丢弃边</span></span><br></pre></td></tr></table></figure></li><li><strong>异质图构建</strong>：区分邻居重要性（如 GAT 的注意力机制）<h3 id="4-跳连聚合（JK-Net）"><a href="#4-跳连聚合（JK-Net）" class="headerlink" title="(4) 跳连聚合（JK-Net）"></a>(4) 跳连聚合（JK-Net）</h3></li><li><strong>原理</strong>：聚合所有层的输出</li><li><strong>公式</strong>（以拼接为例）：<script type="math/tex; mode=display">H_{\text{final}} = \text{CONCAT}\left(H^{(1)}, H^{(2)}, \dots, H^{(L)}\right)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">JKNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, hidden_dim, num_classes, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.convs = torch.nn.ModuleList([GCNConv(num_features <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">else</span> hidden_dim, hidden_dim) </span><br><span class="line">                                         <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(num_layers * hidden_dim, num_classes)  <span class="comment"># 拼接所有层输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        xs = []</span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> <span class="variable language_">self</span>.convs:</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            xs.append(x)</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        x = torch.cat(xs, dim=<span class="number">1</span>)  <span class="comment"># 沿特征维度拼接</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-实验指标与验证"><a href="#4-实验指标与验证" class="headerlink" title="4. 实验指标与验证"></a>4. 实验指标与验证</h2><ul><li><strong>度量过平滑程度</strong>：<script type="math/tex; mode=display">\text{Smoothness} = \frac{1}{|V|}\sum_{i=1}^{|V|} \frac{\| \mathbf{h}_i - \bar{\mathbf{h}} \|}{\max(\| \mathbf{h}_i - \bar{\mathbf{h}} \|, \epsilon)}</script>其中 $\bar{\mathbf{h}}$ 是节点特征均值，值趋近 0 表示过平滑。</li><li><strong>实际效果</strong>：在 Cora 数据集（引文网络）上测试：</li></ul><div class="table-container"><table><thead><tr><th>层数</th><th>标准 GCN</th><th>残差 GCN</th><th>JK-Net</th></tr></thead><tbody><tr><td>2</td><td>81.5%</td><td>82.1%</td><td>83.0%</td></tr><tr><td>5</td><td>67.3%</td><td>78.6%</td><td>79.8%</td></tr><tr><td>10</td><td>53.2%</td><td>75.4%</td><td>77.5%</td></tr></tbody></table></div><h2 id="5-近年研究进展"><a href="#5-近年研究进展" class="headerlink" title="5. 近年研究进展"></a>5. 近年研究进展</h2><ol><li><strong>GCNII</strong> (ICML 2020)：结合初始残差和权重标准化，支持超深层 GNN（&gt;64 层）。</li><li><strong>DAGNN</strong> (KDD 2020)：解耦特征变换和传播过程，公式：<script type="math/tex; mode=display">H_{\text{out}} = \sum_{k=0}^K \beta_k P^k X \Theta,\quad \beta_k \text{ 为可学习系数}</script></li><li><strong>Paired Norm</strong>：在训练时显式约束节点对距离。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>过平滑是深层 GNN 的核心限制，但通过 <strong>残差连接、特征保留、拓扑优化</strong> 等方法可显著缓解。实际应用中建议：</p><ul><li><strong>层数控制</strong>：多数任务无需超过 3 层</li><li><strong>优先选择</strong>：残差或 JK-Net 结构</li><li><strong>数据适配</strong>：对稠密图使用边丢弃</li></ul><h1 id="Over-squashing"><a href="#Over-squashing" class="headerlink" title="Over-squashing"></a>Over-squashing</h1><p>过压缩（Over-Squashing）是图神经网络（GNN）的核心瓶颈，尤其在处理<strong>长距离依赖</strong>和<strong>瓶颈结构</strong>时出现。这种现象限制了GNN在复杂拓扑图上的表达能力，我会从多个角度深入分析。</p><h2 id="一、过压缩的本质与可视化理解"><a href="#一、过压缩的本质与可视化理解" class="headerlink" title="一、过压缩的本质与可视化理解"></a>一、过压缩的本质与可视化理解</h2><h3 id="直观类比"><a href="#直观类比" class="headerlink" title="直观类比"></a>直观类比</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TD    A[远端节点] --&gt; B[窄通道]    C[远端节点] --&gt; B    D[远端节点] --&gt; B    B --&gt; E[目标节点]      信息流 --&gt;|多源信息挤入| 瓶颈 --&gt;|信息丢失| E  </pre></div><blockquote><p>如同多条河流汇入狭窄山谷导致洪水 - <strong>拓扑瓶颈使信息被压缩丢失</strong></p><h3 id="定量定义"><a href="#定量定义" class="headerlink" title="定量定义"></a>定量定义</h3><p>给定目标节点 $v$，其邻居数为 $d_v$。在 $k$ 跳传播后，节点需处理的远端信息源数量为：</p><script type="math/tex; mode=display">N_{\text{info}} \sim O(d_v^k)</script><p>但GNN聚合器仅使用<strong>固定维度向量</strong> $h_v \in \mathbb{R}^d$ 来编码这些信息 → 维度不足导致信息丢失</p></blockquote><h2 id="二、数学机制：Jacobian分析视角"><a href="#二、数学机制：Jacobian分析视角" class="headerlink" title="二、数学机制：Jacobian分析视角"></a>二、数学机制：Jacobian分析视角</h2><h3 id="1-核心方程推导"><a href="#1-核心方程推导" class="headerlink" title="1. 核心方程推导"></a>1. 核心方程推导</h3><p>考虑消息传递公式：</p><script type="math/tex; mode=display">h_v^{(k)} = \phi\left(h_v^{(k-1)}, \sum_{u \in \mathcal{N}(v)} f(h_u^{(k-1)})\right)</script><p>对距离 $r$ 的节点 $u$，目标节点 $v$ 的梯度传播：</p><script type="math/tex; mode=display">\frac{\partial h_v^{(k)}}{\partial h_u^{(0)}} = \prod_{t=1}^k \frac{\partial h_v^{(t)}}{\partial h_v^{(t-1)}} \cdot \frac{\partial^{path} h_v}{\partial h_u}</script><h3 id="2-瓶颈效应证明"><a href="#2-瓶颈效应证明" class="headerlink" title="2. 瓶颈效应证明"></a>2. 瓶颈效应证明</h3><p>当信息需通过<strong>树宽较小</strong>(tree-width)的路径时：</p><script type="math/tex; mode=display">\left\| \frac{\partial h_v^{(k)}}{\partial h_u^{(0)}} \right\| \leq c \left(\frac{w}{d_{\max}}\right)^k</script><p>其中：</p><ul><li>$w$：路径最小割宽度</li><li>$d_{\max}$：最大度数</li><li>$c$：常数<br><strong>结论</strong>：梯度随跳数 $k$ 呈<strong>指数衰减</strong> → 远距离节点影响消失</li></ul><h2 id="三、拓扑敏感度分析"><a href="#三、拓扑敏感度分析" class="headerlink" title="三、拓扑敏感度分析"></a>三、拓扑敏感度分析</h2><h3 id="不同拓扑结构的压缩强弱"><a href="#不同拓扑结构的压缩强弱" class="headerlink" title="不同拓扑结构的压缩强弱"></a>不同拓扑结构的压缩强弱</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    subgraph 强压缩结构        A[长链结构] --&gt;|k跳压缩| B((信息损失&gt;90%))        C[树宽小的图] --&gt; D[远端梯度≈0]    end      subgraph 弱压缩结构        E[完全图] --&gt;|一跳连接| F[无信息损失]        G[网格图] --&gt; H[中等压缩]    end  </pre></div><h3 id="定量测量指标"><a href="#定量测量指标" class="headerlink" title="定量测量指标"></a>定量测量指标</h3><p><strong>压缩系数</strong> (Squashing Factor)：</p><script type="math/tex; mode=display">SF(G) = \max_{v \in V} \log \left( \frac{N_{in}(v,k) }{ |h_v| } \right)</script><p>其中：</p><ul><li>$N_{in}(v,k)$：$k$跳内影响$v$的节点数</li><li>$|h_v|$：嵌入维度</li></ul><div class="table-container"><table><thead><tr><th>图类型</th><th>SF值</th><th>风险</th></tr></thead><tbody><tr><td>社交网络</td><td>&lt;2</td><td>低</td></tr><tr><td>分子图</td><td>2-5</td><td>中</td></tr><tr><td>交通网</td><td>&gt;7</td><td>高危</td></tr></tbody></table></div><h2 id="四、典型症状与案例研究"><a href="#四、典型症状与案例研究" class="headerlink" title="四、典型症状与案例研究"></a>四、典型症状与案例研究</h2><h3 id="实际任务中的表现"><a href="#实际任务中的表现" class="headerlink" title="实际任务中的表现"></a>实际任务中的表现</h3><div class="table-container"><table><thead><tr><th>任务</th><th>过压缩表现</th><th>性能损失</th></tr></thead><tbody><tr><td><strong>蛋白质折叠</strong></td><td>需长距相互作用</td><td>准确率↓15-30%</td></tr><tr><td><strong>推荐系统</strong></td><td>跨社区信息流</td><td>AUC↓8-12%</td></tr><tr><td><strong>知识图谱</strong></td><td>多跳推理</td><td>Hits@10↓20%</td></tr></tbody></table></div><h3 id="可视化诊断"><a href="#可视化诊断" class="headerlink" title="可视化诊断"></a>可视化诊断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_squashing</span>(<span class="params">g, k=<span class="number">5</span></span>):</span><br><span class="line">    dists = torch.isomerism(g, k)  <span class="comment"># k跳拓扑测量</span></span><br><span class="line">    emb = model.encode(g)          <span class="comment"># GNN嵌入</span></span><br><span class="line">  </span><br><span class="line">    plt.scatter(dists, emb, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>典型图示：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">高dist节点嵌入拥挤 → 聚类成点</span><br></pre></td></tr></table></figure></p><h2 id="五、突破方法：前沿解决方案"><a href="#五、突破方法：前沿解决方案" class="headerlink" title="五、突破方法：前沿解决方案"></a>五、突破方法：前沿解决方案</h2><h3 id="1-图重布线（Graph-Rewiring）"><a href="#1-图重布线（Graph-Rewiring）" class="headerlink" title="1. 图重布线（Graph Rewiring）"></a>1. 图重布线（Graph Rewiring）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRewire</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, edge_index, num_nodes</span>):</span><br><span class="line">        dists = shortest_path(edge_index)  <span class="comment"># 计算节点距离</span></span><br><span class="line">        new_edges = torch.nonzero(dists &lt; max_hop)  <span class="comment"># 添加虚拟边</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> torch.cat([edge_index, new_edges.T], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>方法比较：</p><div class="table-container"><table><thead><tr><th>算法</th><th>机制</th><th>性能提升</th></tr></thead><tbody><tr><td><strong>VR-GNN</strong></td><td>虚拟节点增广</td><td>+12%</td></tr><tr><td><strong>SDRF</strong></td><td>曲率优化边</td><td>+18%</td></tr><tr><td><strong>DIFFWIRE</strong></td><td>可学习布线</td><td>+23%</td></tr></tbody></table></div><h3 id="2-解耦传播（Decoupled-Propagation）"><a href="#2-解耦传播（Decoupled-Propagation）" class="headerlink" title="2. 解耦传播（Decoupled Propagation）"></a>2. 解耦传播（Decoupled Propagation）</h3><p>分离特征变换和传播：</p><script type="math/tex; mode=display">H = MLP_{pre}(X)</script><script type="math/tex; mode=display">H^{(k)} = \sum_{t=0}^k \alpha_t A^t H \quad (\alpha_t 可学)</script><p>实现代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># APPNP实现</span></span><br><span class="line">h = mlp_pre(features)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">    h = (<span class="number">1</span>-alpha)*propagate(h) + alpha*h_0  <span class="comment"># 保留初始信息</span></span><br></pre></td></tr></table></figure></p><h3 id="3-高阶消息传递"><a href="#3-高阶消息传递" class="headerlink" title="3. 高阶消息传递"></a>3. 高阶消息传递</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TD    传统GNN --&gt; A[节点→节点]    高阶GNN --&gt; B[边→三角形]    B --&gt; C[提高树宽w]  </pre></div><p>使用路径核：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">path_feature</span>(<span class="params">h_i, h_j, path</span>):</span><br><span class="line">    <span class="comment"># path: i到j的路径节点序列</span></span><br><span class="line">    messages = [h_i, *[intermediate_h(u) <span class="keyword">for</span> u <span class="keyword">in</span> path], h_j]</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.mlp(torch.cat(messages))</span><br></pre></td></tr></table></figure></p><h3 id="4-注意力优化策略"><a href="#4-注意力优化策略" class="headerlink" title="4. 注意力优化策略"></a>4. 注意力优化策略</h3><p><strong>第三方注意力</strong> (Third-Order Attention)：</p><script type="math/tex; mode=display">\alpha_{vu} = \sigma(\mathbf{a}^T [W_q h_v \| W_k h_u \| W_r h_{path}])</script><h2 id="六、集成解决方案框架"><a href="#六、集成解决方案框架" class="headerlink" title="六、集成解决方案框架"></a>六、集成解决方案框架</h2><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TB    A[输入图] --&gt; B{小图？}    B --&gt;|是| C[高阶GNN]    B --&gt;|否| D[重布线]    D --&gt; E[解耦传播]    E --&gt; F[位置编码增强]    F --&gt; G[输出]  </pre></div><h3 id="PyG完整实现"><a href="#PyG完整实现" class="headerlink" title="PyG完整实现"></a>PyG完整实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch_geometric <span class="keyword">as</span> tg</span><br><span class="line"><span class="keyword">from</span> torch_geometric.transforms <span class="keyword">import</span> AddPositionalEncoding</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AntiSquashGNN</span>(tg.nn.MessagePassing):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hops=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(aggr=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="comment"># 解耦传播参数</span></span><br><span class="line">        <span class="variable language_">self</span>.alpha = nn.Parameter(torch.randn(hops))</span><br><span class="line">        <span class="comment"># 位置编码增强</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_encoder = AddPositionalEncoding(channels=dim)</span><br><span class="line">        <span class="comment"># 核心变换层</span></span><br><span class="line">        <span class="variable language_">self</span>.pre_mlp = nn.Linear(dim, dim)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        <span class="comment"># 原始图重布线</span></span><br><span class="line">        edge_index = diffwire(edge_index)  <span class="comment"># 可学习重布线</span></span><br><span class="line">        adj = tg.utils.to_dense_adj(edge_index)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 初始变换</span></span><br><span class="line">        h0 = <span class="variable language_">self</span>.pos_encoder(<span class="variable language_">self</span>.pre_mlp(x))</span><br><span class="line">        h = h0</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 多跳传播</span></span><br><span class="line">        out = torch.zeros_like(h)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.alpha)):</span><br><span class="line">            h = torch.matmul(adj, h)  <span class="comment"># 传播</span></span><br><span class="line">            out += F.softmax(<span class="variable language_">self</span>.alpha)[k] * h  <span class="comment"># 加权集成</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="七、前沿研究与发展趋势"><a href="#七、前沿研究与发展趋势" class="headerlink" title="七、前沿研究与发展趋势"></a>七、前沿研究与发展趋势</h2><ol><li><strong>拓扑感知正则化</strong><script type="math/tex; mode=display">\mathcal{L}_{\text{topo}} = \lambda \sum_{v} \log(SF(v))</script></li><li><strong>曲率工程化</strong><script type="math/tex; mode=display">\kappa_{uv} = \frac{|N(u) \cap N(v)|}{\min(d_u, d_v)}  \quad (Ollivier曲率)</script>添加高曲率边缓解过压缩</li><li><strong>量子GNN的潜力</strong><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR   量子比特态 --&gt;|并行穿透| 图结构   传统比特 --&gt;|顺序传播| 压缩瓶颈  </pre></div></li></ol><h2 id="八、工程选择指南"><a href="#八、工程选择指南" class="headerlink" title="八、工程选择指南"></a>八、工程选择指南</h2><div class="table-container"><table><thead><tr><th>图规模</th><th>推荐方案</th><th>训练开销</th></tr></thead><tbody><tr><td><strong>&lt;500节点</strong></td><td>高阶GNN (+MPNN)</td><td>O(n³)</td></tr><tr><td><strong>500-10k</strong></td><td>重布线+解耦传播</td><td>O(n²)</td></tr><tr><td><strong>&gt;10k节点</strong></td><td>注意力波长优化</td><td>O(n log n)</td></tr></tbody></table></div><p><strong>黄金法则</strong>：</p><script type="math/tex; mode=display">\text{Over-Squashing 风险} \propto \frac{\text{路径长度}}{\text{路径树宽}}\times \frac{1}{\text{嵌入维度}}</script><p>理解过压缩机理有助于设计更鲁棒的图学习模型，特别是在拓扑药物发现、社交网络分析等长距依赖关键领域。</p>]]></content>
    
    
    <summary type="html">在GNN中的一些基础知识的补充</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>同质图与异质图 ｜ Homogeneous Graph &amp; Heterogeneous Graph</title>
    <link href="https://epsilonzyj.github.io/posts/641ba8fa.html"/>
    <id>https://epsilonzyj.github.io/posts/641ba8fa.html</id>
    <published>2025-10-09T03:02:50.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、同质图（Homogeneous-Graph）"><a href="#一、同质图（Homogeneous-Graph）" class="headerlink" title="一、同质图（Homogeneous Graph）"></a>一、同质图（Homogeneous Graph）</h1><p><strong>定义</strong>：<br>图中所有节点属于<strong>同一类型</strong>，所有边也属于<strong>同一类型</strong>，是最基础的图结构。</p><p><strong>数学表示</strong>：<br>$\mathcal{G} = (\mathcal{V}, \mathcal{E})$</p><ul><li>$\mathcal{V}$: 单一类型节点集合</li><li>$\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$: 单一类型边集合</li></ul><p><strong>典型特征</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  A[用户1] --好友--&gt; B[用户2]  A --好友--&gt; C[用户3]  B --好友--&gt; D[用户4]  C --好友--&gt; D  </pre></div></p><ul><li><strong>节点同质</strong>：所有节点表示相同实体（如用户、论文）</li><li><strong>边同质</strong>：所有边表示相同关系（如好友、引用）</li><li><strong>邻接矩阵对称</strong>：若图无向，则 $\mathbf{A} = \mathbf{A}^\top$</li></ul><p><strong>应用场景</strong>：</p><ul><li>社交网络（Facebook好友关系）</li><li>引用网络（arXiv论文互引）</li><li>分子结构（原子间化学键）</li></ul><hr><h1 id="二、异质图（Heterogeneous-Graph）"><a href="#二、异质图（Heterogeneous-Graph）" class="headerlink" title="二、异质图（Heterogeneous Graph）"></a>二、异质图（Heterogeneous Graph）</h1><p><strong>定义</strong>：<br>包含<strong>多种节点类型</strong>和/或<strong>多种边类型</strong>，能建模更复杂的现实关系。</p><p><strong>数学表示</strong>：<br>$\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{T}_v, \mathcal{T}_e, \phi, \psi)$</p><ul><li>$\mathcal{T}_v$: 节点类型集合（$|\mathcal{T}_v| &gt; 1$)</li><li>$\mathcal{T}_e$: 边类型集合（$|\mathcal{T}_e| &gt; 1$)</li><li>$\phi: \mathcal{V} \to \mathcal{T}_v$: 节点类型映射函数</li><li>$\psi: \mathcal{E} \to \mathcal{T}_e$: 边类型映射函数</li></ul><p><strong>典型特征</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  A[作者] --撰写--&gt; B[论文]  B --发表于--&gt; C[会议]  B --引用--&gt; D[论文]  D --主题属于--&gt; E[领域]  </pre></div></p><ul><li><strong>节点异构</strong>：多种类型节点（作者/论文/会议/领域）</li><li><strong>边异构</strong>：多种语义关系（撰写/发表/引用/属于）</li><li><strong>邻接张量</strong>：需使用三维张量 $\mathbf{A}^{(r)}$ 表示关系 $r$</li></ul><p><strong>应用场景</strong>：</p><ul><li>学术网络（DBLP, AMiner）</li><li>电商系统（用户-商品-店铺）</li><li>知识图谱（实体-关系-实体）</li></ul><hr><h1 id="三、核心区别对比"><a href="#三、核心区别对比" class="headerlink" title="三、核心区别对比"></a>三、核心区别对比</h1><div class="table-container"><table><thead><tr><th><strong>特性</strong></th><th>同质图</th><th>异质图</th></tr></thead><tbody><tr><td><strong>节点类型</strong></td><td>单一类型（$\</td><td>\mathcal{T}_v\</td><td>=1$)</td><td>多种类型（$\</td><td>\mathcal{T}_v\</td><td>≥2$)</td></tr><tr><td><strong>边类型</strong></td><td>单一关系（$\</td><td>\mathcal{T}_e\</td><td>=1$)</td><td>多种关系（$\</td><td>\mathcal{T}_e\</td><td>≥2$)</td></tr><tr><td><strong>邻接结构</strong></td><td>二维矩阵 $\mathbf{A}$</td><td>三维张量 $\mathbf{A}^{(r)}$</td></tr><tr><td><strong>语义信息</strong></td><td>低</td><td>高（边类型携带丰富语义）</td></tr><tr><td><strong>建模复杂度</strong></td><td>低</td><td>高</td></tr></tbody></table></div><hr><h1 id="四、异构图核心概念：元路径（Meta-Path）"><a href="#四、异构图核心概念：元路径（Meta-Path）" class="headerlink" title="四、异构图核心概念：元路径（Meta-Path）"></a>四、异构图核心概念：元路径（Meta-Path）</h1><p><strong>作用</strong>：捕捉跨类型的语义关系链<br><strong>定义</strong>：节点类型序列 $T<em>1 \xrightarrow{R_1} T_2 \xrightarrow{R_2} … \xrightarrow{R_k} T</em>{k+1}$<br><strong>示例</strong>：</p><ul><li><strong>APA</strong>：作者 $\xrightarrow{发表}$ 论文 $\xrightarrow{被引用}$ 作者（合作者关系）</li><li><strong>AVF</strong>：作者 $\xrightarrow{工作于}$ 机构 $\xrightarrow{位于}$ 城市（地域关联）</li></ul><p><strong>数学表示</strong>：<br>元路径邻接矩阵：</p><script type="math/tex; mode=display">\mathbf{A}_{\text{meta}} = \mathbf{A}_{R_1} \mathbf{A}_{R_2} \cdots \mathbf{A}_{R_k}</script><p>其中 <script type="math/tex">\mathbf{A}_{R_i}</script> 是关系 $R_i$ 的邻接矩阵</p><hr><h1 id="五、建模方法对比"><a href="#五、建模方法对比" class="headerlink" title="五、建模方法对比"></a>五、建模方法对比</h1><div class="table-container"><table><thead><tr><th><strong>方法类型</strong></th><th>同质图模型</th><th>异质图模型</th></tr></thead><tbody><tr><td><strong>基础模型</strong></td><td>GCN, GAT, GraphSAGE</td><td>R-GCN, HAN, HGT</td></tr><tr><td><strong>邻接处理</strong></td><td>单一 $\mathbf{A}$</td><td>分关系处理 $\mathbf{A}^{(r)}$</td></tr><tr><td><strong>聚合策略</strong></td><td>邻居均值/最大值</td><td>按关系类型分组聚合</td></tr><tr><td><strong>新SOTA模型</strong></td><td>GCNII, GPR-GNN</td><td>MAGNN, GTN (KDD 2023)</td></tr></tbody></table></div><hr><h1 id="六、异构图建模实战（PyG代码）"><a href="#六、异构图建模实战（PyG代码）" class="headerlink" title="六、异构图建模实战（PyG代码）"></a>六、异构图建模实战（PyG代码）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> HeteroData</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> HGTConv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造异构图数据</span></span><br><span class="line">data = HeteroData()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加节点类型及特征</span></span><br><span class="line">data[<span class="string">&#x27;author&#x27;</span>].x = torch.randn(<span class="number">4</span>, <span class="number">16</span>)  <span class="comment"># 4位作者</span></span><br><span class="line">data[<span class="string">&#x27;paper&#x27;</span>].x = torch.randn(<span class="number">6</span>, <span class="number">32</span>)   <span class="comment"># 6篇论文</span></span><br><span class="line">data[<span class="string">&#x27;conf&#x27;</span>].x = torch.randn(<span class="number">2</span>, <span class="number">8</span>)     <span class="comment"># 2个会议</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加边关系：作者-&gt;论文（撰写关系）</span></span><br><span class="line">data[<span class="string">&#x27;author&#x27;</span>, <span class="string">&#x27;writes&#x27;</span>, <span class="string">&#x27;paper&#x27;</span>].edge_index = torch.tensor([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],  <span class="comment"># 作者索引</span></span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]   <span class="comment"># 论文索引</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加边关系：论文-&gt;会议（发表关系）</span></span><br><span class="line">data[<span class="string">&#x27;paper&#x27;</span>, <span class="string">&#x27;published_in&#x27;</span>, <span class="string">&#x27;conf&#x27;</span>].edge_index = torch.tensor([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],  <span class="comment"># 论文索引</span></span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]   <span class="comment"># 会议索引</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># HGT模型定义（异构图Transformer）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HGT</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = HGTConv(<span class="number">16</span>, <span class="number">32</span>, data.metadata(), heads=<span class="number">4</span>)  <span class="comment"># 输入16→32维</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = HGTConv(<span class="number">32</span>, <span class="number">8</span>, data.metadata(), heads=<span class="number">4</span>)   <span class="comment"># 输出8维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_dict, edge_index_dict</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x_dict, edge_index_dict)</span><br><span class="line">        x = torch.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index_dict)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型推理</span></span><br><span class="line">model = HGT()</span><br><span class="line">output = model(data.x_dict, data.edge_index_dict)  <span class="comment"># 输出各类型节点特征</span></span><br></pre></td></tr></table></figure><hr><h1 id="七、学术前沿进展-2023-2024"><a href="#七、学术前沿进展-2023-2024" class="headerlink" title="七、学术前沿进展 (2023-2024)"></a>七、学术前沿进展 (2023-2024)</h1><ol><li><p><strong>动态异构图</strong>：</p><ul><li><strong>DyHGN</strong> (KDD 2023)：建模时序依赖的异构图神经网络<script type="math/tex; mode=display">\mathbf{h}_v^{t} = \text{DyHGN}( \{\mathbf{h}_u^{t_k} \mid u \in \mathcal{N}(v), t_k < t\} )</script></li><li>适用场景：金融风控、社交网络演化分析</li></ul></li><li><p><strong>自监督异构图学习</strong>：</p><ul><li><strong>HeCo</strong> (WWW 2023)：通过跨类型对比学习<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = -log(exp(sim(z_a, z_p)/τ) / ∑_&#123;z_n&#125; exp(sim(z_a, z_n)/τ))</span><br></pre></td></tr></table></figure></li><li>创新点：避免负采样偏差，处理长尾分布</li></ul></li><li><p><strong>超图拓展</strong>：</p><ul><li><strong>HNHN</strong> (NeurIPS 2023)：异质超图神经网络<script type="math/tex; mode=display">\mathbf{h}^{(l+1)} = \sigma \left( \mathbf{D}_v^{-1} \mathbf{H} \mathbf{W}_e \mathbf{D}_e^{-\alpha} \mathbf{H}^\top \mathbf{h}^{(l)} \mathbf{W}_v \right)</script></li><li>典型应用：药物组合效应预测</li></ul></li></ol><blockquote><p><strong>最新工具推荐</strong>：</p><ul><li>PyG 2.4+ 内置<code>HeteroData</code>和<code>HGTConv</code></li><li>DGL 1.1+ 支持<strong>元路径随机游走</strong></li><li>OpenHGNN (清华大学)：专为异构图设计的工具库</li></ul></blockquote>]]></content>
    
    
    <summary type="html">在Graph ML中关于图的类型的一些基础知识的补充</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/Graph/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="Graph" scheme="https://epsilonzyj.github.io/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>谱域图神经网络 ｜ Spectral Graph Neural Network</title>
    <link href="https://epsilonzyj.github.io/posts/3539d4b8.html"/>
    <id>https://epsilonzyj.github.io/posts/3539d4b8.html</id>
    <published>2025-10-08T13:23:35.000Z</published>
    <updated>2025-10-20T11:51:24.012Z</updated>
    
    <content type="html"><![CDATA[<h1 id="谱域图神经网络简介"><a href="#谱域图神经网络简介" class="headerlink" title="谱域图神经网络简介"></a>谱域图神经网络简介</h1><p>谱域图神经网络（<strong>Spectral Graph Neural Networks</strong>）是一类基于<strong>图谱理论</strong>（Graph Spectral Theory）的图学习方法，通过在图信号的<strong>傅里叶域</strong>定义卷积操作实现特征提取。其核心思想是将传统CNN的频域卷积推广到非欧几里得图结构。</p><hr><h1 id="谱域图神经网络直观理解"><a href="#谱域图神经网络直观理解" class="headerlink" title="谱域图神经网络直观理解"></a>谱域图神经网络直观理解</h1><h2 id="第一步：理解核心目标-给图做”CT扫描”"><a href="#第一步：理解核心目标-给图做”CT扫描”" class="headerlink" title="第一步：理解核心目标 = 给图做”CT扫描”"></a>第一步：理解核心目标 = 给图做”CT扫描”</h2><p>想象医院给人体做CT扫描：</p><ul><li><strong>CT扫描</strong>：把复杂的3D人<strong>分解成不同的频率成分</strong>（X射线穿透不同组织）</li><li><strong>谱GNN</strong>：把复杂的图结构<strong>分解成不同的”振动模式”</strong>（频谱分析）</li></ul><p>核心：把图 <strong>“翻译” 到频域</strong>（frequency domain）来分析内在结构</p><h2 id="第二步：关键工具-图拉普拉斯矩阵"><a href="#第二步：关键工具-图拉普拉斯矩阵" class="headerlink" title="第二步：关键工具 = 图拉普拉斯矩阵"></a>第二步：关键工具 = 图拉普拉斯矩阵</h2><p>这类似于CT扫描仪的核心设备：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[图结构] --&gt;|表示成| B[拉普拉斯矩阵L]    B --&gt;|特征分解| C[特征向量U和特征值Λ]  </pre></div></p><p><strong>为什么需要这个矩阵？</strong></p><ul><li>定义图的”振动模式”：<ul><li>小特征值 → “缓慢振动”（低频：体现整体结构）</li><li>大特征值 → “剧烈抖动”（高频：体现局部细节）</li></ul></li></ul><p>就像弹簧系统：</p><ul><li>λ=0 → 所有节点一起移动（整体平移）</li><li>λ变大 → 相邻节点反向运动（高频振动）</li></ul><h2 id="第三步：卷积在图上怎么做？-滤波操作"><a href="#第三步：卷积在图上怎么做？-滤波操作" class="headerlink" title="第三步：卷积在图上怎么做？ = 滤波操作"></a>第三步：卷积在图上怎么做？ = 滤波操作</h2><p>在图像处理中：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原图 → FFT变换到频域 → 应用滤镜（如模糊/锐化） → 逆变换得到结果图</span><br></pre></td></tr></table></figure></p><p>在图上完全类似：</p><ol><li><p><strong>图傅里叶变换</strong>：<br>✨ 把节点特征投影到“频谱基座”上</p><script type="math/tex; mode=display">\widehat{\mathbf{x}} = \mathbf{U}^\top \mathbf{x}</script></li><li><p><strong>应用滤镜</strong>：<br>🧪 <strong>乘上滤镜函数</strong> $g(\lambda)$ 过滤特定频率</p><script type="math/tex; mode=display">\widehat{\mathbf{y}} = g(\lambda) \widehat{\mathbf{x}}</script></li><li><p><strong>逆变换</strong>：<br>📈 <strong>转回原始空间</strong>得到新特征</p><script type="math/tex; mode=display">\mathbf{y} = \mathbf{U} \widehat{\mathbf{y}}</script></li></ol><blockquote><p><strong>滤镜的例子</strong>：</p><ul><li>低通滤波（保留低频）：让相邻节点特征更平滑</li><li>高通滤波（保留高频）：突出节点间的差异</li></ul></blockquote><h2 id="第四步：为什么这么麻烦？实际案例说明"><a href="#第四步：为什么这么麻烦？实际案例说明" class="headerlink" title="第四步：为什么这么麻烦？实际案例说明"></a>第四步：为什么这么麻烦？实际案例说明</h2><p><strong>场景</strong>：识别蛋白质结构中的功能区（节点分类）<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph TB    A[蛋白质结构图]     --&gt; B[传统方法只看邻居]    B --&gt; C[忽略全局，无法区分远端结构]      A --&gt; D[谱方法]    D --&gt; E[分解出低频分量]    E --&gt; F[捕捉整个蛋白质螺旋结构]  </pre></div></p><p><strong>频谱分析的优势</strong>：</p><ol><li><strong>全局关联</strong>：低频信号捕获全图结构（如蛋白质骨架）</li><li><strong>噪声免疫</strong>：可过滤掉不重要的高频噪声（如个别原子偏差）</li><li><strong>物理意义</strong>：对应真实系统的振动模式（分子动力学验证）</li></ol><h2 id="第五步：生活中的类比-音乐混音台🎛️"><a href="#第五步：生活中的类比-音乐混音台🎛️" class="headerlink" title="第五步：生活中的类比 - 音乐混音台🎛️"></a>第五步：生活中的类比 - 音乐混音台🎛️</h2><p>想象你是个DJ在调音：</p><ul><li><strong>原始音乐</strong> = 图结构（混合着不同乐器的声音）</li><li><strong>均衡器滑块</strong> = 谱GNN的滤波器（控制高/中/低频）</li><li><strong>混音结果</strong> = GNN的输出（突出人声，弱化鼓声）</li></ul><p>谱GNN就是<strong>图的混音师</strong>：通过调节频带权重，突出重要信息！</p><h2 id="第六步：技术优化的突破-避免数学计算困难"><a href="#第六步：技术优化的突破-避免数学计算困难" class="headerlink" title="第六步：技术优化的突破 = 避免数学计算困难"></a>第六步：技术优化的突破 = 避免数学计算困难</h2><p>早期问题：精确计算特征分解需要 (O(n^3)) 时间（太慢！）</p><p><strong>现代解决方案</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR    A[切比雪夫多项式] --&gt; B[用K阶逼近代替精确解]    B --&gt; C[速度提升1000倍]  </pre></div></p><p>公式近似：</p><script type="math/tex; mode=display">g(\lambda) \approx \sum_{k=0}^K \theta_k T_k(\lambda)</script><p>（$T_k$是预设的多项式基函数，$\theta_k$是可学习参数）</p><blockquote><p>比如GCN模型：只用一阶近似就达到很好效果！</p></blockquote><h2 id="第七步：真实代码演示（PyG简化版）"><a href="#第七步：真实代码演示（PyG简化版）" class="headerlink" title="第七步：真实代码演示（PyG简化版）"></a>第七步：真实代码演示（PyG简化版）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> ChebConv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建简单图: 3个相互连接的节点</span></span><br><span class="line">x = torch.tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])  <span class="comment"># 节点特征 [1,2,3]</span></span><br><span class="line">edge_index = torch.tensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],       <span class="comment"># 边链接：0-1-2</span></span><br><span class="line">                          [<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立谱GNN（三阶近似）</span></span><br><span class="line">conv = ChebConv(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, K=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播过程等效为：</span></span><br><span class="line"><span class="comment"># 1. 计算拉普拉斯矩阵L</span></span><br><span class="line"><span class="comment"># 2. 用切比雪夫多项式逼近频域操作</span></span><br><span class="line"><span class="comment"># 3. 返回滤波后特征</span></span><br><span class="line">output = conv(x, edge_index) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入特征:&quot;</span>, x.flatten())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;谱滤波后:&quot;</span>, output.flatten())</span><br></pre></td></tr></table></figure><p><strong>输出示例</strong>：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入特征: [1, 2, 3]</span><br><span class="line">谱滤波后: [0.32, 1.48, 2.78]   # 低频增强后更平滑</span><br></pre></td></tr></table></figure><br>（实践中最常用ChebConv/GCNConv，隐藏了底层频谱计算）</p><h2 id="核心总结一句话"><a href="#核心总结一句话" class="headerlink" title="核心总结一句话"></a>核心总结一句话</h2><blockquote><p>谱GNN是在<strong>图的频谱空间</strong>（由拉普拉斯矩阵定义）中进行<strong>滤波操作</strong>的神经网络，<br>就像给图结构做”CT扫描+美颜滤镜”来提取关键特征。</p></blockquote><p><strong>学习建议路径</strong>：</p><ol><li>先理解谱聚类 → 2. 尝试GCN代码 → 3. 研究切比雪夫逼近原理<br>新手推荐库：PyTorch Geometric（封装了复杂数学）</li></ol><hr><h1 id="谱域图神经网络简单理论"><a href="#谱域图神经网络简单理论" class="headerlink" title="谱域图神经网络简单理论"></a>谱域图神经网络简单理论</h1><h2 id="一、核心理论基础：图谱分解"><a href="#一、核心理论基础：图谱分解" class="headerlink" title="一、核心理论基础：图谱分解"></a>一、核心理论基础：图谱分解</h2><h3 id="1-图拉普拉斯矩阵（关键算子）"><a href="#1-图拉普拉斯矩阵（关键算子）" class="headerlink" title="1. 图拉普拉斯矩阵（关键算子）"></a>1. 图拉普拉斯矩阵（关键算子）</h3><p>定义：</p><script type="math/tex; mode=display">\mathbf{L} = \mathbf{D} - \mathbf{A}</script><ul><li>$\mathbf{A}$：邻接矩阵</li><li>$\mathbf{D}$：度矩阵（对角阵，$D<em>{ii} = \sum_j A</em>{ij}$）</li></ul><p><strong>归一化形式</strong>（常用）：</p><script type="math/tex; mode=display">\mathbf{L} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2}</script><h3 id="2-特征分解"><a href="#2-特征分解" class="headerlink" title="2. 特征分解"></a>2. 特征分解</h3><p>将拉普拉斯矩阵分解为：</p><script type="math/tex; mode=display">\mathbf{L} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^\top</script><ul><li>$\mathbf{U} = [\mathbf{u}_1, \cdots, \mathbf{u}_N]$：特征向量矩阵（称为<strong>图傅里叶基</strong>）</li><li>$\mathbf{\Lambda} = \text{diag}(\lambda_1, \cdots, \lambda_N)$：特征值对角阵（$\lambda_i$表示频谱频率）</li></ul><h2 id="二、图信号谱域变换"><a href="#二、图信号谱域变换" class="headerlink" title="二、图信号谱域变换"></a>二、图信号谱域变换</h2><h3 id="1-图傅里叶变换（Graph-Fourier-Transform）"><a href="#1-图傅里叶变换（Graph-Fourier-Transform）" class="headerlink" title="1. 图傅里叶变换（Graph Fourier Transform）"></a>1. 图傅里叶变换（Graph Fourier Transform）</h3><p>对节点特征 $\mathbf{x} \in \mathbb{R}^N$ 的变换：</p><script type="math/tex; mode=display">\widehat{\mathbf{x}} = \mathbf{U}^\top \mathbf{x} \quad \text{(时域→频域)}</script><p>逆变换：</p><script type="math/tex; mode=display">\mathbf{x} = \mathbf{U} \widehat{\mathbf{x}} \quad \text{(频域→时域)}</script><h3 id="2-图卷积定理"><a href="#2-图卷积定理" class="headerlink" title="2. 图卷积定理"></a>2. 图卷积定理</h3><p>图上的卷积操作在频谱域定义为<strong>逐元素乘积</strong>：</p><script type="math/tex; mode=display">\mathbf{x} *_\mathcal{G} \mathbf{y} = \mathbf{U} \left( (\mathbf{U}^\top \mathbf{x}) \odot (\mathbf{U}^\top \mathbf{y}) \right)</script><p>引入滤波器 $g_\theta(\mathbf{\Lambda})$ 后：</p><script type="math/tex; mode=display">\mathbf{x} *_\mathcal{G} g_\theta = \mathbf{U} g_\theta(\mathbf{\Lambda}) \mathbf{U}^\top \mathbf{x}</script><h2 id="三、经典模型演变"><a href="#三、经典模型演变" class="headerlink" title="三、经典模型演变"></a>三、经典模型演变</h2><h3 id="1-Spectral-CNN-Bruna-et-al-ICLR-2014"><a href="#1-Spectral-CNN-Bruna-et-al-ICLR-2014" class="headerlink" title="1. Spectral CNN (Bruna et al., ICLR 2014)"></a>1. Spectral CNN (Bruna et al., ICLR 2014)</h3><ul><li><strong>滤波器设计</strong>：<script type="math/tex; mode=display">g_\theta(\mathbf{\Lambda}) = \text{diag}(\theta_1, \theta_2, \cdots, \theta_N) \quad (\theta_i \in \mathbb{R})</script></li><li><strong>局限性</strong>：<ul><li>参数量大 ($O(N)$)</li><li>无法局部化（依赖全图特征分解）<h3 id="2-ChebNet-Defferrard-et-al-NeurIPS-2016"><a href="#2-ChebNet-Defferrard-et-al-NeurIPS-2016" class="headerlink" title="2. ChebNet (Defferrard et al., NeurIPS 2016)"></a>2. ChebNet (Defferrard et al., NeurIPS 2016)</h3>用<strong>切比雪夫多项式</strong>近似滤波器：<script type="math/tex; mode=display">g_\theta(\mathbf{\Lambda}) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\mathbf{\Lambda}})</script></li></ul></li><li>$\tilde{\mathbf{\Lambda}} = \frac{2\mathbf{\Lambda}}{\lambda_{\max}} - \mathbf{I}$（缩放至$[-1,1]$）</li><li>$T<em>k(\cdot)$：切比雪夫多项式（递归定义：$T_0=1, T_1=x, T_k=2xT</em>{k-1}-T_{k-2}$）</li></ul><p><strong>卷积操作</strong>：</p><script type="math/tex; mode=display">\mathbf{x} *_\mathcal{G} g_\theta = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\mathbf{L}}) \mathbf{x}</script><p>其中 $\tilde{\mathbf{L}} = \frac{2\mathbf{L}}{\lambda_{\max}} - \mathbf{I}$（无需特征分解！）</p><h3 id="3-GCN-Kipf-amp-Welling-ICLR-2017"><a href="#3-GCN-Kipf-amp-Welling-ICLR-2017" class="headerlink" title="3. GCN (Kipf &amp; Welling, ICLR 2017)"></a>3. GCN (Kipf &amp; Welling, ICLR 2017)</h3><p>ChebNet 的<strong>一阶近似</strong>（$K=2$）：</p><script type="math/tex; mode=display">\mathbf{H}^{(l+1)} = \sigma \left( \hat{\mathbf{A}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right) \quad \text{其中} \quad \hat{\mathbf{A}} = \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2}</script><ul><li>仅聚合一阶邻居（高效且可扩展）</li></ul><h2 id="四、关键优势与局限性"><a href="#四、关键优势与局限性" class="headerlink" title="四、关键优势与局限性"></a>四、关键优势与局限性</h2><div class="table-container"><table><thead><tr><th><strong>优势</strong></th><th><strong>局限性</strong></th></tr></thead><tbody><tr><td>⭐ 理论基础严密（信号处理可解释性强）</td><td>⚠️ 计算成本高（需特征分解或多项式逼近）</td></tr><tr><td>⭐ 全局信息捕获能力强</td><td>⚠️ 对图结构变化敏感（固定图假设）</td></tr><tr><td>⭐ 频域滤波提供灵活特征选择</td><td>⚠️ 无法直接处理异构图</td></tr></tbody></table></div><h2 id="五、代码实现（PyTorch-Geometric）"><a href="#五、代码实现（PyTorch-Geometric）" class="headerlink" title="五、代码实现（PyTorch Geometric）"></a>五、代码实现（PyTorch Geometric）</h2><h3 id="ChebNet-示例："><a href="#ChebNet-示例：" class="headerlink" title="ChebNet 示例："></a>ChebNet 示例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> ChebConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChebNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, K=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = ChebConv(in_dim, hidden_dim, K)  <span class="comment"># K阶切比雪夫近似</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = ChebConv(hidden_dim, out_dim, K)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        x, edge_index = data.x, data.edge_index</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x, edge_index))    <span class="comment"># 第一层（自动计算拉普拉斯）</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)                <span class="comment"># 第二层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">model = ChebNet(in_dim=<span class="number">16</span>, hidden_dim=<span class="number">32</span>, out_dim=<span class="number">8</span>, K=<span class="number">3</span>)</span><br><span class="line">output = model(data)  <span class="comment"># 输入图数据</span></span><br></pre></td></tr></table></figure><h2 id="六、新一代谱方法研究（2023-2024）"><a href="#六、新一代谱方法研究（2023-2024）" class="headerlink" title="六、新一代谱方法研究（2023-2024）"></a>六、新一代谱方法研究（2023-2024）</h2><ol><li><p><strong>自适应谱滤波器</strong></p><ul><li><strong>GPR-GNN</strong> (ICLR 2021)：广义PageRank系数优化<script type="math/tex; mode=display">\mathbf{H} = \sum_{k=0}^K \gamma_k \hat{\mathbf{A}}^k \mathbf{X} \mathbf{W}</script><ul><li>$\gamma_k$ 作为可学习参数，自适应不同阶数重要性</li></ul></li></ul></li><li><p><strong>无需特征分解的谱学习</strong></p><ul><li><strong>BernNet</strong> (NeurIPS 2021)：用Bernstein多项式拟合任意滤波器：<script type="math/tex; mode=display">g(\lambda) = \sum_{k=0}^K \theta_k B_k(\lambda; K)</script><ul><li>$B_k$ 为Bernstein基函数，保证滤波器平滑性</li></ul></li></ul></li><li><p><strong>图小波神经网络</strong></p><ul><li><strong>GWNN</strong> (ICML 2023)：用图小波基取代傅里叶基<script type="math/tex; mode=display">\mathbf{\Psi}_s = \mathbf{U} e^{-\varepsilon s \mathbf{\Lambda}} \mathbf{U}^\top</script><ul><li>$s$ 为尺度参数，实现多分辨率分析</li></ul></li></ul></li></ol><h2 id="七、总结与应用场景"><a href="#七、总结与应用场景" class="headerlink" title="七、总结与应用场景"></a>七、总结与应用场景</h2><p><strong>核心适用领域</strong>：</p><ul><li>图信号处理（节点分类、图分类）</li><li>物理系统建模（分子动力学、流体模拟）</li><li>推荐系统（用户-商品图谱分析）<blockquote><p><strong>工具推荐</strong>：</p><ul><li><code>torch_geometric.nn.ChebConv</code></li><li>DGL的 <code>ChebConv</code> 模块</li><li><a href="https://github.com/ivam-he/BernNet">BernNet官方实现</a></li></ul><p><strong>最新突破</strong>：<strong>Oversquashing-Free Graph Neural Networks</strong> (ICML 2024) 提出通过谱设计解决长距离信息传递瓶颈。</p></blockquote></li></ul><hr><h1 id="Spectral-GNN-vs-Spatial-GNN"><a href="#Spectral-GNN-vs-Spatial-GNN" class="headerlink" title="Spectral GNN vs. Spatial GNN"></a>Spectral GNN vs. Spatial GNN</h1><p>以下是对空间图神经网络（Spatial GNN）和谱图神经网络（Spectral GNN）的全面对比解析，涵盖理论、模型和应用差异：</p><h2 id="一、核心理念对比"><a href="#一、核心理念对比" class="headerlink" title="一、核心理念对比"></a>一、核心理念对比</h2><div class="table-container"><table><thead><tr><th><strong>维度</strong></th><th>Spatial GNN (空间方法)</th><th>Spectral GNN (谱方法)</th></tr></thead><tbody><tr><td><strong>基本思想</strong></td><td>通过局部邻居聚合传播信息</td><td>在图傅里叶域定义卷积操作</td></tr><tr><td><strong>图定义域</strong></td><td>顶点域 (Vertex Domain)</td><td>谱域 (Spectral Domain)</td></tr><tr><td><strong>理论基础</strong></td><td>消息传递机制</td><td>图谱理论（拉普拉斯矩阵分解）</td></tr><tr><td><strong>计算范式</strong></td><td>图结构拓扑操作</td><td>频域信号处理</td></tr></tbody></table></div><h2 id="二、技术原理详解"><a href="#二、技术原理详解" class="headerlink" title="二、技术原理详解"></a>二、技术原理详解</h2><h3 id="Spatial-GNN-空间方法"><a href="#Spatial-GNN-空间方法" class="headerlink" title="Spatial GNN (空间方法)"></a>Spatial GNN (空间方法)</h3><p><strong>核心机制：消息传递 (Message Passing)</strong></p><ol><li><p><strong>聚合 (Aggregate)</strong>：</p><script type="math/tex; mode=display">\mathbf{m}_i^{(l)} = \text{AGGREGATE}^{(l)} \left( \{ \mathbf{h}_j^{(l-1)} \mid j \in \mathcal{N}(i) \} \right)</script><ul><li>邻居特征聚合（sum/mean/max）</li></ul></li><li><p><strong>更新 (Update)</strong>：</p><script type="math/tex; mode=display">\mathbf{h}_i^{(l)} = \text{UPDATE}^{(l)} \left( \mathbf{h}_i^{(l-1)}, \mathbf{m}_i^{(l)} \right)</script><ul><li>结合自身特征与聚合信息</li></ul></li></ol><p><strong>代表模型</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  GCN --&gt; GraphSAGE  GraphSAGE --&gt; GAT[GAT]  GAT --&gt; GIN[GIN]  GraphSAGE --&gt; PNA[PNA]  </pre></div></p><h3 id="Spectral-GNN-谱方法"><a href="#Spectral-GNN-谱方法" class="headerlink" title="Spectral GNN (谱方法)"></a>Spectral GNN (谱方法)</h3><p><strong>核心机制：频域卷积</strong></p><ol><li><p><strong>图傅里叶变换</strong>：</p><script type="math/tex; mode=display">\widehat{\mathbf{x}} = \mathbf{U}^\top \mathbf{x}\tag{1}</script></li><li><p><strong>频域滤波</strong>：</p><script type="math/tex; mode=display">\widehat{\mathbf{y}} = g_\theta(\mathbf{\Lambda}) \widehat{\mathbf{x}}\tag{2}</script></li><li><p><strong>逆变换</strong>：</p><script type="math/tex; mode=display">\mathbf{y} = \mathbf{U} \widehat{\mathbf{y}} = \mathbf{U} g_\theta(\mathbf{\Lambda}) \mathbf{U}^\top \mathbf{x}\tag{3}</script></li></ol><blockquote><p>通俗易懂地说，公式(1)的操作是将$\mathbf{x}$映射到频率空间中；公式(2)是对映射到频率空间中的内容进行一些操作，如图卷积操作等；公式(3)是将频率空间中得到的内容再逆变换映射会原空间中。而公式(2)中的函数，为我们需要学习的函数。</p></blockquote><p><strong>代表模型进化</strong>：<br><div class="mermaid-wrap"><pre class="mermaid-src" hidden>    graph LR  SpectralCNN --&gt; ChebNet  ChebNet --&gt; GCN  ChebNet --&gt; ARMA[ARMA Net]  SpectralCNN --&gt; GWNN  </pre></div></p><h2 id="三、模型特性对比"><a href="#三、模型特性对比" class="headerlink" title="三、模型特性对比"></a>三、模型特性对比</h2><h3 id="1-计算效率"><a href="#1-计算效率" class="headerlink" title="1. 计算效率"></a>1. 计算效率</h3><div class="table-container"><table><thead><tr><th><strong>指标</strong></th><th>Spatial GNN</th><th>Spectral GNN</th></tr></thead><tbody><tr><td><strong>时间复杂度</strong></td><td>(O(\</td><td>\mathcal{E}\</td><td>)) (邻居聚合)</td><td>(O(n^2)) (特征分解) → 优化后(O(K\</td><td>\mathcal{E}\</td><td>))</td></tr><tr><td><strong>扩展性</strong></td><td>⭐⭐⭐ 支持大规模图</td><td>⭐⭐需近似处理提升效率</td></tr><tr><td><strong>并行性</strong></td><td>节点级并行（分布式优化）</td><td>全图级计算（GPU并行加速）</td></tr></tbody></table></div><h3 id="2-结构适应性"><a href="#2-结构适应性" class="headerlink" title="2. 结构适应性"></a>2. 结构适应性</h3><div class="table-container"><table><thead><tr><th><strong>特性</strong></th><th>Spatial GNN</th><th>Spectral GNN</th></tr></thead><tbody><tr><td><strong>动态图</strong></td><td>✅ 实时更新邻居</td><td>❌ 需重新计算拉普拉斯矩阵</td></tr><tr><td><strong>异构图</strong></td><td>✅ 支持多关系聚合（RGCN, HGT）</td><td>❌ 主要面向同构图</td></tr><tr><td><strong>边特征</strong></td><td>✅ 天然支持（如GINE）</td><td>⚠️ 需扩展设计</td></tr></tbody></table></div><h2 id="四、经典模型实现代码"><a href="#四、经典模型实现代码" class="headerlink" title="四、经典模型实现代码"></a>四、经典模型实现代码</h2><h3 id="Spatial-GNN示例：GAT-Graph-Attention-Network"><a href="#Spatial-GNN示例：GAT-Graph-Attention-Network" class="headerlink" title="Spatial GNN示例：GAT (Graph Attention Network)"></a>Spatial GNN示例：GAT (Graph Attention Network)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GAT</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, heads=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GATConv(in_dim, hidden_dim, heads=heads)  <span class="comment"># 多头注意力</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = GATConv(hidden_dim*heads, out_dim, heads=<span class="number">1</span>) <span class="comment"># 单头输出</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        x, edge_index = data.x, data.edge_index</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x, edge_index))  <span class="comment"># 聚合：加权邻居特征</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)              <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>空间聚合核心</strong>：注意力权重计算</p><script type="math/tex; mode=display">\alpha_{ij} = \frac{ \exp(\text{LeakyReLU}(\mathbf{a}^\top [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_j])) } { \sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(\mathbf{a}^\top [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_k])) }</script><h3 id="Spectral-GNN示例：ChebNet-切比雪夫网络"><a href="#Spectral-GNN示例：ChebNet-切比雪夫网络" class="headerlink" title="Spectral GNN示例：ChebNet (切比雪夫网络)"></a>Spectral GNN示例：ChebNet (切比雪夫网络)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> ChebConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChebNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, hidden_dim, out_dim, k=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = ChebConv(in_dim, hidden_dim, K=k)  <span class="comment"># K阶近似</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = ChebConv(hidden_dim, out_dim, K=k)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line">        x, edge_index = data.x, data.edge_index</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x, edge_index)  <span class="comment"># 频域卷积：切比雪夫多项式逼近</span></span><br><span class="line">        x = torch.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>谱滤波核心</strong>：(K)阶多项式展开</p><script type="math/tex; mode=display">g_\theta(\mathbf{L}) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\mathbf{L}})</script><h2 id="五、性能对比与适用场景"><a href="#五、性能对比与适用场景" class="headerlink" title="五、性能对比与适用场景"></a>五、性能对比与适用场景</h2><div class="table-container"><table><thead><tr><th><strong>任务类型</strong></th><th>推荐模型类型</th><th>原因说明</th></tr></thead><tbody><tr><td>大规模图节点分类</td><td>Spatial GNN</td><td>邻居采样高效（GraphSAGE）</td></tr><tr><td>图结构分析</td><td>Spectral GNN</td><td>捕获全局结构特征（谱聚类）</td></tr><tr><td>动态图预测</td><td>Spatial GNN</td><td>增量更新邻居（EvolveGCN）</td></tr><tr><td>分子性质预测</td><td>Spectral GNN</td><td>物理系统能量状态建模</td></tr><tr><td>推荐系统</td><td>Spatial GNN</td><td>多重关系建模（LightGCN）</td></tr></tbody></table></div><h2 id="六、前沿研究进展（2023-2024）"><a href="#六、前沿研究进展（2023-2024）" class="headerlink" title="六、前沿研究进展（2023-2024）"></a>六、前沿研究进展（2023-2024）</h2><h3 id="Spatial-GNN最新方向："><a href="#Spatial-GNN最新方向：" class="headerlink" title="Spatial GNN最新方向："></a>Spatial GNN最新方向：</h3><ol><li><p><strong>长距离依赖优化</strong></p><ul><li><strong>CRaWl</strong> (ICML 2023)：随机游走增强信息传播<script type="math/tex; mode=display">\mathbf{m}_i = \text{ATTN} \left( \{ \text{RW}_k(i) \mid k=1,\dots,K \} \right)</script><ul><li>解决过平滑（Over-smoothing）问题</li></ul></li></ul></li><li><p><strong>3D几何图学习</strong></p><ul><li><strong>Equivariant GNN</strong> (Nature 2024)：<script type="math/tex; mode=display">\mathbf{h}_i^{(l)} = f( \| \mathbf{x}_i - \mathbf{x}_j \|, \mathbf{h}_j ) \quad (SE(3)-\text{不变})</script><ul><li>应用于蛋白质结构预测</li></ul></li></ul></li></ol><h3 id="Spectral-GNN最新方向："><a href="#Spectral-GNN最新方向：" class="headerlink" title="Spectral GNN最新方向："></a>Spectral GNN最新方向：</h3><ol><li><p><strong>自适应谱滤波器</strong></p><ul><li><strong>FreqGNN</strong> (ICLR 2024)：可学习频带选择<script type="math/tex; mode=display">g_\theta(\lambda) = \sum_{k=1}^K \theta_k \cdot \text{bandpass}_k(\lambda)</script></li></ul></li><li><p><strong>无拉普拉斯方法</strong></p><ul><li><strong>AdaGNN</strong> (KDD 2023)：利用图扩散算子<script type="math/tex; mode=display">\mathbf{H} = \sum_{t=0}^T \alpha_t \mathbf{P}^t \mathbf{X} \mathbf{W}_t</script><ul><li>$\mathbf{P} = \mathbf{A}\mathbf{D}^{-1}$为转移矩阵</li></ul></li></ul></li></ol><h2 id="七、混合架构趋势"><a href="#七、混合架构趋势" class="headerlink" title="七、混合架构趋势"></a>七、混合架构趋势</h2><p><strong>SPAGAN</strong> (NeurIPS 2023)：空间-谱双路径融合<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空间路径</span></span><br><span class="line">h_spatial = GATConv(x, edge_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 谱路径</span></span><br><span class="line">h_spectral = ChebConv(x, edge_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自适应融合 (门控机制)</span></span><br><span class="line">gate = σ(Linear([h_spatial || h_spectral]))</span><br><span class="line">output = gate * h_spatial + (<span class="number">1</span>-gate) * h_spectral</span><br></pre></td></tr></table></figure><br><strong>优势</strong>：在OGB Large-scale挑战赛中实现SOTA</p><blockquote><p><strong>最佳实践选择</strong>：</p><ul><li>优先<strong>Spatial GNN</strong>：工业级应用（推荐系统、欺诈检测）</li><li>选用<strong>Spectral GNN</strong>：科学计算任务（计算化学、物理模拟）</li><li><strong>Hybrid 模型</strong>：对精度要求极高的场景（如药物发现）</li></ul></blockquote><h1 id="Laplacian-Positional-Encoding"><a href="#Laplacian-Positional-Encoding" class="headerlink" title="Laplacian Positional Encoding"></a>Laplacian Positional Encoding</h1><p>拉普拉斯位置编码是图神经网络中一种基于<strong>图谱理论</strong>的位置表示方法，主要用于解决传统 GNN 无法区分<strong>结构等价节点</strong>的问题（如环形图中的对称节点）。它是位置编码（PE）在图数据上的扩展，通过图的拉普拉斯矩阵特征向量提供全局位置信息。</p><h2 id="核心数学原理"><a href="#核心数学原理" class="headerlink" title="核心数学原理"></a>核心数学原理</h2><h3 id="1-图拉普拉斯矩阵"><a href="#1-图拉普拉斯矩阵" class="headerlink" title="1. 图拉普拉斯矩阵"></a>1. 图拉普拉斯矩阵</h3><p>对于一个无向图 $G=(V,E)$，其归一化拉普拉斯矩阵定义为：</p><script type="math/tex; mode=display">L = I - D^{-1/2}AD^{-1/2}</script><p>其中：</p><ul><li>$A \in \mathbb{R}^{n\times n}$ 为邻接矩阵</li><li>$D$ 为度对角矩阵，$D<em>{ii} = \sum_j A</em>{ij}$</li><li>$L$ 是<strong>对称半正定矩阵</strong><h3 id="2-特征分解-1"><a href="#2-特征分解-1" class="headerlink" title="2. 特征分解"></a>2. 特征分解</h3>对 $L$ 进行特征分解：<script type="math/tex; mode=display">L = U \Lambda U^T</script>其中：</li><li>$\Lambda = \text{diag}(\lambda_1, \lambda_2, …, \lambda_n)$ 是特征值对角阵 ($0 \leq \lambda_1 \leq … \leq \lambda_n$)</li><li>$U = [\mathbf{u}_1, \mathbf{u}_2, …, \mathbf{u}_n]$ 是酉矩阵，每列是对应特征值的特征向量<h3 id="3-位置编码生成"><a href="#3-位置编码生成" class="headerlink" title="3. 位置编码生成"></a>3. 位置编码生成</h3>节点 $v$ 的位置编码为：<script type="math/tex; mode=display">PE(v) = [\mathbf{u}_2(v), \mathbf{u}_3(v), ..., \mathbf{u}_{d+1}(v)]</script>其中：</li><li>排除第一个特征向量 $\mathbf{u}_1$ (对应特征值 $\lambda_1=0$，所有元素均为常数)</li><li>取 $d$ 个最小非零特征值对应的特征向量分量</li></ul><blockquote><p><strong>为什么工作？</strong>：<br>Fiedler 定理表明第二特征向量 $\mathbf{u}_2$ (Fiedler 向量) 将图分割为两个连通分量的最优解，更高维特征向量提供更细粒度的空间位置信息。</p></blockquote><h2 id="特点分析"><a href="#特点分析" class="headerlink" title="特点分析"></a>特点分析</h2><div class="table-container"><table><thead><tr><th>性质</th><th>说明</th><th>影响</th></tr></thead><tbody><tr><td><strong>结构感知</strong></td><td>编码图的拓扑结构</td><td>区分环状图/网格图的对称节点</td></tr><tr><td><strong>正交性</strong></td><td>$\langle \mathbf{u}<em>i, \mathbf{u}_j \rangle=\delta</em>{ij}$</td><td>不同方向位置特征解耦</td></tr><tr><td><strong>排列不变性</strong></td><td>对节点重标号不变</td><td>满足GNN的置换不变性要求</td></tr><tr><td><strong>多尺度性</strong></td><td>小特征值对应全局结构</td><td>不同特征向量捕获不同尺度的位置关系</td></tr></tbody></table></div><h2 id="完整实现代码-PyTorch-PyG"><a href="#完整实现代码-PyTorch-PyG" class="headerlink" title="完整实现代码 (PyTorch+PyG)"></a>完整实现代码 (PyTorch+PyG)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> get_laplacian</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_laplace_pe</span>(<span class="params">edge_index, num_nodes, positive=<span class="literal">False</span>, k=<span class="number">8</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算图的拉普拉斯位置编码</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        edge_index (Tensor): [2, num_edges] 边索引</span></span><br><span class="line"><span class="string">        num_nodes (int): 节点数量</span></span><br><span class="line"><span class="string">        positive (bool): 是否强制值均为正 (用于正定矩阵)</span></span><br><span class="line"><span class="string">        k (int): 使用的特征向量维度</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        pe (Tensor): [num_nodes, k] 位置编码矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算归一化拉普拉斯矩阵</span></span><br><span class="line">    L = get_laplacian(edge_index, num_nodes=num_nodes, normalization=<span class="string">&#x27;sym&#x27;</span>)</span><br><span class="line">    L_sparse = sp.coo_matrix((L[<span class="number">1</span>].numpy(), L[<span class="number">0</span>].numpy()), shape=(num_nodes, num_nodes))</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 特征分解 (仅计算k+1个最小特征值/向量)</span></span><br><span class="line">    evals, evecs = sp.linalg.eigsh(L_sparse, k=k+<span class="number">1</span>, which=<span class="string">&#x27;SM&#x27;</span>)</span><br><span class="line">    <span class="comment"># 删除第一个特征向量(对应λ=0)</span></span><br><span class="line">    evecs = evecs[:, evals.argsort()][:, <span class="number">1</span>:<span class="number">1</span>+k] </span><br><span class="line">  </span><br><span class="line">    pe = torch.tensor(evecs).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment"># 可选：变换为正值(使维度可解释)</span></span><br><span class="line">    <span class="keyword">if</span> positive:</span><br><span class="line">        pe = pe - pe.<span class="built_in">min</span>(<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> pe / pe.<span class="built_in">max</span>(<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> pe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：应用到分子图数据</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> ZINC</span><br><span class="line">dataset = ZINC(root=<span class="string">&#x27;/data/zinc&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>, transform=LaplacePEAdder(k=<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LaplacePEAdder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;PyG数据转换器：自动加入位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k=<span class="number">8</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.k = k</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data: Data</span>):</span><br><span class="line">        edge_index, num_nodes = data.edge_index, data.num_nodes</span><br><span class="line">        pe = compute_laplace_pe(edge_index, num_nodes, k=<span class="variable language_">self</span>.k)</span><br><span class="line">        <span class="comment"># 与原始特征拼接</span></span><br><span class="line">        <span class="keyword">if</span> data.x <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            data.x = pe</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data.x = torch.cat([data.x, pe], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><h2 id="关键优化技术"><a href="#关键优化技术" class="headerlink" title="关键优化技术"></a>关键优化技术</h2><ol><li><strong>GPU加速特征分解</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用cuSPARSE和cuSOLVER进行加速</span></span><br><span class="line"><span class="keyword">import</span> torch.sparse</span><br><span class="line">L_coo = get_laplacian(edge_index, normalization=<span class="string">&#x27;sym&#x27;</span>)</span><br><span class="line">L_indices = torch.vstack(L_coo)</span><br><span class="line">L_value = torch.ones(L_coo.shape[<span class="number">1</span>])</span><br><span class="line">L_sparse = torch.sparse_coo_tensor(L_indices, L_value, (num_nodes, num_nodes))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 截断特征分解</span></span><br><span class="line">evals, evecs = torch.lobpcg(L_sparse, k=k+<span class="number">1</span>, largest=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li><li><strong>处理大规模图</strong><ul><li>Nystrom 近似法：对部分节点采样加速计算 (ICML 2023)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> graphgym.efeat.position <span class="keyword">import</span> nystrom_approximation</span><br><span class="line">pe = nystrom_approximation(L, sample_size=<span class="number">500</span>, dim=k)</span><br></pre></td></tr></table></figure></li></ul></li></ol><h2 id="GNN模型集成示例"><a href="#GNN模型集成示例" class="headerlink" title="GNN模型集成示例"></a>GNN模型集成示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GTPosModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;结合位置编码的图Transformer&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, pe_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.pe_proj = nn.Linear(pe_dim, in_dim)  <span class="comment"># 位置编码投影层</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = GATConv(in_dim, <span class="number">64</span>, heads=<span class="number">4</span>)</span><br><span class="line">        ...</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index, lap_pe</span>):</span><br><span class="line">        <span class="comment"># 融合原始特征和位置编码</span></span><br><span class="line">        fused_feat = x + <span class="variable language_">self</span>.pe_proj(lap_pe)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoder(fused_feat, edge_index)</span><br></pre></td></tr></table></figure><h2 id="应用场景比较"><a href="#应用场景比较" class="headerlink" title="应用场景比较"></a>应用场景比较</h2><div class="table-container"><table><thead><tr><th>图类型</th><th>适用性</th><th>解释</th></tr></thead><tbody><tr><td>环形/网格图</td><td>★★★</td><td>完美区分结构等价节点</td></tr><tr><td>小世界网络</td><td>★★☆</td><td>局部特征优于全局位置</td></tr><tr><td>低维点云图</td><td>★☆</td><td>欧式距离编码更有效</td></tr><tr><td>动态图</td><td>☆</td><td>需每次重新计算特征分解</td></tr></tbody></table></div><h2 id="前沿进展"><a href="#前沿进展" class="headerlink" title="前沿进展"></a>前沿进展</h2><ol><li><p><strong>复值编码</strong> (ICLR 2024突破)<br>使用复值特征向量拓展频谱信息：</p><script type="math/tex; mode=display">\mathcal{CR}PE(v) = e^{-i\theta}\mathbf{u}(v) \quad (\theta \sim \text{learnable})</script></li><li><p><strong>方向可区分编码</strong><br>在异质图中给特征向量赋予方向信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">directed_lap_pe</span>(<span class="params">edge_index, direction=<span class="string">&#x27;out&#x27;</span></span>):</span><br><span class="line">    L_out = D_out^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125; A D_out^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125;  <span class="comment"># 出度拉普拉斯</span></span><br><span class="line">    L_in = D_in^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125; A^T D_in^&#123;-<span class="number">1</span>/<span class="number">2</span>&#125;   <span class="comment"># 入度拉普拉斯</span></span><br><span class="line">    <span class="keyword">return</span> (compute_pe(L_out), compute_pe(L_in))</span><br></pre></td></tr></table></figure></li><li><p><strong>自适应频谱选择</strong><br>基于learnable gating机制动态选择特征向量：</p><script type="math/tex; mode=display">\text{PE}(v) = \sum_{i=2}^k g_i(\mathcal{G}) \cdot \mathbf{u}_i(v)</script></li></ol><h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ul><li>Dwivedi et al. Benchmarking GNNs with Positional Encodings (ICLR 2023)</li><li>Kreuzer et al. Rethinking Graph Transformers with Spectral Attention (NeurIPS 2021)</li><li>Lim et al. Sign and Basis Invariant Networks for Spectral Graph Representation Learning (ICML 2023)</li></ul><blockquote><p><strong>最佳实践</strong>：</p><ul><li>对于&lt;50k节点的图直接计算全分解</li><li>大图使用Nyström近似或Lanczos迭代</li><li>与可学习PE结合（如Random Walk PE）效果更佳</li><li>Transformer架构比GCN/GAT更能利用频谱信息</li></ul></blockquote><hr><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://epsilonzyj.github.io/posts/63fed347.html">图神经网络简介 | An Introduction to GNN</a></li><li><a href="https://www.bilibili.com/video/BV1Vw411R7Fj?spm_id_from=333.788.videopod.episodes&amp;vd_source=2e36fae16810615c2d859efc03aef1c4">图卷积神经网络（GCN）的数学原理详解——谱图理论和傅立叶变换初探-Bilibili</a></li></ol>]]></content>
    
    
    <summary type="html">谱域图神经网络不同于常见的消息传递图神经网络，采用图信号的傅立叶变换进行拓展，本文则简要介绍何为谱域图神经网络。</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="GNN" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/GNN/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="GNN" scheme="https://epsilonzyj.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>使用Mac远程连接Windows WSL</title>
    <link href="https://epsilonzyj.github.io/posts/e4aef2ca.html"/>
    <id>https://epsilonzyj.github.io/posts/e4aef2ca.html</id>
    <published>2025-07-30T16:00:00.000Z</published>
    <updated>2025-10-20T11:51:24.023Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>Windows与MacBook，且处于同一局域网下。安装WSL2的过程略。</p><h2 id="WSL中的配置"><a href="#WSL中的配置" class="headerlink" title="WSL中的配置"></a>WSL中的配置</h2><h3 id="安装配置SSH服务"><a href="#安装配置SSH服务" class="headerlink" title="安装配置SSH服务"></a>安装配置SSH服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install openssh-server</span><br></pre></td></tr></table></figure><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p>将注释的内容全部取消注释：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Port 22</span><br><span class="line">AddressFamily any</span><br><span class="line">ListenAddress 0.0.0.0</span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure><h3 id="启动SSH服务"><a href="#启动SSH服务" class="headerlink" title="启动SSH服务"></a>启动SSH服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ssh-keygen -A</span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> /usr/sbin/service ssh start</span><br></pre></td></tr></table></figure><h2 id="Windows的配置"><a href="#Windows的配置" class="headerlink" title="Windows的配置"></a>Windows的配置</h2><p>由于电脑可能安装了杀毒软件，会导致Windows Defender中防火墙设置被篡改而使得部分功能变为灰色，从而不可用，因此使用Power Shell进行配置。注意，一定要使用管理员身份打开，否则会因为权限不足而无法完成操作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">New-NetFirewallRule -Name sshd -DisplayName <span class="string">&#x27;sshd for WSL&#x27;</span> -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22</span><br></pre></td></tr></table></figure><h2 id="端口转发"><a href="#端口转发" class="headerlink" title="端口转发"></a>端口转发</h2><p>使用管理员身份在Power Shell中运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">​​​​​​​netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=[PORT] connectaddress=[IP] connectport=[PORT]</span><br><span class="line"><span class="comment"># PORT 为你设置的端口，我这里为3333</span></span><br><span class="line"><span class="comment"># IP地址为wls linux子系统的ip地址，可通过ifconfig查看</span></span><br></pre></td></tr></table></figure><h2 id="使用Mac远程连接"><a href="#使用Mac远程连接" class="headerlink" title="使用Mac远程连接"></a>使用Mac远程连接</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh xxx@xxx.xxx.xxx.xxx -p 22</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Windows与MacBook双持的情况下，远程连接WSL使用Linux环境的方法。</summary>
    
    
    
    <category term="Env" scheme="https://epsilonzyj.github.io/categories/Env/"/>
    
    <category term="WSL" scheme="https://epsilonzyj.github.io/categories/Env/WSL/"/>
    
    
    <category term="WSL" scheme="https://epsilonzyj.github.io/tags/WSL/"/>
    
  </entry>
  
  <entry>
    <title>Zotero：A great research assistant</title>
    <link href="https://epsilonzyj.github.io/posts/2286d822.html"/>
    <id>https://epsilonzyj.github.io/posts/2286d822.html</id>
    <published>2025-07-28T16:00:00.000Z</published>
    <updated>2025-10-20T11:51:24.012Z</updated>
    
    
    <summary type="html">科研看paper工具</summary>
    
    
    
    <category term="Tools" scheme="https://epsilonzyj.github.io/categories/Tools/"/>
    
    
    <category term="Research" scheme="https://epsilonzyj.github.io/tags/Research/"/>
    
    <category term="Tools" scheme="https://epsilonzyj.github.io/tags/Tools/"/>
    
  </entry>
  
  <entry>
    <title>ICS实验随笔</title>
    <link href="https://epsilonzyj.github.io/posts/67ad26dd.html"/>
    <id>https://epsilonzyj.github.io/posts/67ad26dd.html</id>
    <published>2025-03-14T16:49:57.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于右移的问题"><a href="#关于右移的问题" class="headerlink" title="关于右移的问题"></a>关于右移的问题</h2><h3 id="问题发现"><a href="#问题发现" class="headerlink" title="问题发现"></a>问题发现</h3><p>在上机进行ICS实验中，有一个函数要求产生highbit到lowbit全为1的数字。一个相当简单的想法是直接将0xFFFFFFFF右移hinghbit+1位后再 移回来，再取反，便可以得到第0位到第highbit位的数字，然后再进行后续操作。</p><p>然而在这个问题中，遇到了一些问题，在此记录一下。</p><h4 id="左右移的位数为负数"><a href="#左右移的位数为负数" class="headerlink" title="左右移的位数为负数"></a>左右移的位数为负数</h4><p><strong>负数移位</strong>（如x &gt;&gt; -1）属于<strong>未定义行为</strong>，不同平台和编译器会产生不同结果，部分会在编译阶段报错，而在一些特定运算下，如x &gt;&gt; (m-n)中，可能会因为某些特殊赋值而出现负数，应当进行避免。</p><h4 id="右移溢出"><a href="#右移溢出" class="headerlink" title="右移溢出"></a>右移溢出</h4><p>根据上面函数所需要求，一开始写出如下的函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((<span class="number">0xFFFFFFFF</span> &gt;&gt; (highbit + <span class="number">1</span>)) &lt;&lt; (highbit + <span class="number">1</span>));</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而，在测试调试时发现，当highbit==31时，函数值则会有问题。溯源后发现，highbit+1==32时，high的值一直为0xFFFFFFFF，并不符合预期的0x0。起初以为是自己逻辑有问题，于是进行了一下测试，首先把highbit+1替换为32：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((<span class="number">0xFFFFFFFF</span> &gt;&gt; <span class="number">32</span>) &lt;&lt; <span class="number">32</span>);</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而出乎意料的是，此时high的值变为正常的0x0，此时也开始报了warning，但显然不看warning是一个好习惯（经典笑话，笑）。则考虑到32与highbit+1的差异性，一开始没考虑常量和变量的问题，以为是32是unsigned类型而另一个是int类型，由于数据类型的问题可能会存在一些区别，虽然感觉这种思路似乎很离谱，且并没有什么道理可言，但是还是测试了一下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((<span class="number">0xFFFFFFFF</span> &gt;&gt; (<span class="type">unsigned</span>)(highbit + <span class="number">1</span>)) &lt;&lt; (<span class="type">unsigned</span>)(highbit + <span class="number">1</span>));</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个时候high又变成了0xFFFFFFFF，这有些出乎意料，一时就没什么头绪，不知道有什么问题，于是就开始怀疑编译器是否有编译优化的问题了。</p><h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><p>经过查找资料，发现右移时右侧操作数大于或等于左侧操作数的位数是属于未定义运算，而编译时确实存在一些编译优化。对于未定义行为，在编译时，分别发生了以下两件事：</p><ul><li>常量表达式：编译器在编译阶段进行常量求值，依据标准未定义的情况下可能直接优化成 0。</li><li>变量表达式：运行时生成的移位指令通常只取移位数的低 5 位（对于 32 位整数而言），因而 32 的低 5 位为 0，实际移位相当于“右移 0 位”，结果保持原值 0xFFFFFFFF。</li></ul><p>当表达式写为 0xFFFFFFFF &gt;&gt; (highbit+1) 时，虽然 (highbit+1) 计算结果为 32，但由于其为运行时求值的变量表达式，编译器生成的汇编指令遵循硬件实际移位机制。</p><p>在许多 CPU 架构（例如 x86）中，用于右移操作的指令（如 SHR 指令）通常只取移位数的低 5 位（对于 32 位操作数），即实际移位数 = 给定移位数 mod 32。</p><p>当 (highbit+1) 等于 32 时，其低 5 位为 0，故实际移位操作相当于“右移 0 位”，因此结果仍为原值 0xFFFFFFFF。</p><p>这种处理方式与硬件实现有关，虽然在 C 语言标准中两种写法的行为都是未定义的，但实际运行时硬件指令的“模 32”特性使得结果不同。</p><p>因此，对于上述问题，一种解决办法就是分开写，即移两次位。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitMask</span><span class="params">(<span class="type">int</span> highbit, <span class="type">int</span> lowbit)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> high = ~((((<span class="number">0xFFFFFFFF</span> &gt;&gt; highbit) &gt;&gt; <span class="number">1</span>) &lt;&lt; highbit) &lt;&lt; <span class="number">1</span>);</span><br><span class="line">    <span class="type">int</span> low  = ((~<span class="number">0</span>) &lt;&lt; lowbit);</span><br><span class="line">    <span class="keyword">return</span> high &amp; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="寄存器保护"><a href="#寄存器保护" class="headerlink" title="寄存器保护"></a>寄存器保护</h2><h3 id="问题发现-1"><a href="#问题发现-1" class="headerlink" title="问题发现"></a>问题发现</h3><p>在某次实验中，根据实验要求，需要使用汇编语言编写其中的部分函数。当然这个实验主要的内容实在优化炫技（划掉），不过在实验中遇到了一个很奇怪的问题。</p><p><em>注意：实验环境为Visual Studio 2022，编译采用x86架，构。其中在生成依赖项时需要对项目进行设置，对项目名称右击后点击生成依赖项下的生成子定义，并勾选masm，这样可以实现汇编和C混合编程。其余均为默认设置。感兴趣的同学可以根据以上内容进行复现。</em></p><p>在实验中需要在Debug版本和Release版本下分别进行程序运行速度的比较，编译阶段并没有问题，Debug版本可以正常运行。但是在Release版本中，当运行即将退出主函数时，抛出了异常。（由于时间较久远，且笔者较懒，没有复现的图片，了解一下原理即可）</p><h3 id="问题解决-1"><a href="#问题解决-1" class="headerlink" title="问题解决"></a>问题解决</h3><p>根据反汇编调试，在return 0处打断点，在查看反汇编后，发现在退出主函数之前进行了一些安全检查，其中一项就是进行寄存器的检查。而在本次场景中，因为Release版本优化程度较高，在程序编译时默认不会使用到寄存器ebx，因而在程序退出时会检查ebx是否进行改动。很遗憾，笔者水平较差，在编写x86汇编的时候用到了ebx并且没有进行寄存器保护，因此由于篡改寄存器导致程序崩溃。</p><p>以下给出原始的代码（后期模拟出来的）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">.686P</span><br><span class="line">.model flat, c</span><br><span class="line">printf proto c :ptr sbyte, :vararg</span><br><span class="line">; includelib  libcmt.lib</span><br><span class="line">includelib  legacy_stdio_definitions.lib </span><br><span class="line"></span><br><span class="line">student  struct</span><br><span class="line">    sname   db   8 dup(0)</span><br><span class="line">    sid     db   11 dup(0)</span><br><span class="line">    align   2    ; 指明对齐方式，汇编语言默认是紧凑存放</span><br><span class="line">                 ; 可以实验一下，去掉对齐方式伪指令的结果</span><br><span class="line">    scores  dw   8  dup(0)</span><br><span class="line">    average dw   0</span><br><span class="line">student   ends</span><br><span class="line"></span><br><span class="line">.data</span><br><span class="line">   lpfmt  db &quot;%s %s %d %d&quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_string  db &quot;%s  &quot;,0</span><br><span class="line">   lpfmt_num  db &quot;%d  &quot;,0</span><br><span class="line">   lpfmt_size    db  &quot;size of struct %d  &quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_offset  db  0dh,0ah,&quot;offset of scores %d  &quot;,0dh,0ah,0</span><br><span class="line"></span><br><span class="line">.code</span><br><span class="line">;  显示学生信息</span><br><span class="line">;  sptr 学生数组的首地址</span><br><span class="line">;  num  学生人数</span><br><span class="line">;  注意， printf 中会用到一些寄存器，也没有保护</span><br><span class="line">;         即执行 printf前后，一个寄存器中的内容发生变化</span><br><span class="line"></span><br><span class="line">computeAverageScoreAsmOpt proc sptr: dword, num: dword</span><br><span class="line">    local s: dword</span><br><span class="line"></span><br><span class="line">    mov edx, sptr</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov ecx, 0</span><br><span class="line"></span><br><span class="line">LOOPI:</span><br><span class="line">                    ;int sum = 0</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov s, eax</span><br><span class="line">                    ;s[i].scores[0]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 014h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[1]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 016h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[2]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 018h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[3]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ah</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[4]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ch</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[5]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Eh</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[6]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 020h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx  </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[7]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 022h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;sum / 8</span><br><span class="line">    mov eax, s</span><br><span class="line">    sar eax, 3</span><br><span class="line">                    ;s[i].average = sum/8</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    mov word ptr[edx+ebx+024h], ax</span><br><span class="line">                    ;i ++</span><br><span class="line">    inc ecx</span><br><span class="line">                    ;i == num ?</span><br><span class="line">    cmp ecx,num</span><br><span class="line">    jne LOOPI</span><br><span class="line"></span><br><span class="line">    ret</span><br><span class="line">computeAverageScoreAsmOpt endp</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>寄存器保护其实是一个非常简单非常常见的想法，但是由于在编写函数时，笔者原想要减少指令以换取（似乎）更快的速度，同时也是懒得写（划掉，怎么能把最重要的原因写出来），因此并没有做任何寄存器保护的办法。</p><p>接下来就简单讲一下寄存器保护思路：</p><ul><li>对于eax等寄存器，直接压入栈中即可</li><li>对于ebp，将ebp压入栈中，同时将esp赋值给ebp（即让ebp指向原始栈顶，方便后续esp和ebp等在函数返回时/后恢复原来的值）</li><li>对于esp，esp减少一部分值，即指向更小的地址，建立一个新栈</li></ul><p>而在实验中，由于并没有（直接）使用ebp、esp等寄存器，因而就对这两个不做特殊的保护了，而将其余寄存器全部压入栈中，在函数结束时再弹出。由于懒（划掉），笔者将所有的可能用到的寄存器均压入了栈中，而不是将此函数中用到的寄存器压入栈中（反正结果对了就行了，划掉）。</p><p>以下是修改后的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line">.686P</span><br><span class="line">.model flat, c</span><br><span class="line">printf proto c :ptr sbyte, :vararg</span><br><span class="line">; includelib  libcmt.lib</span><br><span class="line">includelib  legacy_stdio_definitions.lib </span><br><span class="line"></span><br><span class="line">student  struct</span><br><span class="line">    sname   db   8 dup(0)</span><br><span class="line">    sid     db   11 dup(0)</span><br><span class="line">    align   2    ; 指明对齐方式，汇编语言默认是紧凑存放</span><br><span class="line">                 ; 可以实验一下，去掉对齐方式伪指令的结果</span><br><span class="line">    scores  dw   8  dup(0)</span><br><span class="line">    average dw   0</span><br><span class="line">student   ends</span><br><span class="line"></span><br><span class="line">.data</span><br><span class="line">   lpfmt  db &quot;%s %s %d %d&quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_string  db &quot;%s  &quot;,0</span><br><span class="line">   lpfmt_num  db &quot;%d  &quot;,0</span><br><span class="line">   lpfmt_size    db  &quot;size of struct %d  &quot;,0dh,0ah,0</span><br><span class="line">   lpfmt_offset  db  0dh,0ah,&quot;offset of scores %d  &quot;,0dh,0ah,0</span><br><span class="line"></span><br><span class="line">.code</span><br><span class="line">;  显示学生信息</span><br><span class="line">;  sptr 学生数组的首地址</span><br><span class="line">;  num  学生人数</span><br><span class="line">;  注意， printf 中会用到一些寄存器，也没有保护</span><br><span class="line">;         即执行 printf前后，一个寄存器中的内容发生变化</span><br><span class="line"></span><br><span class="line">computeAverageScoreAsmOpt proc sptr: dword, num: dword</span><br><span class="line">    local s: dword</span><br><span class="line"></span><br><span class="line">    push eax</span><br><span class="line">    push ebx</span><br><span class="line">    push ecx</span><br><span class="line">    push edx</span><br><span class="line">    push esi</span><br><span class="line">    push edi</span><br><span class="line"></span><br><span class="line">    mov edx, sptr</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov ecx, 0</span><br><span class="line"></span><br><span class="line">LOOPI:</span><br><span class="line">                    ;int sum = 0</span><br><span class="line">    mov eax, 0</span><br><span class="line">    mov s, eax</span><br><span class="line">                    ;s[i].scores[0]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 014h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[1]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 016h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[2]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 018h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[3]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ah</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[4]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Ch</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line"></span><br><span class="line">                    ;s[i].scores[5]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 01Eh</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;s[i].scores[6]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 020h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx  </span><br><span class="line">    </span><br><span class="line">                    ;s[i].scores[7]</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    add ebx, 022h</span><br><span class="line">    movzx eax, word ptr[edx+ebx]</span><br><span class="line">                    ;sum = sum + s[i].scores[j]</span><br><span class="line">    mov ebx, s</span><br><span class="line">    add ebx, eax</span><br><span class="line">    mov s, ebx</span><br><span class="line">                    </span><br><span class="line">                    ;sum / 8</span><br><span class="line">    mov eax, s</span><br><span class="line">    sar eax, 3</span><br><span class="line">                    ;s[i].average = sum/8</span><br><span class="line">    mov ebx, 026h</span><br><span class="line">    imul ebx, ecx</span><br><span class="line">    mov word ptr[edx+ebx+024h], ax</span><br><span class="line">                    ;i ++</span><br><span class="line">    inc ecx</span><br><span class="line">                    ;i == num ?</span><br><span class="line">    cmp ecx,num</span><br><span class="line">    jne LOOPI</span><br><span class="line"></span><br><span class="line">    pop edi</span><br><span class="line">    pop esi</span><br><span class="line">    pop edx</span><br><span class="line">    pop ecx</span><br><span class="line">    pop ebx</span><br><span class="line">    pop eax</span><br><span class="line"></span><br><span class="line">    ret</span><br><span class="line">computeAverageScoreAsmOpt endp</span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">ICS课程中所踩的坑及编程中可能有的问题</summary>
    
    
    
    <category term="Course" scheme="https://epsilonzyj.github.io/categories/Course/"/>
    
    <category term="ICS" scheme="https://epsilonzyj.github.io/categories/Course/ICS/"/>
    
    
    <category term="HUST" scheme="https://epsilonzyj.github.io/tags/HUST/"/>
    
    <category term="ICS" scheme="https://epsilonzyj.github.io/tags/ICS/"/>
    
    <category term="C/C++" scheme="https://epsilonzyj.github.io/tags/C-C/"/>
    
    <category term="Assembly" scheme="https://epsilonzyj.github.io/tags/Assembly/"/>
    
  </entry>
  
  <entry>
    <title>HUST仓库</title>
    <link href="https://epsilonzyj.github.io/posts/c723aac6.html"/>
    <id>https://epsilonzyj.github.io/posts/c723aac6.html</id>
    <published>2025-03-03T14:13:31.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<p>本博客用于记录作者本科时期的课程实验或课程设计。</p><ol><li><a href="https://github.com/EpsilonZYJ/HUST-Material/tree/main/计算机科学与技术/IAC语言程序设计/code">C语言程序设计实验</a></li><li><a href="https://github.com/EpsilonZYJ/HUST-Material/tree/main/计算机科学与技术/IA计算思维">计算思维</a></li><li><a href="https://github.com/EpsilonZYJ/DataStructureExp">数据结构实验</a></li><li><a href="https://github.com/EpsilonZYJ/Sudoku">基于SAT问题的数独求解（程序综合课程设计）</a></li><li><a href="https://github.com/EpsilonZYJ/AlgorithmExperiment">算法设计实验</a></li><li><a href="https://github.com/EpsilonZYJ/CppExperiement">C++实验</a></li><li><a href="https://github.com/EpsilonZYJ/Digital-Circuit-Experiment">数字电路与逻辑设计实验</a></li><li><a href="https://github.com/EpsilonZYJ/IntroductionToAI">人工智能导论课程作业</a></li><li><a href="https://github.com/EpsilonZYJ/DataBaseExperiement/tree/main">数据库系统原理实验</a></li><li><a href="https://github.com/EpsilonZYJ/Introduction-To-Machine-Learning/tree/main">机器学习导论实验与课设</a></li><li><a href="https://github.com/EpsilonZYJ/HUST-Java">Java实验</a></li></ol>]]></content>
    
    
    <summary type="html">HUST实验和课程设计demo</summary>
    
    
    
    <category term="HUST" scheme="https://epsilonzyj.github.io/categories/HUST/"/>
    
    
    <category term="HUST" scheme="https://epsilonzyj.github.io/tags/HUST/"/>
    
  </entry>
  
  <entry>
    <title>GNN经典模型 | GNN Basic Models</title>
    <link href="https://epsilonzyj.github.io/posts/4a93988c.html"/>
    <id>https://epsilonzyj.github.io/posts/4a93988c.html</id>
    <published>2025-03-02T11:36:18.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="GNN" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/GNN/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="GNN" scheme="https://epsilonzyj.github.io/tags/GNN/"/>
    
    <category term="GNN With Attention" scheme="https://epsilonzyj.github.io/tags/GNN-With-Attention/"/>
    
  </entry>
  
  <entry>
    <title>图神经网络简介 | An Introduction to GNN</title>
    <link href="https://epsilonzyj.github.io/posts/63fed347.html"/>
    <id>https://epsilonzyj.github.io/posts/63fed347.html</id>
    <published>2025-02-27T13:49:55.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="图的表示"><a href="#图的表示" class="headerlink" title="图的表示"></a>图的表示</h2><p>对于图，有</p><ul><li>V：Vertex(or node) attributes</li><li>E：Edge(or link) attributes and directions</li><li>U：Global(or master node) attributes</li></ul><p>分别可以使用向量embedding来进行表示。</p><h3 id="图片转化为图"><a href="#图片转化为图" class="headerlink" title="图片转化为图"></a>图片转化为图</h3><p>将第$(i, j)$位置的像素映射为第$(i, j)$位置的节点（可以将节点按照类似像素排序的规则排序，即每行每列个数均固定。对于图采用邻接矩阵进行存储。对于某个像素的邻居，为上下左右和对角线上至多8个节点，进而表示为图</p><h3 id="文本转化为图"><a href="#文本转化为图" class="headerlink" title="文本转化为图"></a>文本转化为图</h3><p>可以将每一个词表示为一个顶点，上一个词和下一个词之间有一个有向的边。</p><h3 id="其它问题转化为图"><a href="#其它问题转化为图" class="headerlink" title="其它问题转化为图"></a>其它问题转化为图</h3><h4 id="分子转化为图"><a href="#分子转化为图" class="headerlink" title="分子转化为图"></a>分子转化为图</h4><p>可以使每个原子表示一个节点，原子之间有边相连则表示一条边。</p><h4 id="社交网络图"><a href="#社交网络图" class="headerlink" title="社交网络图"></a>社交网络图</h4><p>如同一场景中出现过的人，可以将其对应的节点连一条边。</p><h4 id="引用图"><a href="#引用图" class="headerlink" title="引用图"></a>引用图</h4><p>如文章A引用文章B，则可以连接A指向B的有向边。</p><h3 id="图机器学习的任务"><a href="#图机器学习的任务" class="headerlink" title="图机器学习的任务"></a>图机器学习的任务</h3><ul><li>图层面的任务，如识别有几个环进而对图进行分类</li><li>顶点层面的任务，如对两个复杂的社交网络图，对其中以两个人为核心的两个不同阵营，判断每个人属于哪个阵营</li><li>边层面的任务，预测边的属性</li></ul><h3 id="机器学习用于图上的挑战"><a href="#机器学习用于图上的挑战" class="headerlink" title="机器学习用于图上的挑战"></a>机器学习用于图上的挑战</h3><p><em>（此处机器学习特指神经网络）</em></p><p>图上有四种信息：顶点的属性、边的属性、全局信息和连接性。前三种均可用向量表示，但连接性表示比较困难。</p><p>一种朴素想法是使用邻接矩阵，连接则用1表示，未连接用0表示。但是这种方法表示的矩阵会非常大。如果使用稀疏矩阵，在存储上可行，但要高效计算或者在GPU上计算较为困难。此外，由于交换任意行和任意列不会产生影响，这意味着交换任意行或任意列后的图放进神经网络，出来的结果应该与原先相同。</p><p>因此我们可以采用如下的形式。使用一个向量来表示节点，每个节点的属性使用一个标量来表示；用一个向量来表示边，每个边的属性也使用一个标量来表示；使用邻接链表来表示连接性。如下图所示：</p><p><img src="/img/GNN1.png" alt=""></p><h2 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h2><p>图神经网络是对图上所有的属性，包括顶点、边、上下文等，进行一个可以优化的变换，而这个变换可以保持住图的对称信息（顶点重新排序后结果不变）。此处采用信息传递的神经网络。图神经网络输入为图，输出也是图，对图的顶点、边和全局上下文进行变换，但不对连接性产生改变。</p><h3 id="最简单的GNN"><a href="#最简单的GNN" class="headerlink" title="最简单的GNN"></a>最简单的GNN</h3><p>使用上文所提出的数据结构，对全局上下问信息、顶点和边分别建立多层感知机，从而获得一个新的图。</p><h3 id="GNN-Predictions-by-Pooling-Information"><a href="#GNN-Predictions-by-Pooling-Information" class="headerlink" title="GNN Predictions by Pooling Information"></a>GNN Predictions by Pooling Information</h3><p>若没有顶点点向量，则使用汇聚/池化（Pooling）来得到节点向量。将与该节点的边的向量和全局向量一起相加得到新的向量（此处假设全局向量和边的向量维度相同，如果不同则需要进行投影），得到的新的向量作为节点的向量。最后进入全连接层得到顶点的输出。</p><p><img src="/img/GNN2.png" alt=""></p><p>同样的，如果只有顶点向量和全局向量，则将顶点向量和全局向量汇聚到边上，然后进入边向量的输出层，最后得到边的输出。</p><p>如果没有全局向量，则可以把所有的顶点向量加起来得到一个全局向量，并进入全局的输出层得到全局的输出。</p><p>因此最简单的GNN为如下的结构：给定输入的图，进入一系列的GNN层，每个层有三个MLP对应三种不同的属性。最后输出得到保持整个图结构的输出，但里面所有的属性发生了变化，而根据要对哪个属性做预测则添加合适的输出层，如果有信息缺失的话则加入合适的汇聚层即可。这样就可以完成一个简单的预测。</p><p><img src="/img/GNN3.png" alt=""></p><p>然而这种方式有所欠缺，因为将三种属性割裂开，并不能有效地融合和利用整个图的信息，顶点与边分开单独计算。因此需要一种其它方式。</p><h3 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h3><p>在顶点输入MLP时，不再只是单纯输入顶点向量，而是采用将顶点向量与此顶点连接的顶点的向量相加组成的向量输入MLP进行顶点的更新，即聚合步与更新步。当叠加很多层，可以实现顶点信息长距离的传递。</p><p>其中顶点周围距离为1的邻居成为1-近邻。上述步骤即$\rho_{V_n \rightarrow V_n}$</p><p><img src="/img/GNN4.png" alt=""></p><h3 id="Learning-edge-representation"><a href="#Learning-edge-representation" class="headerlink" title="Learning edge representation"></a>Learning edge representation</h3><p>对于如何将顶点的信息传递给边，将边的信息传递给顶点，有如下的方式：</p><ul><li><p>首先通过$\rho_{V_n \rightarrow E_n}$将顶点的向量传递给边，若维度不同则进行投影再传递或使用concat将两个向量并在一起。</p></li><li><p>然后再进行$\rho_{E_n \rightarrow V_n}$将边的信息再传递给顶点。</p></li></ul><p>也可以反过来：</p><ul><li><p>先做顶点的更新$\rho_{E_n \rightarrow V_n}$</p></li><li><p>再做边的更新$\rho_{V_n \rightarrow E_n}$</p></li></ul><p>以上两种方法会出现不同的结果，且没有孰优孰劣之分。此外还可以进行交替更新。</p><p><img src="/img/GNN5.png" alt=""></p><h3 id="Adding-global-representations"><a href="#Adding-global-representations" class="headerlink" title="Adding global representations"></a>Adding global representations</h3><p><em>全局信息的更新</em></p><p>可以增加一个虚拟的顶点，称为<strong>master node</strong>或<strong>context vector</strong>，这个顶点与所有的V和E里面的内容均相连。当把顶点的信息汇聚给边的时候，会把U的信息也汇聚过来；当汇聚顶点时，也会把U汇聚过来；当汇聚U的时候，会把顶点和边的信息一起汇聚到U上，再做更新。</p><p><img src="/img/GNN6.png" alt=""></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>GNN对超参数较为敏感，通常参数有四种：网络有多少层，每个属性的嵌入（embedding）维度有多高，汇聚（pooling）的操作是什么类型（最大值、均值等），怎么做信息传递（是否做信息传递，哪些属性之间做信息传递）。</p><h2 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h2><h3 id="采样（batching）"><a href="#采样（batching）" class="headerlink" title="采样（batching）"></a>采样（batching）</h3><p>如果对整个图进行计算，可能最终中间结果会非常大，因而要进行采样。常见的采样方法如下：</p><ol><li>随机采样一些点，将这些点点最近的邻居找出来，在计算时在这个子图上进行计算；</li><li>随机游走：随机在图上找一条边，沿着这条边走到下一个节点，沿着这个图随机走，规定最多随机走多少步，从而得到一个子图；</li><li>结合上面两种，随机走三步，然后把这三步中的每个邻居的节点全部找出来；</li><li>随机选一个点，找出第1近邻，2近邻…k近邻，即做宽度遍历得到子图。</li></ol><h3 id="Inductive-bias"><a href="#Inductive-bias" class="headerlink" title="Inductive bias"></a>Inductive bias</h3><p>此模型假设了在神经网络中保持了图的对称性。</p><h3 id="汇聚操作的比较"><a href="#汇聚操作的比较" class="headerlink" title="汇聚操作的比较"></a>汇聚操作的比较</h3><p>汇聚操作可以求和、求平均、求最大值，然而并没有哪种特别理想。</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://distill.pub/2021/gnn-intro/">Sanchez-Lengeling, et al., “A Gentle Introduction to Graph Neural Networks”, Distill, 2021.</a></li><li><a href="https://www.bilibili.com/video/BV1iT4y1d7zP">零基础多图详解图神经网络</a></li></ol>]]></content>
    
    
    <summary type="html">对图神经网络的简要介绍和入门</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    <category term="GNN" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/GNN/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
    <category term="GNN" scheme="https://epsilonzyj.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>CS224W | Machine Learning with Graphs</title>
    <link href="https://epsilonzyj.github.io/posts/c6b8d3b6.html"/>
    <id>https://epsilonzyj.github.io/posts/c6b8d3b6.html</id>
    <published>2025-02-26T16:00:00.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="传统基于特征的方法｜Traditional-Feature-base-Methods"><a href="#传统基于特征的方法｜Traditional-Feature-base-Methods" class="headerlink" title="传统基于特征的方法｜Traditional Feature-base Methods"></a>传统基于特征的方法｜Traditional Feature-base Methods</h2><h3 id="节点中心性｜Node-Centrality"><a href="#节点中心性｜Node-Centrality" class="headerlink" title="节点中心性｜Node Centrality"></a>节点中心性｜Node Centrality</h3><h4 id="特征向量中心度｜Eigenvector-centrality"><a href="#特征向量中心度｜Eigenvector-centrality" class="headerlink" title="特征向量中心度｜Eigenvector centrality"></a>特征向量中心度｜Eigenvector centrality</h4><p>衡量节点重要性的一种中心性指标，不仅考虑节点的直接连接数量，还考虑其相邻节点的重要性。特征向量中心性通过计算图的邻接矩阵的主导特征向量（最大特征值对应的特征向量）来确定，每个节点的中心性得分与其邻居的得分成正比。这种方法可用于识别网络中影响力较大的节点，如社交网络中的关键人物或传播网络中的核心节点。</p><p>思想：对于节点v，如果v被重要的邻居节点$u\in N(v)$包围，那么这个节点就相对重要</p><script type="math/tex; mode=display">c_v = \frac{1}{\lambda} \sum_{u \in N(v)}c_u \longleftrightarrow \lambda \mathbf{c} = \mathbf{Ac}</script><p>其中：</p><ul><li>$\lambda$是某个正常量</li><li>A是邻接矩阵，且若$u\in N(v), A_{uv} = 1$</li><li>c为中心性向量(Centrality vector)</li></ul><p>最大的特征值$\lambda_{max}$总是正数且唯一</p><p>主导特征向量 $C_max$用于衡量图的中心性。</p><h4 id="中介中心性｜Betweenness-Centrality"><a href="#中介中心性｜Betweenness-Centrality" class="headerlink" title="中介中心性｜Betweenness Centrality"></a>中介中心性｜Betweenness Centrality</h4><p>衡量一个节点在图中充当其他节点之间“桥梁”作用的程度。具体而言，它表示该节点出现在所有最短路径中的频率，数值越高，说明该节点在信息传递或网络流动中的重要性越大。</p><p>思想：如果许多的最短路径都经过这个节点，那么这个节点就相对重要</p><p>公式具体如下：</p><script type="math/tex; mode=display">c_v = \sum_{s\neq v \neq t}\frac{在s和t之间包含节点v的最短路径数}{在s和t中的最短路径数}</script><h4 id="接近中心性｜Closeness-Centrality"><a href="#接近中心性｜Closeness-Centrality" class="headerlink" title="接近中心性｜Closeness Centrality"></a>接近中心性｜Closeness Centrality</h4><p>衡量一个节点与图中所有其他节点的平均最短路径距离的倒数。数值越高，表示该节点更容易接触到图中的其他节点，信息传播效率更高，通常用于评估节点在网络中的传播能力。</p><p>思想：如果一个节点到所有的节点的距离最小，那么这个节点相对重要</p><p>公式如下：</p><script type="math/tex; mode=display">c_v = \frac{1}{\sum_{u\neq v}u和v之间最短路径的长度}</script><h4 id="聚类系数｜Clustering-Coefficient"><a href="#聚类系数｜Clustering-Coefficient" class="headerlink" title="聚类系数｜Clustering Coefficient"></a>聚类系数｜Clustering Coefficient</h4><p>针对单个节点，定义为该节点的邻居之间实际存在的边数与可能存在的最大边数之比</p><p>公式：</p><script type="math/tex; mode=display">e_v = \frac{v节点的邻居节点之间的边数}{C_{k_v}^{2}}</script><p>其中，分母表示节点v的邻居节点两两相连得到的边数。</p><h4 id="图元频率向量｜Graphlet-Degree-Vector"><a href="#图元频率向量｜Graphlet-Degree-Vector" class="headerlink" title="图元频率向量｜Graphlet Degree Vector"></a>图元频率向量｜Graphlet Degree Vector</h4><h5 id="图元"><a href="#图元" class="headerlink" title="图元"></a>图元</h5><p>图元（Graphlets）：指小规模的、无方向或有方向的连通子图，通常用于分析复杂网络的局部结构。图元可以帮助识别网络的模式、节点的结构角色，并用于特征提取、网络比较和生物网络分析。</p><ul><li>度数计算的是节点接触多少条边</li><li>聚类系数计算的是节点接触多少个三角形</li><li>GDV计算的是节点接触多少个图元</li></ul><h5 id="图元度向量的作用"><a href="#图元度向量的作用" class="headerlink" title="图元度向量的作用"></a>图元度向量的作用</h5><p>考虑由2到5个节点组成的图元时：</p><ul><li>通过分析2到5个节点的所有可能连通子图，构建一个包含73维度的向量，该向量被视为节点的特征签名，用于描述其局部拓扑结构。</li><li>该向量不仅考虑节点的直接邻居，还能捕捉其在最多4跳范围内的互连关系</li><li>图元度向量提供了比单纯的节点度（Degree）或聚类系数（Clustering Coefficient）更丰富的结构信息</li></ul><h3 id="链接级预测任务｜Link-Level-Prediction-Task"><a href="#链接级预测任务｜Link-Level-Prediction-Task" class="headerlink" title="链接级预测任务｜Link-Level Prediction Task"></a>链接级预测任务｜Link-Level Prediction Task</h3><p>在此任务中，是基于现有的链接预测新的链接。在测试阶段，对所有的未连接节点对进行排序，并且预测最高的K个节点对。<strong>关键是设计一对节点的特征</strong>。</p><p>两种链接预测的形式：</p><ul><li>随机链接缺失边：随机移除一些链接，并且尝试去预测它们</li><li>随时间进行链接：<ul><li>对于给定的图$G[t_0,t_0’]$，图的边在时间$t_0’$之前，需要产生一个对在原图中不存在的链接的排序的表，其中这些链接是被预测在$G[t_1,t_1’]$中出现</li><li>评估：<ul><li>$n=|E_{new}|$：在测试阶段$[t_1,t_1’]$新出现的边</li><li>取出L中前n个元素并且计算正确的边数</li></ul></li></ul></li></ul><h4 id="通过相似性的链接预测"><a href="#通过相似性的链接预测" class="headerlink" title="通过相似性的链接预测"></a>通过相似性的链接预测</h4><p>方法：</p><ol><li>对于每一对节点$(x,y)$，计算相似性分数$c(x,y)$<ul><li>例如，可以计算相同的邻居数目</li></ul></li><li>按照相似性分数降序排列节点对</li><li>预测分数最高的n对作为新的链接</li><li>查看这些链接中哪些真实存在于图$G[t_1,t_1’]$中</li></ol><h4 id="基于距离的特征｜Distance-Based-Features"><a href="#基于距离的特征｜Distance-Based-Features" class="headerlink" title="基于距离的特征｜Distance-Based Features"></a>基于距离的特征｜Distance-Based Features</h4><p>使用两个节点之间最短路径的长度</p><p>缺陷：不能捕获领域重叠的信息</p><h4 id="局部邻域重叠｜Local-Neighborhood-Overlap"><a href="#局部邻域重叠｜Local-Neighborhood-Overlap" class="headerlink" title="局部邻域重叠｜Local Neighborhood Overlap"></a>局部邻域重叠｜Local Neighborhood Overlap</h4><p>捕获两个节点$v_1,v_2$之间共享的邻居节点</p><ul><li>Common neighbors: $|N(v_1)\cap N(v_2)|$</li><li>Jaccard’s coefficient: $\frac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}$</li><li>Adamic-Adar index: $\sum_{u\in N(v_1)\cap N(v_2)}\frac{1}{log(k_u)}, k_u: degrees\quad of\quad node\quad u$</li></ul><p>缺陷：当两个节点没有任何公共邻居时总为0，但是这两个节点在未来仍有可能被链接</p><h4 id="全局邻域重叠｜Global-Neighborhood-Overlap"><a href="#全局邻域重叠｜Global-Neighborhood-Overlap" class="headerlink" title="全局邻域重叠｜Global Neighborhood Overlap"></a>全局邻域重叠｜Global Neighborhood Overlap</h4><p>Katz index：计算一个给定的节点对中所有长度的路径的数目</p><p>方法：<strong>使用邻接矩阵的幂可以计算各种长度的路径数</strong></p><ul><li>$A_{uv}$表示节点u和v之间长度为1的路径</li><li>$A_{uv}^{l}$表示节点u和v之间长度为l的路径</li></ul><p>对于$v_1,v_2$的Katz index计算如下：</p><script type="math/tex; mode=display">S_{v_1v_2} = \sum_{l=1}^{\infty}\beta^{l}A_{v_1v_2}^{l}, 0<\beta<1:discount\quad factor</script><p>Katz index矩阵闭式计算：</p><script type="math/tex; mode=display">\mathbf{S} = \sum_{i=1}^{\infty}\beta^i\mathbf{A}^i = (\mathbf{I} - \beta\mathbf{A})^{-1}-\mathbf{I}</script><h3 id="图级别的特征｜Graph-Level-Features"><a href="#图级别的特征｜Graph-Level-Features" class="headerlink" title="图级别的特征｜Graph-Level Features"></a>图级别的特征｜Graph-Level Features</h3><h4 id="背景：核方法｜Kernel-Methods"><a href="#背景：核方法｜Kernel-Methods" class="headerlink" title="背景：核方法｜Kernel Methods"></a>背景：核方法｜Kernel Methods</h4><p>核方法被广泛应用于传统机器学习中的图级别预测任务。</p><p>核心思想: 设计核函数而非特征向量</p><ul><li>核函数$K(G, G’)\in R$衡量数据之间的相似性</li><li>核矩阵$K=(K(G, G’))_{G,G’}$必须始终保持半正定(即具有非负特征值)</li><li>存在特征表示$\Phi(·)$使得$K(G,G’)=\Phi(G)^T\Phi(G’)$</li><li>一旦定义好核函数，就可以直接使用现成的机器学习模型(如核支持向量机)进行预测</li></ul><h4 id="图核｜Graph-Kernels"><a href="#图核｜Graph-Kernels" class="headerlink" title="图核｜Graph Kernels"></a>图核｜Graph Kernels</h4><p>核心思想是设计核函数来衡量图之间的相似性，而不是直接使用特征向量。图核方法允许在不显式构造高维特征向量的情况下，利用核函数将图映射到一个高维特征空间，并应用标准的机器学习模型（如支持向量机 SVM）进行预测。</p><ul><li><a href="#图元核graphlet-kernel">Graphlet Kernel</a></li><li><a href="#weisfeiler-lehman-kernel">Weisfeiler-Lehman Kernel</a></li><li>Random-walk kernel</li><li>Shortest-path graph kernel</li></ul><h4 id="词袋｜Bag-of-Words"><a href="#词袋｜Bag-of-Words" class="headerlink" title="词袋｜Bag-of-Words"></a>词袋｜Bag-of-Words</h4><p>为图设计特征向量$\phi (G)$，一种方法就是设计词袋（Bag-of-Words，BoW）。</p><p>词袋（BoW）：对于文本，直接对出现的单词进行计数并作为其特征，其中并没有考虑顺序的因素。</p><p>对于拓展到图上的朴素的思想就是将节点视作单词。</p><p>e.g.对于向量[1, 4, 0]，可以表示为度为1，2，3的节点分别有1，4，0个。</p><h4 id="图元核｜Graphlet-Kernel"><a href="#图元核｜Graphlet-Kernel" class="headerlink" title="图元核｜Graphlet Kernel"></a>图元核｜Graphlet Kernel</h4><p>核心思想：计算在图中的不同图元的数目，将得到的特征向量记为$\mathbf{f}_G$</p><p>对给定的图$G$，以及一个图元表</p><script type="math/tex; mode=display">G_{k}=(g_{1}, g_{2},...,g_{n_k})</script><p>我们将图元计数得到的向量$\mathbf{f}_{G}\in R^{n_k}$定义为</p><script type="math/tex; mode=display">(\mathbf{f}_G)_i = (g_i \subseteq G),i = 1,2,...,n_k</script><p>对于给定的两个图G,G’，图元核可以通过以下的方式进行计算：</p><script type="math/tex; mode=display">K(G,G') = \mathbf{f}_{G}^{T}\mathbf{f}_{G'}</script><p><strong>问题：</strong>如果G和G’拥有不同的大小，那么值将会有偏移</p><p><strong>解决方法：</strong>将每个特征向量进行归一化</p><script type="math/tex; mode=display">\mathbf{h}_{G} = \frac{\mathbf{f}_{G}}{Sum(\mathbf{f}_{G})}</script><script type="math/tex; mode=display">K(G, G') = \mathbf{h}_{G}^{T}\mathbf{h}_{G'}</script><p><strong>缺陷：</strong>计算图元开销很大。对于计算一个大小为n的图中大小为k的图元，使用枚举法需要$n^k$次。计算某图是否是另一个图的子图是NP难问题。但对于节点度数上限为$d$的图，计算大小为k的图元数目，现在存在一个$O(nd^{k-1})$的算法。</p><h4 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h4><p>思想：使用领域的结构来丰富节点的表示。</p><p>从Bag of node degrees提取出的信息是1-邻域的信息，使用算法Color refinement可以提取多领域信息。</p><h5 id="颜色细化｜Color-Refinement"><a href="#颜色细化｜Color-Refinement" class="headerlink" title="颜色细化｜Color Refinement"></a>颜色细化｜Color Refinement</h5><p>输入：图G和节点集合V</p><ul><li>对于每个节点v，给定一个初试颜色$c^{(0)}(v)$</li><li>通过以下公式迭代更新节点颜色，其中HASH将不同的输入映射到不同颜色：</li></ul><script type="math/tex; mode=display">c^{(k+1)}(v) = HASH(\{c^{(k)}(v),\{c^{(k)}(u)\}_{u\in N(v)}\})</script><ul><li>通过K步颜色更新，$c^{K}(v)$将提取出K邻域的结构信息</li></ul><h5 id="WL-Kernel的优势"><a href="#WL-Kernel的优势" class="headerlink" title="WL Kernel的优势"></a>WL Kernel的优势</h5><p>WL kernel计算效率很高，因为每次更新的时间复杂度都是线性的。而计算核值的时候，只有同时出现在两个图中的颜色才会被追踪。因此，颜色数目最多不会超过节点数目。计算颜色的数目的时间复杂度为线性的。因此，总的时间复杂度为线性的。</p><h2 id="节点嵌入｜Node-Embedding"><a href="#节点嵌入｜Node-Embedding" class="headerlink" title="节点嵌入｜Node Embedding"></a>节点嵌入｜Node Embedding</h2><p>假设目前拥有一个图$G$，其中</p><ul><li>$V$是节点集</li><li>$A$是邻接矩阵（假设为0-1邻接矩阵）</li><li>为了简便起见，假设没有节点信息和额外信息</li><li>假设是无向图</li></ul><p>目标：在原始网络中的相似性<script type="math/tex">similarity(u, v)\approx\mathbf{z_{v}^{T}z_{u}}</script></p><p>节点嵌入学习：</p><ol><li>编码器将节点映射为嵌入</li><li>定义节点相似性函数（例如用于衡量原始网络中的相似性）</li><li>解码器DEC将嵌入映射为相似性分数</li><li>优化编码器的参数使得<script type="math/tex">similarity(u, v)\approx\mathbf{z_{v}^{T}z_{u}}</script></li></ol><h3 id="编码器｜Encoder"><a href="#编码器｜Encoder" class="headerlink" title="编码器｜Encoder"></a>编码器｜Encoder</h3><p>将每个节点映射到一个低维向量</p><script type="math/tex; mode=display">ENC(v) = \mathbf{z}_v</script><h4 id="最简单的编码"><a href="#最简单的编码" class="headerlink" title="最简单的编码"></a>最简单的编码</h4><p>最简单的编码方法：编码仅为一个嵌入的查找</p><script type="math/tex; mode=display">ENC(v) = \mathbf{z}_{v} = \mathbf{Z}\cdot v</script><script type="math/tex; mode=display">\mathbf{Z}\in R^{d\times |V|}：矩阵，每一列是一个节点嵌入（我们需要学习/优化的目标）</script><script type="math/tex; mode=display">v\in \mathrm{II}^{|V|}：指示向量，在一列中只有0和1，用于指示节点v</script><p>缺陷：参数过多，矩阵大小与节点数成正比。节点数较多的情况下运算很慢。</p><h3 id="相似性函数｜Similarity-function"><a href="#相似性函数｜Similarity-function" class="headerlink" title="相似性函数｜Similarity function"></a>相似性函数｜Similarity function</h3><p>指定在向量空间中的关系如何映射到原始网络中的关系</p><script type="math/tex; mode=display">similarity(u,v)\approx\mathbf{z}_{v}^{T}\mathbf{z}_{u}</script><h3 id="随机游走嵌入｜Random-Walk-Embedding"><a href="#随机游走嵌入｜Random-Walk-Embedding" class="headerlink" title="随机游走嵌入｜Random-Walk Embedding"></a>随机游走嵌入｜Random-Walk Embedding</h3><ol><li>估计使用随机游走策略的决策$R$，从节点$u$出发访问节点$v$的概率</li><li>优化嵌入方式来编码这些随机游走的概率</li></ol><p>优化的特征学习：</p><ul><li>给定图$G=(V,E)$</li><li>目标是学习一个映射$f:u\rightarrow R^{d}: f(u) = z_{u}$</li><li>对数相似性目标：<script type="math/tex; mode=display"> max_{f}\sum_{u\in V}logP(N_{R}(u)|\mathbf{z}_{u}),N_{R}(u)是使用策略R的节点u的一个邻居</script></li></ul><p>对于给定的节点u，我们想要学习在随机游走的邻域$N_{R}(u)$中可预测的节点的特征表示。</p><h4 id="随机游走优化算法"><a href="#随机游走优化算法" class="headerlink" title="随机游走优化算法"></a>随机游走优化算法</h4><ol><li>从每个节点u开始在图中使用随机游走策略R走固定长度的短路径</li><li>对于每个节点u得到可重集合$N_{R}(u)$（记录使用从u开始的随机游走策略访问的节点）</li><li>根据给定节点$u$预测邻域$N_{R}(u)$来优化嵌入</li></ol><p>等价的，优化目标变为</p><script type="math/tex; mode=display">L = \sum_{u\in V}\sum_{v\in N_{R}(u)}-log(P(v|\mathbf{z}_u))</script><p>目标：优化嵌入$z_u$以最大化随机游走的可能性</p><p>使用softmax参数化$P(v|\mathbf{z}_u)$:</p><script type="math/tex; mode=display">P(v|\mathbf{z}_u) = \frac{exp(\mathbf{z}_{u}^{T}\mathbf{z}_{v})}{\sum_{n\in V}exp(\mathbf{z}_{u}^{T}\mathbf{z}_{n})}</script><p>因此，最终的优化的目标为（时间复杂度为$O(|V|^{2})）$：</p><script type="math/tex; mode=display">L = \sum_{u\in V}\sum_{v\in N_{R}(u)}-log(\frac{exp(\mathbf{z}_{u}^{T}\mathbf{z}_{v})}{\sum_{n\in V}exp(\mathbf{z}_{u}^{T}\mathbf{z}_{n})})</script><h4 id="负采样｜Negative-Sampling"><a href="#负采样｜Negative-Sampling" class="headerlink" title="负采样｜Negative Sampling"></a>负采样｜Negative Sampling</h4><p>由于原来需要优化的目标时间复杂度过高，需要对目标进近似，此处采用负采样的方法：</p><script type="math/tex; mode=display">log(\frac{exp(\mathbf{z}_{u}^{T}\mathbf{z}_{v})}{\sum_{n\in V}exp(\mathbf{z}_{u}^{T}\mathbf{z}_{n})}) \\approx log(\sigma(\mathbf{z}_{u}^{T}\mathbf{z}_{v})) - \sum_{i=1}^{k}log(\sigma(\mathbf{z}_{u}^{T}\mathbf{z_{n_{i}}})),n_{i}\sim P_{V}</script><p>在此过程中，对于每个探测随机采样k个负节点</p><p>其中更大的k给出的估计越具有鲁棒性，同时也会在负样例中给出更高的偏移。</p><p>实际中k取5到20。</p><p>优化方法使用<strong>随机梯度下降法</strong>。</p><hr><h2 id="参考文献｜References"><a href="#参考文献｜References" class="headerlink" title="参考文献｜References"></a>参考文献｜References</h2><ol><li>CS224W-Video:<a href="https://www.bilibili.com/video/BV1RZ4y1c7Co">CN</a>,<a href="https://www.youtube.com/watch?v=JAB_plj2rbA">EN</a></li></ol>]]></content>
    
    
    <summary type="html">本文基于Stanford的CS224W中一些理论内容，介绍有关图机器学习的手段与方法</summary>
    
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/categories/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/categories/AI/Graph-ML/"/>
    
    
    <category term="AI" scheme="https://epsilonzyj.github.io/tags/AI/"/>
    
    <category term="Graph ML" scheme="https://epsilonzyj.github.io/tags/Graph-ML/"/>
    
  </entry>
  
  <entry>
    <title>软件与工具集 | Fancy Tools</title>
    <link href="https://epsilonzyj.github.io/posts/ebcd0e10.html"/>
    <id>https://epsilonzyj.github.io/posts/ebcd0e10.html</id>
    <published>2025-02-25T16:00:00.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="跨平台工具"><a href="#跨平台工具" class="headerlink" title="跨平台工具"></a>跨平台工具</h2><h3 id="AI工具"><a href="#AI工具" class="headerlink" title="AI工具"></a>AI工具</h3><h4 id="API中转"><a href="#API中转" class="headerlink" title="API中转"></a>API中转</h4><ol><li><a href="https://cloud.siliconflow.cn/models">SiliconFlow</a>：国产厂商，优势在于没有汇率等影响而相对较便宜，缺点就是不含国外模型厂商的API，当需要使用Claude等在某些场景下表现更好的国外模型时有明显缺陷</li><li><a href="https://openrouter.ai">OpenRouter</a>：国外大模型API中转平台，可免费调用一些比较好的模型，但因为有汇率因素导致部分模型价格对学生等群体来说比较高</li></ol><h4 id="AI工具及模型"><a href="#AI工具及模型" class="headerlink" title="AI工具及模型"></a>AI工具及模型</h4><p><em>写在前面：如果是用于做研究或者是考虑深入某一领域等，国产大模型很多似乎都表现得比较拉垮，因此笔者也懒得去尝试一些模型。就身边的同学等使用体验来看，对于国产大模型，在下文中提到的相对比较高质量，其它的模型甚至BAT的似乎更多的是提供情绪价值，并没有多少辅助工作学习的实际作用。当然，考虑到国产大模型其实免费的偏多，并且可以不依赖镜像站等直接访问，相对来说还是有优势的。因此，如果要深入某一领域，还是需要自行寻找一下比较好的模型，从而达到事半功倍的效果。</em></p><ol><li><a href="https://iflow.cn">心流AI</a>：可用于查找论文，阅读论文（论文翻译）等</li><li><a href="https://ai-bot.cn">AI工具集</a>：比较全的AI工具集，查找一些想要的AI工具在这个平台上都能找到</li><li><a href="https://claude.ai/">Claude</a>：写代码全靠Claude捞😋</li><li><a href="https://chatgpt.com">ChatGPT</a>：通用模型，特殊领域表现并不如其它模型</li><li><a href="https://chat.deepseek.com">DeepSeek</a>：数学推理等比较强，可以作为通用模型，文本处理比较厉害，多模态（如图片识别之类的任务）能力较弱，写代码能力还可以。当然，建议使用深度求索公司直接提供的模型，其它厂商自己训练的模型由于数据集等问题，训练效果良莠不齐。</li><li><a href="https://www.kimi.com">Kimi AI</a>：能力还算可以，可以用于当作通用模型，可用于搜索一些内容，多模态能力比较强</li><li><a href="https://manus.im/app">Manus</a>：校友产品支持一波～～可以进行一些复杂任务</li></ol><p><em>注：当前国产AI对于学术和开发友好的模型，笔者及周围测试下来觉得不错的有DeepSeek R1-0528，Kimi K2，Qwen3 Coder.</em></p><h4 id="AI杂项-前端"><a href="#AI杂项-前端" class="headerlink" title="AI杂项/前端"></a>AI杂项/前端</h4><ol><li><a href="https://github.com/CherryHQ/cherry-studio?tab=readme-ov-file">Cherry Studio</a>：可以集成多种大语言模型的客户端</li></ol><h3 id="计算机学术工具"><a href="#计算机学术工具" class="headerlink" title="计算机学术工具"></a>计算机学术工具</h3><ol><li><a href="https://arxiv.org">arXiv</a>：未发表或已发表的文章均有，可以查找最新的科研进展</li><li><a href="https://dblp.org">dblp</a>：CS论文</li><li><a href="https://scholar.google.com">Google Scholar</a>：各种论文</li><li><a href="https://paperswithcode.com">Paper with code</a>：主要是关于机器学习和数据科学的论文和开源代码</li><li><a href="https://snip.mathpix.com/home">Mathpix</a></li><li><a href="https://simpletex.cn/ai/latex_ocr">Simpleletex</a>：Mathpix的平替</li><li><a href="https://app.diagrams.net">Draw IO</a>：在线绘图工具（流程图等），<a href="https://github.com/jgraph/drawio-desktop/tree/v26.2.15">客户端</a></li><li><a href="https://typora.io">Typora</a>：Markdown编辑器</li><li><a href="https://obsidian.md">Obsidian</a>：类似Markdown Pro，用于整理知识、笔记等内容，同时有助于一些使用Markdown进行博客部署的框架进行博客的快速部署</li><li><a href="https://www.zotero.org">Zotero</a>：文献管理</li></ol><h3 id="服务器等"><a href="#服务器等" class="headerlink" title="服务器等"></a>服务器等</h3><ol><li><a href="https://www.autodl.com/home">AutoDL</a>：深度学习租卡网站</li></ol><h3 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h3><ol><li><a href="https://github.com/Eugeny/tabby">Tabby终端</a>：开源免费的终端</li></ol><h3 id="IDE-编辑器"><a href="#IDE-编辑器" class="headerlink" title="IDE/编辑器"></a>IDE/编辑器</h3><ol><li><a href="https://code.visualstudio.com">VS Code</a></li><li><a href="https://www.jetbrains.com/zh-cn/">Jetbrains全家桶</a></li><li><a href="https://www.cursor.com">Cursor</a>：AI编辑器</li><li><a href="https://kiro.dev">Kiro</a>：AI编辑器，由AWS开发</li></ol><h2 id="macOS"><a href="#macOS" class="headerlink" title="macOS"></a>macOS</h2><h3 id="必推"><a href="#必推" class="headerlink" title="必推"></a>必推</h3><ol><li><a href="https://github.com/Homebrew/brew/releases">HomeBrew</a>：macOS上（或许）最好的软件包管理器</li><li><a href="https://github.com/iina/iina/tree/v1.4.0-beta1">IINA播放器</a>：macOS上较好的播放器</li><li><a href="https://pilotmoon.com/scrollreverser/">Scroll Reverser</a>：macOS上鼠标滚轮反向，不使用鼠标则无需使用，<a href="https://github.com/pilotmoon/Scroll-Reverser">源码</a></li><li><a href="https://iterm2.com">iTerm2</a>：macOS上最好的终端（认真）。之前用过一段时间Tabby，但后来改用iTerm2后才知道有多强。配置方法直接参照知乎上的大佬配置就行了，<a href="https://zhuanlan.zhihu.com/p/550022490">文章链接</a>。</li></ol><h3 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h3><ol><li><a href="https://developer.apple.com/xcode/">XCode</a>：macOS上的开发工具，iOS开发必备</li><li><a href="https://github.com/nicklockwood/SwiftFormat">SwiftFormat for Xcode</a>：Swift代码格式化工具</li><li><a href="https://apps.apple.com/us/app/devcleaner-for-xcode/id1388020431">DevCleaner</a>：macOS上的垃圾清理工具</li><li><a href="https://github.com/github/copilot-extension">Github Copilot for Xcode</a>：Xcode上的Github Copilot插件，虽然不是很好用，但是能用就行</li></ol><h3 id="系统工具-杂项"><a href="#系统工具-杂项" class="headerlink" title="系统工具/杂项"></a>系统工具/杂项</h3><p>得益于Mac强大的社区，我们总是可以找到一些很有意思的工具或者是插件。此部分很多东西都来源于Github，在此也需要感谢每一位无私用心付出的开源社区作者。</p><ol><li><a href="https://www.parallels.cn/products/desktop/">Parallels Desktop</a>：macOS上的虚拟机，可以运行Windows，还有很多附加的工具，虽然不是很好用，但是也还算可以</li><li><a href="https://apps.apple.com/us/app/hidden-bar/id1452453066">Hidden Bar</a>：macOS上隐藏菜单栏的工具</li><li><a href="https://cleanmymac.com">CleanMyMac</a>：macOS上的垃圾清理工具，很贵但很好用</li><li><a href="https://www.macbartender.com">Bartender</a>：和Hidden Bar类似，但是功能更加强大，可以隐藏菜单栏，Dock栏，Finder栏等，但是很贵</li><li><a href="https://theunarchiver.com">The Unarchiver</a>：macOS自带解压，但对于一些除了zip之外其它的压缩包格式并不支持，而这个软件支持几乎所有压缩包格式的解压，而且免费</li><li><a href="https://theboring.name">boringNotch</a>：没什么比较大的实际用处，只是将MacBook上的摄像头的一部分变为和iPhone一样的灵动岛，装饰使用</li><li><a href="https://icemenubar.app">IceMenubar</a>：和Hidden Bar类似，开源，免费，功能强大，比较好用</li><li><a href="https://apps.apple.com/cn/app/amphetamine/id937984704?mt=12">Amphetamine</a>：用于设置防止系统睡眠，适合开发、编译、进行深度学习等需要不熄屏的场景，并且可以个性化设置，无需将整个系统设置为不熄屏</li><li><a href="https://www.keka.io/en/">Keka</a>：Mac上最强大的压缩软件之一，可以去除Mac自带的系统文件。在官网上可以免费下载，但有条件可以在App Store上购买支持良心作者。</li><li><a href="https://apps.apple.com/us/app/pixelstyle-photo-editor/id1244649277?mt=12">PixelStyle</a>：Mac上比较好的图片编辑软件，相比Adobe Photoshop等的优势就是免费</li><li><a href="https://apps.apple.com/cn/app/irightmenu-右键新建文件菜单/id1542347829?mt=12">iRightMenu</a>：为macOS创造类似于Windows系统的右键菜单</li></ol><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><p><del>Windows没有必要推荐工具。</del>笔者太懒了，以后再来写吧。如果一定要推荐，那就推荐WSL吧。</p><ol><li><a href="https://learn.microsoft.com/en-us/windows/wsl/install">WSL文档</a>：<del>不知道放什么，就放个文档链接吧。</del>既想用Linux又想用Windows的用户的福音，同时共享同一个文件系统，当然还可以直接用来做深度学习。</li></ol>]]></content>
    
    
    <summary type="html">一些实用工具集与开发工具以及评测（实时更新）</summary>
    
    
    
    <category term="Tools" scheme="https://epsilonzyj.github.io/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>Java环境配置</title>
    <link href="https://epsilonzyj.github.io/posts/f7539049.html"/>
    <id>https://epsilonzyj.github.io/posts/f7539049.html</id>
    <published>2025-02-25T16:00:00.000Z</published>
    <updated>2025-10-20T11:51:24.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MacOS"><a href="#MacOS" class="headerlink" title="MacOS"></a>MacOS</h2><h3 id="使用Homebrew安装JDK"><a href="#使用Homebrew安装JDK" class="headerlink" title="使用Homebrew安装JDK"></a>使用Homebrew安装JDK</h3><ul><li>配置Homebrew</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;</span><br></pre></td></tr></table></figure><h4 id="安装OpenJDK"><a href="#安装OpenJDK" class="headerlink" title="安装OpenJDK"></a>安装OpenJDK</h4><p>查询jdk版本信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew search jdk</span><br></pre></td></tr></table></figure><p>安装特定版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install openjdk@17</span><br></pre></td></tr></table></figure><h4 id="配置JDK"><a href="#配置JDK" class="headerlink" title="配置JDK"></a>配置JDK</h4><p>注意，运行完以上命令后，终端会出现如下的信息：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">For the system Java wrappers to find this JDK, symlink it with</span><br><span class="line">  sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk</span><br><span class="line"></span><br><span class="line">openjdk@17 is keg-only, which means it was not symlinked into /opt/homebrew,</span><br><span class="line">because this is an alternate version of another formula.</span><br><span class="line"></span><br><span class="line">If you need to have openjdk@17 first in your PATH, run:</span><br><span class="line">  echo &#x27;export PATH=&quot;/opt/homebrew/opt/openjdk@17/bin:$PATH&quot;&#x27; &gt;&gt; ~/.zshrc</span><br><span class="line"></span><br><span class="line">For compilers to find openjdk@17 you may need to set:</span><br><span class="line">  export CPPFLAGS=&quot;-I/opt/homebrew/opt/openjdk@17/include&quot;</span><br><span class="line">==&gt; Summary</span><br><span class="line">🍺  /opt/homebrew/Cellar/openjdk@17/17.0.14: 636 files, 304.3MB</span><br><span class="line">==&gt; Running `brew cleanup openjdk@17`...</span><br><span class="line">Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.</span><br><span class="line">Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).</span><br></pre></td></tr></table></figure><p>根据指示，运行相应的命令。首先运行如下命令使得系统可以找到当前下载的JDK：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk</span><br></pre></td></tr></table></figure><p>可以通过tree命令来检查是否成功。</p><ul><li>若没有安装过tree，则运行以下命令：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tree</span><br></pre></td></tr></table></figure><p>检查是否成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree /Library/Java</span><br></pre></td></tr></table></figure><h4 id="检查当前JDK以及Java环境"><a href="#检查当前JDK以及Java环境" class="headerlink" title="检查当前JDK以及Java环境"></a>检查当前JDK以及Java环境</h4><p>执行以下命令可以查看当前系统使用的JDK版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/libexec/java_home</span><br></pre></td></tr></table></figure><p>执行以下命令可查看当前系统使用的Java版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java --version</span><br></pre></td></tr></table></figure><h4 id="多版本JDK管理"><a href="#多版本JDK管理" class="headerlink" title="多版本JDK管理"></a>多版本JDK管理</h4><p>先要查找到JDK的地址，采用如下的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/libexec/java_home -V</span><br></pre></td></tr></table></figure><p>若没有建过.bash_profile文件，则在根目录下建此配置文件，并打开此配置文件。如已经有，则添加即可。其中地址均使用上面命令查找出来的地址。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#配置JDK路径</span><br><span class="line">export JAVA_11_HOME=/Library/Java/JavaVirtualMachines/jdk-11.jdk/Contents/Home</span><br><span class="line">export JAVA_17_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home</span><br><span class="line"> </span><br><span class="line"># 设置默认JDK版本，默认使用 JDK17</span><br><span class="line">export JAVA_HOME=$JAVA_17_HOME</span><br><span class="line">CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:.</span><br><span class="line"> </span><br><span class="line"># 配置alias命令动态切换JDK版本  </span><br><span class="line">alias jdk11=&quot;export JAVA_HOME=$JAVA_11_HOME&quot;</span><br><span class="line">alias jdk17=&quot;export JAVA_HOME=$JAVA_17_HOME&quot;</span><br><span class="line"> </span><br><span class="line">export JAVA_HOME</span><br><span class="line">export CLASSPATH</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>若嫌麻烦可以跳过查看地址那一步，直接在上述.bash_profile中编辑：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=$(/usr/libexec/java_home -v11)</span><br><span class="line">export JAVA_8_HOME=$(/usr/libexec/java_home -v1.8)</span><br><span class="line">export JAVA_11_HOME=$(/usr/libexec/java_home -v11)</span><br><span class="line"></span><br><span class="line">alias java8=&#x27;export JAVA_HOME=$JAVA_8_HOME&#x27;</span><br><span class="line">alias java11=&#x27;export JAVA_HOME=$JAVA_11_HOME&#x27;</span><br></pre></td></tr></table></figure><p>保存配置文件，在终端中输出如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source .bash_profile</span><br></pre></td></tr></table></figure><p>可以通过如下命令查看是否配置成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $JAVA_HOME</span><br></pre></td></tr></table></figure><h4 id="JDK版本切换"><a href="#JDK版本切换" class="headerlink" title="JDK版本切换"></a>JDK版本切换</h4><p>在终端中输入命令：jdk/java版本号 即可，如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdk17</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java17</span><br></pre></td></tr></table></figure><p>注意看清楚编辑时alias后面的命令到底是jdk还是java</p><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><h3 id="下载并安装JDK"><a href="#下载并安装JDK" class="headerlink" title="下载并安装JDK"></a>下载并安装JDK</h3><p>上<a href="https://www.oracle.com/java/technologies/downloads/?er=221886">官网</a>下载所需的JDK，运行相应的安装程序即可完成安装。若从未注册过账号，需要在官网注册账号后才能下载。</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以上的JDK目录，在系统变量中，配置为<strong>JAVA_HOME</strong>，路径为JDK路径，如：C:\Program Files\Java\jdk1.8.0_191。</p><p>然后，在Path中添加两个路径，分别为</p><ul><li><p><strong>%JAVA_HOME%\bin</strong></p></li><li><p><strong>%JAVA_HOME%\jre\bin</strong></p></li></ul><p>此外，还需要配置<strong>CLASSPATH</strong>变量。若本来存在这个变量，则进行编辑，若无则新建。变量值为</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;</span><br></pre></td></tr></table></figure><p>可以通过以下命令来检查是否安装成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java</span><br></pre></td></tr></table></figure><h2 id="Java运行"><a href="#Java运行" class="headerlink" title="Java运行"></a>Java运行</h2><p>最方便和最简单的当然是直接拿IDEA一键运行啦，当然稍微麻烦点的就是配置VSCode。但如果除去这两个，就需要依靠运行脚本来运行Java程序了。</p><p>下面将展示脚本编写示例。</p><h3 id="Windows脚本"><a href="#Windows脚本" class="headerlink" title="Windows脚本"></a>Windows脚本</h3><p>在当前工程目录下，创建run.bat脚本文件（文件名是什么随便起就行，不一定要为run.bat）。假设我们的工程目录是如下的情况，我们可以在脚本文件中对应编辑如下内容：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">JavaDemo</span><br><span class="line">├── .idea</span><br><span class="line">├── bin</span><br><span class="line">│   └── production</span><br><span class="line">│       └── JavaDemo</span><br><span class="line">│           └── hust</span><br><span class="line">│               └── cs</span><br><span class="line">│                   └── javacourse</span><br><span class="line">│                       └── ch1</span><br><span class="line">│                           └── HelloWorld.class</span><br><span class="line">├── src</span><br><span class="line">│   └── hust.cs.javacourse.ch1</span><br><span class="line">│       └── HelloWorld.java</span><br><span class="line">└── JavaDemo.iml</span><br></pre></td></tr></table></figure><figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> JAVA_HOME=D:\jdk1.<span class="number">8</span>.<span class="number">0</span>_231_64bit</span><br><span class="line"><span class="built_in">set</span> PROJECT_HOME=D:\IdeaWorkspace\JavaDemo</span><br><span class="line"><span class="built_in">set</span> <span class="built_in">path</span>=<span class="variable">%path%</span>;<span class="variable">%JAVA_HOME%</span>\bin</span><br><span class="line"><span class="built_in">set</span> classpath=<span class="variable">%classpath%</span>;<span class="variable">%PROJECT_HOME%</span>\bin\production\JavaDemo</span><br><span class="line"></span><br><span class="line">java -classpath <span class="variable">%classpath%</span> hust.cs.javacourse.ch1.HelloWorld</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><p>JAVA_HOME设置JAVA_HOME环境变量</p></li><li><p>PROJECT_HOME设置PROJECT_HOME环境变量</p></li><li><p>path设置把JAVA_HOME目录的子目录bin加到环境变量PATH</p></li><li><p>classpath把PROJECT_HOME目录的子目录bin\production\JavaDemo加到环境变量CLASSPATH，这个目录是类HelloWorld所属包的顶级目录</p></li><li><p>最后一行为运行指令，启动类时，用类的完全限定名（带包名限定），并且带-classspath选项</p></li></ul><p>运行时直接命令行输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./run.bat</span><br></pre></td></tr></table></figure><hr><p>参考资料：</p><ol><li><p><a href="https://blog.csdn.net/Jarvs/article/details/134669580">Mac JDK环境变量配置 及 JDK多版本切换</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/298535991">新 Mac 如何优雅地配置 Java 开发环境</a></p></li></ol>]]></content>
    
    
    <summary type="html">Java环境配置方法（兼MacOS和Windows）</summary>
    
    
    
    <category term="Env" scheme="https://epsilonzyj.github.io/categories/Env/"/>
    
    
    <category term="Java" scheme="https://epsilonzyj.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Hello Hexo</title>
    <link href="https://epsilonzyj.github.io/posts/4a17b156.html"/>
    <id>https://epsilonzyj.github.io/posts/4a17b156.html</id>
    <published>2025-02-22T16:00:00.000Z</published>
    <updated>2025-10-20T11:51:24.023Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="More"><a href="#More" class="headerlink" title="More"></a>More</h2><p>可以参考安知鱼的<a href="https://blog.anheyu.com/posts/sdxhu.html#配置自定义-css">博客</a>和<a href="https://www.bilibili.com/video/BV1CG41157fr?spm_id_from=333.788.player.switch&amp;vd_source=2e36fae16810615c2d859efc03aef1c4">Bilibili</a></p>]]></content>
    
    
    <summary type="html">Hexo+Butterfly框架参考文档</summary>
    
    
    
    <category term="本站搭建" scheme="https://epsilonzyj.github.io/categories/%E6%9C%AC%E7%AB%99%E6%90%AD%E5%BB%BA/"/>
    
    
  </entry>
  
</feed>
